<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10 Bootstrap Examples and the Jackknife | Elements of Nonparametric Statistics</title>
  <meta name="description" content="Course notes for Biostatistics 685/Statistics 560 (Winter 2020)." />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 10 Bootstrap Examples and the Jackknife | Elements of Nonparametric Statistics" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://nchenderson.github.io/elements-nonpar-stat/" />
  
  <meta property="og:description" content="Course notes for Biostatistics 685/Statistics 560 (Winter 2020)." />
  <meta name="github-repo" content="nchenderson/elements-nonpar-stat" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 Bootstrap Examples and the Jackknife | Elements of Nonparametric Statistics" />
  
  <meta name="twitter:description" content="Course notes for Biostatistics 685/Statistics 560 (Winter 2020)." />
  

<meta name="author" content="Nicholas Henderson" />


<meta name="date" content="2020-04-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bootstrap-main.html"/>
<link rel="next" href="kernel-regression-and-local-regression.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Biostat 685/Stat 560</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#sec:whatisnonpar"><i class="fa fa-check"></i><b>1.1</b> What is Nonparametric Statistics?</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#sec:course-outline"><i class="fa fa-check"></i><b>1.2</b> Outline of Course</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#sec:example-nonpar-tests"><i class="fa fa-check"></i><b>1.3</b> Example 1: Nonparametric vs.Â Parametric Two-Sample Testing</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#sec:example-nonpar-estimation"><i class="fa fa-check"></i><b>1.4</b> Example 2: Nonparametric Estimation</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#sec:example-nonpar-confint"><i class="fa fa-check"></i><b>1.5</b> Example 3: Confidence Intervals</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#sec:example-nonpar-regress1"><i class="fa fa-check"></i><b>1.6</b> Example 4: Nonparametric Regression with a Single Covariate</a></li>
<li class="chapter" data-level="1.7" data-path="intro.html"><a href="intro.html#sec:example-nonpar-regress2"><i class="fa fa-check"></i><b>1.7</b> Example 5: Classification and Regression Trees (CART)</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>2</b> Working with R</a></li>
<li class="part"><span><b>I Nonparametric Testing</b></span></li>
<li class="chapter" data-level="3" data-path="rank-tests.html"><a href="rank-tests.html"><i class="fa fa-check"></i><b>3</b> Rank and Sign Statistics</a><ul>
<li class="chapter" data-level="3.1" data-path="rank-tests.html"><a href="rank-tests.html#ranks"><i class="fa fa-check"></i><b>3.1</b> Ranks</a><ul>
<li class="chapter" data-level="3.1.1" data-path="rank-tests.html"><a href="rank-tests.html#definition"><i class="fa fa-check"></i><b>3.1.1</b> Definition</a></li>
<li class="chapter" data-level="3.1.2" data-path="rank-tests.html"><a href="rank-tests.html#handling-ties"><i class="fa fa-check"></i><b>3.1.2</b> Handling Ties</a></li>
<li class="chapter" data-level="3.1.3" data-path="rank-tests.html"><a href="rank-tests.html#properties-of-ranks"><i class="fa fa-check"></i><b>3.1.3</b> Properties of Ranks</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="rank-tests.html"><a href="rank-tests.html#the-wilcoxon-rank-sum-wrs-test-a-two-sample-test"><i class="fa fa-check"></i><b>3.2</b> The Wilcoxon Rank Sum (WRS) Test: A Two-Sample Test</a><ul>
<li class="chapter" data-level="3.2.1" data-path="rank-tests.html"><a href="rank-tests.html#goal-of-the-test"><i class="fa fa-check"></i><b>3.2.1</b> Goal of the Test</a></li>
<li class="chapter" data-level="3.2.2" data-path="rank-tests.html"><a href="rank-tests.html#definition-of-the-wrs-test-statistic"><i class="fa fa-check"></i><b>3.2.2</b> Definition of the WRS Test Statistic</a></li>
<li class="chapter" data-level="3.2.3" data-path="rank-tests.html"><a href="rank-tests.html#computing-p-values-for-the-wrs-test"><i class="fa fa-check"></i><b>3.2.3</b> Computing p-values for the WRS Test</a></li>
<li class="chapter" data-level="3.2.4" data-path="rank-tests.html"><a href="rank-tests.html#computing-the-wrs-test-in-r"><i class="fa fa-check"></i><b>3.2.4</b> Computing the WRS test in R</a></li>
<li class="chapter" data-level="3.2.5" data-path="rank-tests.html"><a href="rank-tests.html#additional-notes-for-the-wrs-test"><i class="fa fa-check"></i><b>3.2.5</b> Additional Notes for the WRS test</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="rank-tests.html"><a href="rank-tests.html#one-sample-tests"><i class="fa fa-check"></i><b>3.3</b> One Sample Tests</a><ul>
<li class="chapter" data-level="3.3.1" data-path="rank-tests.html"><a href="rank-tests.html#sign-test"><i class="fa fa-check"></i><b>3.3.1</b> The Sign Test</a></li>
<li class="chapter" data-level="3.3.2" data-path="rank-tests.html"><a href="rank-tests.html#the-wilcoxon-signed-rank-test"><i class="fa fa-check"></i><b>3.3.2</b> The Wilcoxon Signed Rank Test</a></li>
<li class="chapter" data-level="3.3.3" data-path="rank-tests.html"><a href="rank-tests.html#using-r-to-perform-the-sign-and-wilcoxon-tests"><i class="fa fa-check"></i><b>3.3.3</b> Using R to Perform the Sign and Wilcoxon Tests</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="rank-tests.html"><a href="rank-tests.html#power-and-comparisons-with-parametric-tests"><i class="fa fa-check"></i><b>3.4</b> Power and Comparisons with Parametric Tests</a><ul>
<li class="chapter" data-level="3.4.1" data-path="rank-tests.html"><a href="rank-tests.html#the-power-function-of-a-test"><i class="fa fa-check"></i><b>3.4.1</b> The Power Function of a Test</a></li>
<li class="chapter" data-level="3.4.2" data-path="rank-tests.html"><a href="rank-tests.html#power-comparisons-and-asymptotic-relative-efficiency"><i class="fa fa-check"></i><b>3.4.2</b> Power Comparisons and Asymptotic Relative Efficiency</a></li>
<li class="chapter" data-level="3.4.3" data-path="rank-tests.html"><a href="rank-tests.html#efficiency-examples"><i class="fa fa-check"></i><b>3.4.3</b> Efficiency Examples</a></li>
<li class="chapter" data-level="3.4.4" data-path="rank-tests.html"><a href="rank-tests.html#efficiency-comparisons-for-several-distributions"><i class="fa fa-check"></i><b>3.4.4</b> Efficiency Comparisons for Several Distributions</a></li>
<li class="chapter" data-level="3.4.5" data-path="rank-tests.html"><a href="rank-tests.html#a-power-contest"><i class="fa fa-check"></i><b>3.4.5</b> A Power âContestâ</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="rank-tests.html"><a href="rank-tests.html#linear-rank-statistics-in-general"><i class="fa fa-check"></i><b>3.5</b> Linear Rank Statistics in General</a><ul>
<li class="chapter" data-level="3.5.1" data-path="rank-tests.html"><a href="rank-tests.html#definition-1"><i class="fa fa-check"></i><b>3.5.1</b> Definition</a></li>
<li class="chapter" data-level="3.5.2" data-path="rank-tests.html"><a href="rank-tests.html#properties-of-linear-rank-statistics"><i class="fa fa-check"></i><b>3.5.2</b> Properties of Linear Rank Statistics</a></li>
<li class="chapter" data-level="3.5.3" data-path="rank-tests.html"><a href="rank-tests.html#other-examples-of-linear-rank-statistics"><i class="fa fa-check"></i><b>3.5.3</b> Other Examples of Linear Rank Statistics</a></li>
<li class="chapter" data-level="3.5.4" data-path="rank-tests.html"><a href="rank-tests.html#choosing-the-scores-a_ni"><i class="fa fa-check"></i><b>3.5.4</b> Choosing the scores <span class="math inline">\(a_{N}(i)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="rank-tests.html"><a href="rank-tests.html#additional-reading"><i class="fa fa-check"></i><b>3.6</b> Additional Reading</a></li>
<li class="chapter" data-level="3.7" data-path="rank-tests.html"><a href="rank-tests.html#exercises"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="krusk-wallis.html"><a href="krusk-wallis.html"><i class="fa fa-check"></i><b>4</b> Rank Tests for Multiple Groups</a><ul>
<li class="chapter" data-level="4.1" data-path="krusk-wallis.html"><a href="krusk-wallis.html#the-kruskal-wallis-test"><i class="fa fa-check"></i><b>4.1</b> The Kruskal-Wallis Test</a><ul>
<li class="chapter" data-level="4.1.1" data-path="krusk-wallis.html"><a href="krusk-wallis.html#definition-2"><i class="fa fa-check"></i><b>4.1.1</b> Definition</a></li>
<li class="chapter" data-level="4.1.2" data-path="krusk-wallis.html"><a href="krusk-wallis.html#asymptotic-distribution-and-connection-to-one-way-anova"><i class="fa fa-check"></i><b>4.1.2</b> Asymptotic Distribution and Connection to One-Way ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="krusk-wallis.html"><a href="krusk-wallis.html#performing-the-kruskal-wallis-test-in-r"><i class="fa fa-check"></i><b>4.2</b> Performing the Kruskal-Wallis Test in R</a></li>
<li class="chapter" data-level="4.3" data-path="krusk-wallis.html"><a href="krusk-wallis.html#comparison-of-specific-groups"><i class="fa fa-check"></i><b>4.3</b> Comparison of Specific Groups</a></li>
<li class="chapter" data-level="4.4" data-path="krusk-wallis.html"><a href="krusk-wallis.html#an-additional-example"><i class="fa fa-check"></i><b>4.4</b> An Additional Example</a></li>
<li class="chapter" data-level="4.5" data-path="krusk-wallis.html"><a href="krusk-wallis.html#additional-reading-1"><i class="fa fa-check"></i><b>4.5</b> Additional Reading</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="permutation.html"><a href="permutation.html"><i class="fa fa-check"></i><b>5</b> Permutation Tests</a><ul>
<li class="chapter" data-level="5.1" data-path="permutation.html"><a href="permutation.html#notation"><i class="fa fa-check"></i><b>5.1</b> Notation</a></li>
<li class="chapter" data-level="5.2" data-path="permutation.html"><a href="permutation.html#permutation-tests-for-the-two-sample-problem"><i class="fa fa-check"></i><b>5.2</b> Permutation Tests for the Two-Sample Problem</a><ul>
<li class="chapter" data-level="5.2.1" data-path="permutation.html"><a href="permutation.html#example-1"><i class="fa fa-check"></i><b>5.2.1</b> Example 1</a></li>
<li class="chapter" data-level="5.2.2" data-path="permutation.html"><a href="permutation.html#permutation-test-p-values"><i class="fa fa-check"></i><b>5.2.2</b> Permutation Test p-values</a></li>
<li class="chapter" data-level="5.2.3" data-path="permutation.html"><a href="permutation.html#example-2-ratios-of-means"><i class="fa fa-check"></i><b>5.2.3</b> Example 2: Ratios of Means</a></li>
<li class="chapter" data-level="5.2.4" data-path="permutation.html"><a href="permutation.html#example-3-differences-in-quantiles"><i class="fa fa-check"></i><b>5.2.4</b> Example 3: Differences in Quantiles</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="permutation.html"><a href="permutation.html#the-permutation-test-as-a-conditional-test"><i class="fa fa-check"></i><b>5.3</b> The Permutation Test as a Conditional Test</a></li>
<li class="chapter" data-level="5.4" data-path="permutation.html"><a href="permutation.html#a-permutation-test-for-correlation"><i class="fa fa-check"></i><b>5.4</b> A Permutation Test for Correlation</a></li>
<li class="chapter" data-level="5.5" data-path="permutation.html"><a href="permutation.html#a-permutation-test-for-variable-importance-in-regression-and-machine-learning"><i class="fa fa-check"></i><b>5.5</b> A Permutation Test for Variable Importance in Regression and Machine Learning</a></li>
</ul></li>
<li class="part"><span><b>II Nonparametric Estimation</b></span></li>
<li class="chapter" data-level="6" data-path="ustat.html"><a href="ustat.html"><i class="fa fa-check"></i><b>6</b> U-Statistics</a><ul>
<li class="chapter" data-level="6.1" data-path="ustat.html"><a href="ustat.html#definition-3"><i class="fa fa-check"></i><b>6.1</b> Definition</a></li>
<li class="chapter" data-level="6.2" data-path="ustat.html"><a href="ustat.html#examples"><i class="fa fa-check"></i><b>6.2</b> Examples</a><ul>
<li class="chapter" data-level="6.2.1" data-path="ustat.html"><a href="ustat.html#example-1-the-sample-mean"><i class="fa fa-check"></i><b>6.2.1</b> Example 1: The Sample Mean</a></li>
<li class="chapter" data-level="6.2.2" data-path="ustat.html"><a href="ustat.html#example-2-the-sample-variance"><i class="fa fa-check"></i><b>6.2.2</b> Example 2: The Sample Variance</a></li>
<li class="chapter" data-level="6.2.3" data-path="ustat.html"><a href="ustat.html#example-3-ginis-mean-difference"><i class="fa fa-check"></i><b>6.2.3</b> Example 3: Giniâs Mean Difference</a></li>
<li class="chapter" data-level="6.2.4" data-path="ustat.html"><a href="ustat.html#example-4-wilcoxon-signed-rank-statistic"><i class="fa fa-check"></i><b>6.2.4</b> Example 4: Wilcoxon Signed Rank Statistic</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="ustat.html"><a href="ustat.html#inference-using-u-statistics"><i class="fa fa-check"></i><b>6.3</b> Inference using U-statistics</a></li>
<li class="chapter" data-level="6.4" data-path="ustat.html"><a href="ustat.html#u-statistics-for-two-sample-problems"><i class="fa fa-check"></i><b>6.4</b> U-statistics for Two-Sample Problems</a><ul>
<li class="chapter" data-level="6.4.1" data-path="ustat.html"><a href="ustat.html#the-mann-whitney-statistic"><i class="fa fa-check"></i><b>6.4.1</b> The Mann-Whitney Statistic</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="ustat.html"><a href="ustat.html#measures-of-association"><i class="fa fa-check"></i><b>6.5</b> Measures of Association</a><ul>
<li class="chapter" data-level="6.5.1" data-path="ustat.html"><a href="ustat.html#spearmans-rank-correlation"><i class="fa fa-check"></i><b>6.5.1</b> Spearmanâs Rank Correlation</a></li>
<li class="chapter" data-level="6.5.2" data-path="ustat.html"><a href="ustat.html#kendalls-tau"><i class="fa fa-check"></i><b>6.5.2</b> Kendallâs tau</a></li>
<li class="chapter" data-level="6.5.3" data-path="ustat.html"><a href="ustat.html#distance-covariance-and-correlation"><i class="fa fa-check"></i><b>6.5.3</b> Distance Covariance and Correlation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="edf.html"><a href="edf.html"><i class="fa fa-check"></i><b>7</b> The Empirical Distribution Function</a><ul>
<li class="chapter" data-level="7.1" data-path="edf.html"><a href="edf.html#definition-and-basic-properties"><i class="fa fa-check"></i><b>7.1</b> Definition and Basic Properties</a></li>
<li class="chapter" data-level="7.2" data-path="edf.html"><a href="edf.html#confidence-intervals-for-ft"><i class="fa fa-check"></i><b>7.2</b> Confidence intervals for F(t)</a></li>
<li class="chapter" data-level="7.3" data-path="edf.html"><a href="edf.html#the-empirical-distribution-function-in-r"><i class="fa fa-check"></i><b>7.3</b> The Empirical Distribution Function in R</a></li>
<li class="chapter" data-level="7.4" data-path="edf.html"><a href="edf.html#the-kolmogorov-smirnov-test"><i class="fa fa-check"></i><b>7.4</b> The Kolmogorov-Smirnov Test</a></li>
<li class="chapter" data-level="7.5" data-path="edf.html"><a href="edf.html#the-empirical-distribution-function-and-statistical-functionals"><i class="fa fa-check"></i><b>7.5</b> The empirical distribution function and statistical functionals</a></li>
<li class="chapter" data-level="7.6" data-path="edf.html"><a href="edf.html#additional-reading-2"><i class="fa fa-check"></i><b>7.6</b> Additional Reading</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="density-estimation.html"><a href="density-estimation.html"><i class="fa fa-check"></i><b>8</b> Density Estimation</a><ul>
<li class="chapter" data-level="8.1" data-path="density-estimation.html"><a href="density-estimation.html#introduction"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="density-estimation.html"><a href="density-estimation.html#histograms"><i class="fa fa-check"></i><b>8.2</b> Histograms</a><ul>
<li class="chapter" data-level="8.2.1" data-path="density-estimation.html"><a href="density-estimation.html#definition-5"><i class="fa fa-check"></i><b>8.2.1</b> Definition</a></li>
<li class="chapter" data-level="8.2.2" data-path="density-estimation.html"><a href="density-estimation.html#histograms-in-r"><i class="fa fa-check"></i><b>8.2.2</b> Histograms in R</a></li>
<li class="chapter" data-level="8.2.3" data-path="density-estimation.html"><a href="density-estimation.html#performance-of-the-histogram-estimate-and-bin-width-selection"><i class="fa fa-check"></i><b>8.2.3</b> Performance of the Histogram Estimate and Bin Width Selection</a></li>
<li class="chapter" data-level="8.2.4" data-path="density-estimation.html"><a href="density-estimation.html#choosing-the-histogram-bin-width"><i class="fa fa-check"></i><b>8.2.4</b> Choosing the Histogram Bin Width</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="density-estimation.html"><a href="density-estimation.html#a-box-type-density-estimate"><i class="fa fa-check"></i><b>8.3</b> A Box-type Density Estimate</a></li>
<li class="chapter" data-level="8.4" data-path="density-estimation.html"><a href="density-estimation.html#kernel-density-estimation"><i class="fa fa-check"></i><b>8.4</b> Kernel Density Estimation</a><ul>
<li class="chapter" data-level="8.4.1" data-path="density-estimation.html"><a href="density-estimation.html#definition-6"><i class="fa fa-check"></i><b>8.4.1</b> Definition</a></li>
<li class="chapter" data-level="8.4.2" data-path="density-estimation.html"><a href="density-estimation.html#bias-variance-and-amise-of-kernel-density-estimates"><i class="fa fa-check"></i><b>8.4.2</b> Bias, Variance, and AMISE of Kernel Density Estimates</a></li>
<li class="chapter" data-level="8.4.3" data-path="density-estimation.html"><a href="density-estimation.html#bandwidth-selection-with-the-normal-reference-rule-and-silvermans-rule-of-thumb"><i class="fa fa-check"></i><b>8.4.3</b> Bandwidth Selection with the Normal Reference Rule and Silvermanâs âRule of Thumbâ</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="density-estimation.html"><a href="density-estimation.html#cross-validation-for-bandwidth-selection"><i class="fa fa-check"></i><b>8.5</b> Cross-Validation for Bandwidth Selection</a><ul>
<li class="chapter" data-level="8.5.1" data-path="density-estimation.html"><a href="density-estimation.html#squared-error-cross-validation"><i class="fa fa-check"></i><b>8.5.1</b> Squared-Error Cross-Validation</a></li>
<li class="chapter" data-level="8.5.2" data-path="density-estimation.html"><a href="density-estimation.html#computing-the-cross-validation-bandwidth"><i class="fa fa-check"></i><b>8.5.2</b> Computing the Cross-validation Bandwidth</a></li>
<li class="chapter" data-level="8.5.3" data-path="density-estimation.html"><a href="density-estimation.html#likelihood-cross-validation"><i class="fa fa-check"></i><b>8.5.3</b> Likelihood Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="density-estimation.html"><a href="density-estimation.html#density-estimation-in-r"><i class="fa fa-check"></i><b>8.6</b> Density Estimation in R</a></li>
<li class="chapter" data-level="8.7" data-path="density-estimation.html"><a href="density-estimation.html#additional-reading-3"><i class="fa fa-check"></i><b>8.7</b> Additional Reading</a></li>
</ul></li>
<li class="part"><span><b>III Quantifying Uncertainty</b></span></li>
<li class="chapter" data-level="9" data-path="bootstrap-main.html"><a href="bootstrap-main.html"><i class="fa fa-check"></i><b>9</b> The Bootstrap</a><ul>
<li class="chapter" data-level="9.1" data-path="bootstrap-main.html"><a href="bootstrap-main.html#introduction-1"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="bootstrap-main.html"><a href="bootstrap-main.html#description-of-the-bootstrap"><i class="fa fa-check"></i><b>9.2</b> Description of the Bootstrap</a><ul>
<li class="chapter" data-level="9.2.1" data-path="bootstrap-main.html"><a href="bootstrap-main.html#description"><i class="fa fa-check"></i><b>9.2.1</b> Description</a></li>
<li class="chapter" data-level="9.2.2" data-path="bootstrap-main.html"><a href="bootstrap-main.html#example-confidence-intervals-for-the-rate-parameter-of-an-exponential-distribution"><i class="fa fa-check"></i><b>9.2.2</b> Example: Confidence Intervals for the Rate Parameter of an Exponential Distribution</a></li>
<li class="chapter" data-level="9.2.3" data-path="bootstrap-main.html"><a href="bootstrap-main.html#example-confidence-intervals-for-the-ratio-of-two-quantiles"><i class="fa fa-check"></i><b>9.2.3</b> Example: Confidence Intervals for the Ratio of Two Quantiles</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="bootstrap-main.html"><a href="bootstrap-main.html#why-is-the-bootstrap-procedure-reasonable"><i class="fa fa-check"></i><b>9.3</b> Why is the Bootstrap Procedure Reasonable?</a></li>
<li class="chapter" data-level="9.4" data-path="bootstrap-main.html"><a href="bootstrap-main.html#pivotal-bootstrap-confidence-intervals"><i class="fa fa-check"></i><b>9.4</b> Pivotal Bootstrap Confidence Intervals</a></li>
<li class="chapter" data-level="9.5" data-path="bootstrap-main.html"><a href="bootstrap-main.html#the-parametric-bootstrap"><i class="fa fa-check"></i><b>9.5</b> The Parametric Bootstrap</a><ul>
<li class="chapter" data-level="9.5.1" data-path="bootstrap-main.html"><a href="bootstrap-main.html#parametric-bootstrap-for-the-median-age-from-the-kidney-data"><i class="fa fa-check"></i><b>9.5.1</b> Parametric Bootstrap for the Median Age from the Kidney Data</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="bootstrap-main.html"><a href="bootstrap-main.html#additional-reading-4"><i class="fa fa-check"></i><b>9.6</b> Additional Reading</a></li>
<li class="chapter" data-level="9.7" data-path="bootstrap-main.html"><a href="bootstrap-main.html#exercises-1"><i class="fa fa-check"></i><b>9.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ci.html"><a href="ci.html"><i class="fa fa-check"></i><b>10</b> Bootstrap Examples and the Jackknife</a><ul>
<li class="chapter" data-level="10.1" data-path="ci.html"><a href="ci.html#the-parametric-bootstrap-for-an-ar1-model"><i class="fa fa-check"></i><b>10.1</b> The Parametric Bootstrap for an AR(1) model</a></li>
<li class="chapter" data-level="10.2" data-path="ci.html"><a href="ci.html#using-the-bootstrap-in-regression"><i class="fa fa-check"></i><b>10.2</b> Using the Bootstrap in Regression</a><ul>
<li class="chapter" data-level="10.2.1" data-path="ci.html"><a href="ci.html#parametric-bootstrap-for-regression"><i class="fa fa-check"></i><b>10.2.1</b> Parametric Bootstrap for Regression</a></li>
<li class="chapter" data-level="10.2.2" data-path="ci.html"><a href="ci.html#nonparametric-bootstrap-for-regression"><i class="fa fa-check"></i><b>10.2.2</b> Nonparametric Bootstrap for Regression</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="ci.html"><a href="ci.html#pointwise-confidence-intervals-for-a-density-function"><i class="fa fa-check"></i><b>10.3</b> Pointwise Confidence Intervals for a Density Function</a></li>
<li class="chapter" data-level="10.4" data-path="ci.html"><a href="ci.html#when-can-the-bootstrap-fail"><i class="fa fa-check"></i><b>10.4</b> When can the Bootstrap Fail?</a><ul>
<li class="chapter" data-level="10.4.1" data-path="ci.html"><a href="ci.html#example-the-shifted-exponential-distribution"><i class="fa fa-check"></i><b>10.4.1</b> Example: The Shifted Exponential Distribution</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="ci.html"><a href="ci.html#the-jackknife"><i class="fa fa-check"></i><b>10.5</b> The Jackknife</a></li>
</ul></li>
<li class="part"><span><b>IV Nonparametric Regression: Part I</b></span></li>
<li class="chapter" data-level="11" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html"><i class="fa fa-check"></i><b>11</b> Kernel Regression and Local Regression</a><ul>
<li class="chapter" data-level="11.1" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#introduction-2"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#kernel-regression"><i class="fa fa-check"></i><b>11.2</b> Kernel Regression</a><ul>
<li class="chapter" data-level="11.2.1" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#the-regressogram"><i class="fa fa-check"></i><b>11.2.1</b> The Regressogram</a></li>
<li class="chapter" data-level="11.2.2" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#the-local-average-estimator"><i class="fa fa-check"></i><b>11.2.2</b> The Local Average Estimator</a></li>
<li class="chapter" data-level="11.2.3" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#k-nearest-neighbor-k-nn-regression"><i class="fa fa-check"></i><b>11.2.3</b> k-Nearest Neighbor (k-NN) Regression</a></li>
<li class="chapter" data-level="11.2.4" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#the-nadaraya-watson-estimator"><i class="fa fa-check"></i><b>11.2.4</b> The Nadaraya-Watson Estimator</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#local-linear-regression"><i class="fa fa-check"></i><b>11.3</b> Local Linear Regression</a><ul>
<li class="chapter" data-level="11.3.1" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#definition-7"><i class="fa fa-check"></i><b>11.3.1</b> Definition</a></li>
<li class="chapter" data-level="11.3.2" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#advantages-of-the-local-linear-estimator"><i class="fa fa-check"></i><b>11.3.2</b> Advantages of the Local Linear Estimator</a></li>
<li class="chapter" data-level="11.3.3" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#an-example-in-r"><i class="fa fa-check"></i><b>11.3.3</b> An Example in R</a></li>
<li class="chapter" data-level="11.3.4" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#local-polynomial-regression"><i class="fa fa-check"></i><b>11.3.4</b> Local Polynomial Regression</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#selecting-the-bandwidthsmoothing-parameter"><i class="fa fa-check"></i><b>11.4</b> Selecting the Bandwidth/Smoothing Parameter</a><ul>
<li class="chapter" data-level="11.4.1" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#representing-in-linear-form"><i class="fa fa-check"></i><b>11.4.1</b> Representing in Linear Form</a></li>
<li class="chapter" data-level="11.4.2" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#the-cp-statistic"><i class="fa fa-check"></i><b>11.4.2</b> The Cp Statistic</a></li>
<li class="chapter" data-level="11.4.3" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>11.4.3</b> Leave-one-out Cross Validation</a></li>
<li class="chapter" data-level="11.4.4" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#example-choosing-the-best-bin-width-for-the-local-average-estimator."><i class="fa fa-check"></i><b>11.4.4</b> Example: Choosing the Best Bin Width for the Local Average Estimator.</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#additional-reading-5"><i class="fa fa-check"></i><b>11.5</b> Additional Reading</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="inference-for-regression.html"><a href="inference-for-regression.html"><i class="fa fa-check"></i><b>12</b> Splines and Penalized Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="inference-for-regression.html"><a href="inference-for-regression.html#introduction-3"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="inference-for-regression.html"><a href="inference-for-regression.html#spline-basis-functions"><i class="fa fa-check"></i><b>12.2</b> Spline Basis Functions</a></li>
<li class="chapter" data-level="12.3" data-path="inference-for-regression.html"><a href="inference-for-regression.html#smoothing-splinespenalized-regression"><i class="fa fa-check"></i><b>12.3</b> Smoothing Splines/Penalized Regression</a><ul>
<li class="chapter" data-level="12.3.1" data-path="inference-for-regression.html"><a href="inference-for-regression.html#selection-of-smoothing-parameter"><i class="fa fa-check"></i><b>12.3.1</b> Selection of Smoothing Parameter</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>V Nonparametric Regression: Part II</b></span></li>
<li class="chapter" data-level="13" data-path="decision-tree.html"><a href="decision-tree.html"><i class="fa fa-check"></i><b>13</b> Decision Trees and CART</a></li>
<li class="chapter" data-level="14" data-path="ensemble.html"><a href="ensemble.html"><i class="fa fa-check"></i><b>14</b> Ensemble Methods for Prediction</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Elements of Nonparametric Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ci" class="section level1">
<h1><span class="header-section-number">Chapter 10</span> Bootstrap Examples and the Jackknife</h1>
<div id="the-parametric-bootstrap-for-an-ar1-model" class="section level2">
<h2><span class="header-section-number">10.1</span> The Parametric Bootstrap for an AR(1) model</h2>
<ul>
<li><p>Consider the time series <span class="math inline">\(X_{1}, X_{2}, \ldots, X_{m}\)</span>. Here,
<span class="math inline">\(X_{t}\)</span> denotes an observation made at time <span class="math inline">\(t\)</span>.</p></li>
<li><p>An autoregressive model of order 1 (usually called an AR(1) model) for this time series is
<span class="math display">\[\begin{eqnarray}
X_{1} &amp;=&amp; \frac{c_{0}}{1 - \alpha} + \varepsilon_{1} \nonumber \\
X_{t} &amp;=&amp; c_{0} + \alpha X_{t-1} + \varepsilon_{t}, \qquad t=2,\ldots,m. \nonumber
\end{eqnarray}\]</span></p></li>
<li><p>It is usually assumed that <span class="math inline">\(|\alpha| &lt; 1\)</span>.</p></li>
<li>In the AR(1) model, it is assumed that
<ul>
<li><span class="math inline">\(E(\varepsilon_{t}) = 0\)</span></li>
<li><span class="math inline">\(\textrm{Var}(\varepsilon_{t}) = \sigma^{2}\)</span>,</li>
<li><span class="math inline">\(\varepsilon_{2}, \ldots, \varepsilon_{m}\)</span> are i.i.d.</li>
<li><span class="math inline">\(\varepsilon_{t}\)</span> and <span class="math inline">\(X_{t-1}\)</span> are independent.</li>
</ul></li>
<li><p>In addition to these assumptions, we will assume that
<span class="math display">\[\begin{equation}
\varepsilon_{t} \sim \textrm{Normal}(0, \sigma^{2})  \nonumber 
\end{equation}\]</span></p></li>
<li><p>The AR(1) model implies that
<span class="math display">\[\begin{equation}
\textrm{Corr}(X_{t}, X_{t-1}) = \alpha  \nonumber 
\end{equation}\]</span>
and, more generally, that
<span class="math display">\[\begin{equation}
\textrm{Corr}(X_{t}, X_{t-p}) = \alpha^{p}  \nonumber
\end{equation}\]</span></p></li>
</ul>
<hr />
<ul>
<li>For known values of <span class="math inline">\(c_{0}, \alpha\)</span>, and <span class="math inline">\(\sigma^{2}\)</span>, we can simulate
an AR(1) time series with the following <code>R</code> code:</li>
</ul>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb186-1" data-line-number="1">SimulateParAR1 &lt;-<span class="st"> </span><span class="cf">function</span>(m, c0, alpha, sig.sq) {</a>
<a class="sourceLine" id="cb186-2" data-line-number="2">     xx &lt;-<span class="st"> </span><span class="kw">numeric</span>(m)</a>
<a class="sourceLine" id="cb186-3" data-line-number="3">     xx[<span class="dv">1</span>] &lt;-<span class="st"> </span>c0<span class="op">/</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>alpha) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, <span class="dt">sd=</span><span class="kw">sqrt</span>(sig.sq))</a>
<a class="sourceLine" id="cb186-4" data-line-number="4">     <span class="cf">for</span>(t <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>m) { </a>
<a class="sourceLine" id="cb186-5" data-line-number="5">         xx[t] &lt;-<span class="st"> </span>c0 <span class="op">+</span><span class="st"> </span>alpha<span class="op">*</span>xx[t<span class="dv">-1</span>] <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, <span class="dt">sd=</span><span class="kw">sqrt</span>(sig.sq))</a>
<a class="sourceLine" id="cb186-6" data-line-number="6">     }</a>
<a class="sourceLine" id="cb186-7" data-line-number="7">     <span class="kw">return</span>(xx)</a>
<a class="sourceLine" id="cb186-8" data-line-number="8">}</a></code></pre></div>
<p><img src="10-confidence-intervals_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<ul>
<li>In <code>R</code>, estimates of <span class="math inline">\(c_{0}, \alpha,\)</span> and <span class="math inline">\(\sigma^{2}\)</span> can be found by using the <code>ar</code> function. For example,</li>
</ul>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb187-1" data-line-number="1">x &lt;-<span class="st"> </span><span class="kw">SimulateParAR1</span>(<span class="dv">1000</span>, <span class="dv">1</span>, <span class="fl">0.8</span>, <span class="dt">sig.sq=</span>.<span class="dv">25</span>)</a>
<a class="sourceLine" id="cb187-2" data-line-number="2">ar1.fit &lt;-<span class="st"> </span><span class="kw">ar</span>(x, <span class="dt">aic=</span><span class="ot">FALSE</span>, <span class="dt">order.max =</span> <span class="dv">1</span>, <span class="dt">method=</span><span class="st">&quot;mle&quot;</span>)</a>
<a class="sourceLine" id="cb187-3" data-line-number="3"></a>
<a class="sourceLine" id="cb187-4" data-line-number="4">c0.est &lt;-<span class="st"> </span>ar1.fit<span class="op">$</span>x.mean<span class="op">*</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>ar1.fit<span class="op">$</span>ar)</a>
<a class="sourceLine" id="cb187-5" data-line-number="5">alpha.est &lt;-<span class="st"> </span>ar1.fit<span class="op">$</span>ar</a>
<a class="sourceLine" id="cb187-6" data-line-number="6">sigsq.est &lt;-<span class="st"> </span>ar1.fit<span class="op">$</span>var.pred</a></code></pre></div>
<hr />
<ul>
<li><p>Suppose we want to construct confidence intervals for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\sigma\)</span> using a bootstrap method.</p></li>
<li><p>Using the direct, nonparametric bootstrap described in the previous chapter will not work
because our observations are not independent. There are âblock bootstrapsâ that
are designed to work for time series, but we will not discuss those here (see e.g., <span class="citation">BÃ¼hlmann (<a href="#ref-buhlmann2002">2002</a>)</span> or Chapter 8 of <span class="citation">Davison and Hinkley (<a href="#ref-davison1997">1997</a>)</span> for
more details).</p></li>
<li><p>With the parametric bootstrap, we only have to use the following steps to generate bootstrap replications
<span class="math inline">\(\hat{\alpha}_{r}^{*}\)</span> and <span class="math inline">\(\hat{\sigma}_{r}^{2,*}\)</span> for estimates of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\hat{\sigma}^{2}\)</span>.</p></li>
<li>For <span class="math inline">\(r = 1, \ldots, R\)</span>:
<ul>
<li>Simulate a time series <span class="math inline">\(X_{1}^{*}, \ldots, X_{m}^{*}\)</span> from an AR(1) model with parameters <span class="math inline">\((\hat{c}_{0}, \hat{\alpha}, \hat{\sigma}^{2})\)</span>.</li>
<li>Compute <span class="math inline">\(\hat{\alpha}_{r}^{*} = \hat{\alpha}(X_{1}^{*}, \ldots, X_{m}^{*})\)</span>.</li>
<li>Compute <span class="math inline">\(\hat{\sigma}_{r}^{2,*} = \hat{\sigma}^{2}(X_{1}^{*}, \ldots, X_{m}^{*})\)</span></li>
</ul></li>
</ul>
<hr />
<ul>
<li>To see how this parametric bootstrap works, we will use the <code>nhtemp</code> dataset that is available in <code>R</code>.</li>
</ul>
<p><img src="10-confidence-intervals_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<ul>
<li>The <code>nhtemp</code> dataset contains the mean annual temperature in New Haven, Connecticut from the years 1912-1971</li>
</ul>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb188-1" data-line-number="1"><span class="kw">head</span>(nhtemp)</a></code></pre></div>
<pre><code>## [1] 49.9 52.3 49.4 51.1 49.4 47.9</code></pre>
<ul>
<li>The estimated autocorrelation parameter <span class="math inline">\(\alpha\)</span> is about <span class="math inline">\(0.31\)</span> for this data</li>
</ul>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb190-1" data-line-number="1">ar1.temp &lt;-<span class="st"> </span><span class="kw">ar</span>(nhtemp, <span class="dt">aic=</span><span class="ot">FALSE</span>, <span class="dt">order.max =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb190-2" data-line-number="2">c0.hat &lt;-<span class="st"> </span>ar1.temp<span class="op">$</span>x.mean<span class="op">*</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>ar1.temp<span class="op">$</span>ar)</a>
<a class="sourceLine" id="cb190-3" data-line-number="3">alpha.hat &lt;-<span class="st"> </span>ar1.temp<span class="op">$</span>ar</a>
<a class="sourceLine" id="cb190-4" data-line-number="4">sigsq.hat &lt;-<span class="st"> </span>ar1.temp<span class="op">$</span>var.pred</a>
<a class="sourceLine" id="cb190-5" data-line-number="5">alpha.hat</a></code></pre></div>
<pre><code>## [1] 0.3148269</code></pre>
<ul>
<li>Now, that we have estimated all the parameter of the AR(1) model, we can run our parametric bootstrap for <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\sigma}\)</span>:</li>
</ul>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb192-1" data-line-number="1">R &lt;-<span class="st"> </span><span class="dv">500</span></a>
<a class="sourceLine" id="cb192-2" data-line-number="2">alpha.boot &lt;-<span class="st"> </span><span class="kw">numeric</span>(R)</a>
<a class="sourceLine" id="cb192-3" data-line-number="3">sigsq.boot &lt;-<span class="st"> </span><span class="kw">numeric</span>(R)</a>
<a class="sourceLine" id="cb192-4" data-line-number="4"><span class="cf">for</span>(r <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>R) {</a>
<a class="sourceLine" id="cb192-5" data-line-number="5">  x &lt;-<span class="st"> </span><span class="kw">SimulateParAR1</span>(<span class="dv">60</span>, <span class="dt">c0=</span>c0.hat, <span class="dt">alpha=</span>alpha.hat, <span class="dt">sig.sq=</span>sigsq.hat)</a>
<a class="sourceLine" id="cb192-6" data-line-number="6">  ar1.fit &lt;-<span class="st"> </span><span class="kw">ar</span>(x, <span class="dt">aic=</span><span class="ot">FALSE</span>, <span class="dt">order.max =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb192-7" data-line-number="7">  </a>
<a class="sourceLine" id="cb192-8" data-line-number="8">  alpha.boot[r] &lt;-<span class="st"> </span>ar1.fit<span class="op">$</span>ar</a>
<a class="sourceLine" id="cb192-9" data-line-number="9">  sigsq.boot[r] &lt;-<span class="st"> </span>ar1.fit<span class="op">$</span>var.pred</a>
<a class="sourceLine" id="cb192-10" data-line-number="10">}</a></code></pre></div>
<ul>
<li>Normal bootstrap standard error confidence intervals for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\sigma^{2}\)</span> are</li>
</ul>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb193-1" data-line-number="1"><span class="kw">round</span>(<span class="kw">c</span>(alpha.hat <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span><span class="op">*</span><span class="kw">sd</span>(alpha.boot), alpha.hat <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span><span class="op">*</span><span class="kw">sd</span>(alpha.boot)), <span class="dv">3</span>)</a></code></pre></div>
<pre><code>## [1] 0.085 0.545</code></pre>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb195-1" data-line-number="1"><span class="kw">round</span>(<span class="kw">c</span>(sigsq.hat <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span><span class="op">*</span><span class="kw">sd</span>(sigsq.boot), sigsq.hat <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span><span class="op">*</span><span class="kw">sd</span>(sigsq.boot)), <span class="dv">3</span>)</a></code></pre></div>
<pre><code>## [1] 0.979 1.957</code></pre>
<ul>
<li>We can compare our confidence interval for <span class="math inline">\(\alpha\)</span> with the confidence interval
obtained from using a large-sample approximation:</li>
</ul>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb197-1" data-line-number="1">asymp.se &lt;-<span class="st"> </span><span class="kw">sqrt</span>(ar1.temp<span class="op">$</span>asy.var.coef)</a>
<a class="sourceLine" id="cb197-2" data-line-number="2"><span class="kw">round</span>(<span class="kw">c</span>(alpha.hat <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span><span class="op">*</span>asymp.se, alpha.hat <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span><span class="op">*</span>asymp.se), <span class="dv">3</span>)</a></code></pre></div>
<pre><code>## [1] 0.071 0.559</code></pre>
</div>
<div id="using-the-bootstrap-in-regression" class="section level2">
<h2><span class="header-section-number">10.2</span> Using the Bootstrap in Regression</h2>
<ul>
<li>In linear regression with a single, univariate covariate, we work with the following model
<span class="math display">\[\begin{equation}
Y_{i} = \beta_{0} + \beta_{1}x_{i} + \varepsilon_{i}, \qquad i = 1, \ldots, n.  \nonumber 
\end{equation}\]</span>
<ul>
<li><span class="math inline">\(Y_{i}\)</span> - the responses</li>
<li><span class="math inline">\(x_{i}\)</span> - the covariates</li>
<li><span class="math inline">\(\beta_{0}, \beta_{1}\)</span> - the regression coefficients</li>
<li><span class="math inline">\(\varepsilon_{i}\)</span> - the residuals</li>
</ul></li>
<li><p>Typically, confidence intervals for the regression coefficients <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>
are constructed under the assumption that <span class="math inline">\(\varepsilon_{i} \sim \textrm{Normal}(0, \sigma^{2})\)</span>.</p></li>
<li><p>The bootstrap allows us to compute confidence intervals for <span class="math inline">\((\beta_{0}, \beta_{1})\)</span> without
relying on this normality assumption.</p></li>
<li><p>How to compute bootstrap confidence intervals for <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>?</p></li>
</ul>
<hr />
<ul>
<li><p>The least-squares estimates of <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> are
<span class="math display">\[\begin{equation}
\hat{\beta}_{0} = \bar{y} - \hat{\beta}_{1}\bar{x} \qquad \qquad \hat{\beta}_{1} = \frac{\sum_{i=1}^{n}(x_{i} - \bar{x})(y_{i} - \bar{y})}{S_{xx}}  \nonumber
\end{equation}\]</span>
where <span class="math inline">\(S_{xx} = \sum_{i=1}^{n}( x_{i} - \bar{x})^{2}\)</span>.</p></li>
<li><p>Assuming the covariates are fixed design points, the variance of <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span> are
<span class="math display" id="eq:stderr-regression-formulas">\[\begin{equation}
\textrm{Var}(\hat{\beta}_{0}) = \sigma^{2}\Big(\frac{\tfrac{1}{n}\sum_{i=1}^{n} x_{i}^{2}}{S_{xx}} \Big) \qquad \textrm{Var}(\hat{\beta}_{1}) = \frac{\sigma^{2}}{S_{xx}} 
\tag{10.1}
\end{equation}\]</span></p></li>
</ul>
<div id="parametric-bootstrap-for-regression" class="section level3">
<h3><span class="header-section-number">10.2.1</span> Parametric Bootstrap for Regression</h3>
<ul>
<li>With a parametric bootstrap, we will simulate outcomes <span class="math inline">\(Y_{i}\)</span> from the model
<span class="math display">\[\begin{equation}
Y_{i} = \hat{\beta}_{0} + \hat{\beta}_{1}x_{i} + \varepsilon_{i},  \nonumber
\end{equation}\]</span>
<ul>
<li><span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span> are the least-squares estimates of <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>,</li>
<li><span class="math inline">\(\varepsilon_{1}, \ldots, \varepsilon_{n}\)</span> are i.i.d. random variables with mean zero and variance <span class="math inline">\(\hat{\sigma}^{2}\)</span>.</li>
</ul></li>
<li><p>It is most common to assume that <span class="math inline">\(\varepsilon_{i} \sim \textrm{Normal}(0, \hat{\sigma}^{2})\)</span>,
where <span class="math inline">\(\hat{\sigma}^{2}\)</span> is an estimate of the residual variance.</p></li>
<li><p>However, we could easily use an alternative parametric model for <span class="math inline">\(\varepsilon_{i}\)</span> if we thought
it was appropriate.</p></li>
</ul>
<hr />
<ul>
<li><p>A t-distribution with a small number of degrees of freedom can be useful
when the residuals are thought to follow a distribution with âheavier tailsâ.</p></li>
<li><p>If we assume <span class="math inline">\(\varepsilon_{i} \sim \sigma \times t_{3}\)</span>, then <span class="math inline">\(\textrm{Var}(\varepsilon_{i}) = 3\sigma^{2}\)</span>.</p></li>
<li><p>So, with a <span class="math inline">\(t_{3}\)</span> residual distribution we want to simulate from the model
<span class="math display">\[\begin{equation}
Y_{i} = \hat{\beta}_{0} + \hat{\beta}_{1}x_{i} + \frac{\hat{\sigma}}{\sqrt{3}}u_{i},  \qquad u_{i} \sim t_{3}, \nonumber
\end{equation}\]</span>
where <span class="math inline">\(\hat{\sigma}^{2}\)</span> is the following estimate of the residual variance:
<span class="math display">\[\begin{equation}
\hat{\sigma}^{2} = \frac{1}{n-2}\sum_{i=1}^{n} (Y_{i} - \hat{\beta}_{0} - \hat{\beta}_{1})^{2} \nonumber
\end{equation}\]</span></p></li>
</ul>
<hr />
<ul>
<li><p>To show how this parametric-t bootstrap works in practice we will look
at the kidney function data.</p></li>
<li><p>We will look at a linear regression where the measure of kidney function is
the outcome and age is the covariate.</p></li>
</ul>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb199-1" data-line-number="1">kidney &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&quot;https://web.stanford.edu/~hastie/CASI_files/DATA/kidney.txt&quot;</span>, </a>
<a class="sourceLine" id="cb199-2" data-line-number="2">                     <span class="dt">header=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<p><img src="10-confidence-intervals_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<ul>
<li>Bootstrap replications of <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span> can
be computed using the following <code>R</code> code:</li>
</ul>
<div class="sourceCode" id="cb200"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb200-1" data-line-number="1"><span class="co">## First find the parameter estimates</span></a>
<a class="sourceLine" id="cb200-2" data-line-number="2">lm.kidney &lt;-<span class="st"> </span><span class="kw">lm</span>(tot <span class="op">~</span><span class="st"> </span>age, <span class="dt">data=</span>kidney)</a>
<a class="sourceLine" id="cb200-3" data-line-number="3">beta0.hat &lt;-<span class="st"> </span>lm.kidney<span class="op">$</span>coef[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb200-4" data-line-number="4">beta1.hat &lt;-<span class="st"> </span>lm.kidney<span class="op">$</span>coef[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb200-5" data-line-number="5">sigsq.hat &lt;-<span class="st"> </span><span class="kw">sum</span>(lm.kidney<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span>(<span class="dv">157</span> <span class="op">-</span><span class="st"> </span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb200-6" data-line-number="6"></a>
<a class="sourceLine" id="cb200-7" data-line-number="7"><span class="co">## Using these estimates, run a parametric bootstrap to generate</span></a>
<a class="sourceLine" id="cb200-8" data-line-number="8"><span class="co">## bootstrap replications of beta0.hat and beta1.hat</span></a>
<a class="sourceLine" id="cb200-9" data-line-number="9">R &lt;-<span class="st"> </span><span class="dv">500</span></a>
<a class="sourceLine" id="cb200-10" data-line-number="10">beta0.boot &lt;-<span class="st"> </span><span class="kw">numeric</span>(R)</a>
<a class="sourceLine" id="cb200-11" data-line-number="11">beta1.boot &lt;-<span class="st"> </span><span class="kw">numeric</span>(R)</a>
<a class="sourceLine" id="cb200-12" data-line-number="12">se.beta0.boot &lt;-<span class="st"> </span><span class="kw">numeric</span>(R)</a>
<a class="sourceLine" id="cb200-13" data-line-number="13">se.beta1.boot &lt;-<span class="st"> </span><span class="kw">numeric</span>(R)</a>
<a class="sourceLine" id="cb200-14" data-line-number="14"><span class="cf">for</span>(r <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>R) {</a>
<a class="sourceLine" id="cb200-15" data-line-number="15">  ysim &lt;-<span class="st"> </span>beta0.hat <span class="op">+</span><span class="st"> </span>beta1.hat<span class="op">*</span>kidney<span class="op">$</span>age <span class="op">+</span><span class="st"> </span><span class="kw">sqrt</span>(sigsq.hat<span class="op">/</span><span class="dv">3</span>)<span class="op">*</span><span class="kw">rt</span>(<span class="dv">157</span>, <span class="dt">df=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb200-16" data-line-number="16">  lm.boot &lt;-<span class="st"> </span><span class="kw">lm</span>(ysim <span class="op">~</span><span class="st"> </span>kidney<span class="op">$</span>age)</a>
<a class="sourceLine" id="cb200-17" data-line-number="17">  </a>
<a class="sourceLine" id="cb200-18" data-line-number="18">  beta0.boot[r] &lt;-<span class="st"> </span>lm.boot<span class="op">$</span>coef[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb200-19" data-line-number="19">  beta1.boot[r] &lt;-<span class="st"> </span>lm.boot<span class="op">$</span>coef[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb200-20" data-line-number="20">  </a>
<a class="sourceLine" id="cb200-21" data-line-number="21">  <span class="co">## This code can be used to find the standard errors from this bootstrap sample</span></a>
<a class="sourceLine" id="cb200-22" data-line-number="22">  sig.hatr &lt;-<span class="st"> </span><span class="kw">summary</span>(lm.boot)<span class="op">$</span>sigma</a>
<a class="sourceLine" id="cb200-23" data-line-number="23">  se.beta0.boot[r] &lt;-<span class="st"> </span>sig.hatr<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">summary</span>(lm.boot)<span class="op">$</span>cov.unscaled[<span class="dv">1</span>,<span class="dv">1</span>])</a>
<a class="sourceLine" id="cb200-24" data-line-number="24">  se.beta1.boot[r] &lt;-<span class="st"> </span>sig.hatr<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">summary</span>(lm.boot)<span class="op">$</span>cov.unscaled[<span class="dv">2</span>,<span class="dv">2</span>])</a>
<a class="sourceLine" id="cb200-25" data-line-number="25">}</a></code></pre></div>
<hr />
<ul>
<li><p>Because we have the formulas for the standard errors of <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>,
we can use studentized bootstrap confidence intervals without using the double bootstrap approach.</p></li>
<li><p>Estimates of the standard error for the <span class="math inline">\(r^{th}\)</span> bootstrap replication are
<span class="math display">\[\begin{eqnarray}
\hat{se}_{r}(\beta_{0}) &amp;=&amp; \hat{\sigma}_{r}\sqrt{\frac{\tfrac{1}{n}\sum_{i=1}^{n} x_{i}^{2}}{S_{xx}}} \nonumber \\
\hat{se}_{r}(\beta_{1}) &amp;=&amp; \hat{\sigma}_{r}/\sqrt{S_{xx}}
\end{eqnarray}\]</span></p></li>
<li><p>These standard error estimates come from applying the formulas in <a href="ci.html#eq:stderr-regression-formulas">(10.1)</a> to the <span class="math inline">\(r^{th}\)</span> bootstrap sample.</p></li>
<li><p>Recall from Chapter 9 that the studentized confidence intervals are found by using the following formula.
<span class="math display">\[\begin{equation}
\Big[ T_{n} - se_{boot} \times \hat{K}_{R}^{-1}(1 - \alpha/2), T_{n} - se_{boot} \times \hat{K}_{R}^{-1}(\alpha/2) \Big] \nonumber
\end{equation}\]</span></p></li>
</ul>
<hr />
<ul>
<li><code>R</code> code to compute the studentized confidence intervals is given below:</li>
</ul>
<div class="sourceCode" id="cb201"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb201-1" data-line-number="1"><span class="co">## First get estimates of the standard error of our estimates</span></a>
<a class="sourceLine" id="cb201-2" data-line-number="2"><span class="co">## I use the formulas for the regression standard errors, but</span></a>
<a class="sourceLine" id="cb201-3" data-line-number="3"><span class="co">## we could have used a bootstrap estimate.</span></a>
<a class="sourceLine" id="cb201-4" data-line-number="4">se.est0 &lt;-<span class="st"> </span><span class="kw">summary</span>(lm.kidney)<span class="op">$</span>sigma<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">summary</span>(lm.boot)<span class="op">$</span>cov.unscaled[<span class="dv">1</span>,<span class="dv">1</span>])</a>
<a class="sourceLine" id="cb201-5" data-line-number="5">se.est1 &lt;-<span class="st"> </span><span class="kw">summary</span>(lm.kidney)<span class="op">$</span>sigma<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">summary</span>(lm.boot)<span class="op">$</span>cov.unscaled[<span class="dv">2</span>,<span class="dv">2</span>])</a>
<a class="sourceLine" id="cb201-6" data-line-number="6"></a>
<a class="sourceLine" id="cb201-7" data-line-number="7">stu.quants0 &lt;-<span class="st"> </span><span class="kw">quantile</span>( (beta0.boot <span class="op">-</span><span class="st"> </span>beta0.hat)<span class="op">/</span>se.beta0.boot, <span class="dt">probs=</span><span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))</a>
<a class="sourceLine" id="cb201-8" data-line-number="8">stu.quants1 &lt;-<span class="st"> </span><span class="kw">quantile</span>( (beta1.boot <span class="op">-</span><span class="st"> </span>beta1.hat)<span class="op">/</span>se.beta1.boot, <span class="dt">probs=</span><span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))</a></code></pre></div>
<ul>
<li>The studentized bootstrap confidence intervals are then</li>
</ul>
<div class="sourceCode" id="cb202"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb202-1" data-line-number="1"><span class="co">## Confidence interval for beta0</span></a>
<a class="sourceLine" id="cb202-2" data-line-number="2"><span class="kw">c</span>(beta0.hat <span class="op">-</span><span class="st"> </span>stu.quants0[<span class="dv">2</span>]<span class="op">*</span>se.est0, beta0.hat <span class="op">-</span><span class="st"> </span>stu.quants0[<span class="dv">1</span>]<span class="op">*</span>se.est0)</a></code></pre></div>
<pre><code>## (Intercept) (Intercept) 
##        2.22        3.54</code></pre>
<div class="sourceCode" id="cb204"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb204-1" data-line-number="1"><span class="co">## Confidence interval for beta1</span></a>
<a class="sourceLine" id="cb204-2" data-line-number="2"><span class="kw">c</span>(beta1.hat <span class="op">-</span><span class="st"> </span>stu.quants1[<span class="dv">2</span>]<span class="op">*</span>se.est1, beta1.hat <span class="op">-</span><span class="st"> </span>stu.quants1[<span class="dv">1</span>]<span class="op">*</span>se.est1)</a></code></pre></div>
<pre><code>##     age     age 
## -0.0954 -0.0621</code></pre>
<ul>
<li>Compare these studentized bootstrap confidence intervals with the confidence
intervals computed under the normality assumption for the residuals:</li>
</ul>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb206-1" data-line-number="1"><span class="kw">confint</span>(lm.kidney)</a></code></pre></div>
<pre><code>##               2.5 %  97.5 %
## (Intercept)  2.1497  3.5703
## age         -0.0965 -0.0607</code></pre>
<hr />
<ul>
<li><strong>Exercise 10.1</strong> Using the parametric bootstrap, compute studentized bootstrap confidence for <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>
in the kidney data example. This time, assume that <span class="math inline">\(\varepsilon_{i} \sim \textrm{Normal}(0, \hat{\sigma}^{2})\)</span>.</li>
</ul>
<hr />
</div>
<div id="nonparametric-bootstrap-for-regression" class="section level3">
<h3><span class="header-section-number">10.2.2</span> Nonparametric Bootstrap for Regression</h3>
<ul>
<li><p>If we think of the <span class="math inline">\(x_{i}\)</span> as fixed values, the <span class="math inline">\(Y_{i}\)</span> in a linear regression are not i.i.d.
because the means are not the same.</p></li>
<li><p>This suggests that we cannot use the usual nonparametric bootstrap to construct confidence
intervals for <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>.</p></li>
<li><p>However, if we also view the <span class="math inline">\(x_{i}\)</span> as random, we can view the <strong>pairs</strong> of
observations <span class="math inline">\((Y_{1}, x_{1}), \ldots, (Y_{n}, x_{n})\)</span> as i.i.d. observations
from a bivariate distribution.</p></li>
<li><p>In this case, you can also think of <span class="math inline">\(\hat{\beta}_{1}\)</span> as an
estimate of the following quantity
<span class="math display">\[\begin{equation}
\rho_{YX}\frac{\sigma_{y}}{\sigma_{x}} \nonumber
\end{equation}\]</span>
where <span class="math inline">\(\rho_{YX} = \textrm{Corr}(Y_{i}, x_{i})\)</span>.</p></li>
<li><p>In the case when <span class="math inline">\((Y_{i}, x_{i})\)</span> are bivariate normal, the conditional expectation
of <span class="math inline">\(Y_{i}\)</span> given <span class="math inline">\(x_{i}\)</span> has the linear regression structure:
<span class="math display">\[\begin{equation}
E(Y_{i}| x_{i}) = \beta_{0} + \beta_{1}x_{i}, \nonumber
\end{equation}\]</span>
where <span class="math inline">\(\beta_{0} = \mu_{y} - \rho_{YX}\frac{\sigma_{y}\mu_{x}}{\sigma_{x}}\)</span>
and <span class="math inline">\(\beta_{1} = \rho_{YX}\frac{\sigma_{y}}{\sigma_{x}}\)</span>.</p></li>
<li><p>So, even if the linear model is not exactly true, our estimate and
confidence interval still has a clear interpretation.</p></li>
<li><p>If the linear model assumption is true, the true standard error of <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span>
will be slightly different than the formulas shown in <a href="ci.html#eq:stderr-regression-formulas">(10.1)</a>. Nevertheless,
<a href="ci.html#eq:stderr-regression-formulas">(10.1)</a> can be thought of as consistent estimates of the true standard error.</p></li>
</ul>
<hr />
<ul>
<li><p>If we are thinking of the observations <span class="math inline">\((Y_{1}, x_{1}), \ldots, (Y_{n}, x_{n})\)</span>
as i.i.d. pairs, we can use the nonparametric bootstrap by just subsampling pairs
of observations.</p></li>
<li>So, to generate bootstrap replications <span class="math inline">\(\hat{\beta}_{0,r}^{*}\)</span>, <span class="math inline">\(\hat{\beta}_{1, r}^{*}\)</span> for <span class="math inline">\(\hat{\beta}_{0}\)</span>
and <span class="math inline">\(\hat{\beta}_{1}\)</span>, we just use the following procedure</li>
<li>For <span class="math inline">\(r = 1, \ldots, R\)</span>:
<ul>
<li>Draw a sample of size <span class="math inline">\(n\)</span>: <span class="math inline">\(\big((Y_{1}^{*}, x_{1}^{*}), \ldots, (Y_{n}^{*}, x_{n}^{*}) \big)\)</span> by sampling with replacement from the original data.</li>
<li>Compute <span class="math inline">\(\hat{\beta}_{0,r}^{*}\)</span> and <span class="math inline">\(\hat{\beta}_{1,r}^{*}\)</span> from this bootstrap sample.</li>
</ul></li>
<li><p><code>R</code> code for generating these bootstrap replications for the <code>kidney</code> data is below:</p></li>
</ul>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb208-1" data-line-number="1">R &lt;-<span class="st"> </span><span class="dv">500</span></a>
<a class="sourceLine" id="cb208-2" data-line-number="2">beta0.boot.np &lt;-<span class="st"> </span><span class="kw">numeric</span>(R)</a>
<a class="sourceLine" id="cb208-3" data-line-number="3">beta1.boot.np &lt;-<span class="st"> </span><span class="kw">numeric</span>(R)</a>
<a class="sourceLine" id="cb208-4" data-line-number="4">se.beta0.boot.np &lt;-<span class="st"> </span><span class="kw">numeric</span>(R)</a>
<a class="sourceLine" id="cb208-5" data-line-number="5">se.beta1.boot.np &lt;-<span class="st"> </span><span class="kw">numeric</span>(R)</a>
<a class="sourceLine" id="cb208-6" data-line-number="6"><span class="cf">for</span>(r <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>R) {</a>
<a class="sourceLine" id="cb208-7" data-line-number="7">  subsamp.ind &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">157</span>, <span class="dt">size=</span><span class="dv">157</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb208-8" data-line-number="8">  kidney.tmp &lt;-<span class="st"> </span>kidney[subsamp.ind,]</a>
<a class="sourceLine" id="cb208-9" data-line-number="9">  lm.boot &lt;-<span class="st"> </span><span class="kw">lm</span>(tot <span class="op">~</span><span class="st"> </span>age, <span class="dt">data=</span>kidney.tmp)</a>
<a class="sourceLine" id="cb208-10" data-line-number="10">  </a>
<a class="sourceLine" id="cb208-11" data-line-number="11">  beta0.boot.np[r] &lt;-<span class="st"> </span>lm.boot<span class="op">$</span>coef[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb208-12" data-line-number="12">  beta1.boot.np[r] &lt;-<span class="st"> </span>lm.boot<span class="op">$</span>coef[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb208-13" data-line-number="13">  </a>
<a class="sourceLine" id="cb208-14" data-line-number="14">  <span class="co">## This code can be used to find the standard errors from this bootstrap sample</span></a>
<a class="sourceLine" id="cb208-15" data-line-number="15">  sig.hatr &lt;-<span class="st"> </span><span class="kw">summary</span>(lm.boot)<span class="op">$</span>sigma</a>
<a class="sourceLine" id="cb208-16" data-line-number="16">  se.beta0.boot.np[r] &lt;-<span class="st"> </span>sig.hatr<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">summary</span>(lm.boot)<span class="op">$</span>cov.unscaled[<span class="dv">1</span>,<span class="dv">1</span>])</a>
<a class="sourceLine" id="cb208-17" data-line-number="17">  se.beta1.boot.np[r] &lt;-<span class="st"> </span>sig.hatr<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">summary</span>(lm.boot)<span class="op">$</span>cov.unscaled[<span class="dv">2</span>,<span class="dv">2</span>])</a>
<a class="sourceLine" id="cb208-18" data-line-number="18">}</a></code></pre></div>
<ul>
<li>To find the studentized confidence intervals for this nonparametric bootstrap, we can use the following code:</li>
</ul>
<div class="sourceCode" id="cb209"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb209-1" data-line-number="1">se.est0 &lt;-<span class="st"> </span><span class="kw">summary</span>(lm.kidney)<span class="op">$</span>sigma<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">summary</span>(lm.boot)<span class="op">$</span>cov.unscaled[<span class="dv">1</span>,<span class="dv">1</span>])</a>
<a class="sourceLine" id="cb209-2" data-line-number="2">se.est1 &lt;-<span class="st"> </span><span class="kw">summary</span>(lm.kidney)<span class="op">$</span>sigma<span class="op">*</span><span class="kw">sqrt</span>(<span class="kw">summary</span>(lm.boot)<span class="op">$</span>cov.unscaled[<span class="dv">2</span>,<span class="dv">2</span>])</a>
<a class="sourceLine" id="cb209-3" data-line-number="3"></a>
<a class="sourceLine" id="cb209-4" data-line-number="4">stu.quants0.np &lt;-<span class="st"> </span><span class="kw">quantile</span>( (beta0.boot.np <span class="op">-</span><span class="st"> </span>beta0.hat)<span class="op">/</span>se.beta0.boot.np, </a>
<a class="sourceLine" id="cb209-5" data-line-number="5">                            <span class="dt">probs=</span><span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))</a>
<a class="sourceLine" id="cb209-6" data-line-number="6">stu.quants1.np &lt;-<span class="st"> </span><span class="kw">quantile</span>( (beta1.boot.np <span class="op">-</span><span class="st"> </span>beta1.hat)<span class="op">/</span>se.beta1.boot.np, </a>
<a class="sourceLine" id="cb209-7" data-line-number="7">                            <span class="dt">probs=</span><span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))</a></code></pre></div>
<ul>
<li>The studentized bootstrap confidence intervals for <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> are then</li>
</ul>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb210-1" data-line-number="1"><span class="co">## Confidence interval for beta0</span></a>
<a class="sourceLine" id="cb210-2" data-line-number="2"><span class="kw">c</span>(beta0.hat <span class="op">-</span><span class="st"> </span>stu.quants0.np[<span class="dv">2</span>]<span class="op">*</span>se.est0, beta0.hat <span class="op">-</span><span class="st"> </span>stu.quants0.np[<span class="dv">1</span>]<span class="op">*</span>se.est0)</a></code></pre></div>
<pre><code>## (Intercept) (Intercept) 
##        2.09        3.62</code></pre>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb212-1" data-line-number="1"><span class="co">## Confidence interval for beta1</span></a>
<a class="sourceLine" id="cb212-2" data-line-number="2"><span class="kw">c</span>(beta1.hat <span class="op">-</span><span class="st"> </span>stu.quants1.np[<span class="dv">2</span>]<span class="op">*</span>se.est1, beta1.hat <span class="op">-</span><span class="st"> </span>stu.quants1.np[<span class="dv">1</span>]<span class="op">*</span>se.est1)</a></code></pre></div>
<pre><code>##     age     age 
## -0.0997 -0.0599</code></pre>
<hr />
<ul>
<li><strong>Exercise 10.2</strong> Another way of using the bootstrap in a regression context
is to resample the residuals from the fitted regression model. Speficially,
we first fit the linear regression model and compute residuals <span class="math inline">\(\hat{e}_{1}, \ldots, \hat{e}_{n}\)</span>
via
<span class="math display">\[\begin{equation}
\hat{e}_{i} = Y_{i} - \hat{\beta}_{0} - \hat{\beta}_{1}x_{i} \nonumber
\end{equation}\]</span>
One then generates a bootstrap sample
by first subsampling <span class="math inline">\((\hat{e}_{1}^{*}, \ldots, \hat{e}_{n}^{*})\)</span> from the
vector of âoriginalâ residuals <span class="math inline">\((\hat{e}_{1}, \ldots, \hat{e}_{n})\)</span> and then
setting <span class="math inline">\(Y_{i}^{*} = \hat{\beta}_{0} + \hat{\beta}_{1}x_{i} + \hat{e}_{i}^{*}\)</span>.
You then compute the bootstrap replications <span class="math inline">\(\hat{\beta}_{0,r}^{*}\)</span> and <span class="math inline">\(\hat{\beta}_{1,r}^{*}\)</span>
by fitting a linear regression with data: <span class="math inline">\((Y_{1}^{*}, x_{1}), \ldots, (Y_{n}^{*}, x_{n})\)</span>.</li>
</ul>
<p>Using the kidney data, try using this procedure to construct <span class="math inline">\(95\%\)</span> bootstrap confidence intervals
for <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>.</p>
<hr />
<p><strong>Regression with more than 1 covariate</strong></p>
<ul>
<li><p>If we have more than one covariate in our model, for example,
<span class="math display">\[\begin{equation}
Y_{i} = \beta_{0} + \beta_{1}x_{i1} + \ldots + \beta_{p}x_{ip} + \varepsilon_{i}, \nonumber
\end{equation}\]</span>
the bootstrap works essentially the same as for the case with a single covariate.</p></li>
<li><p>For the parametric bootstrap with a Normal residual distribution, you would just simulate
<span class="math inline">\(Y_{i}^{*} \sim \textrm{Normal}(\hat{\beta}_{0} + \hat{\beta}_{1}x_{i1} + \ldots + \hat{\beta}_{p}x_{ip}, \hat{\sigma}^{2})\)</span>.</p></li>
<li><p>For the nonparametric bootstrap, you would subsample pairs <span class="math inline">\((Y_{1}^{*}, x_{1}^{*}), \ldots, (Y_{n}^{*}, x_{n}^{*})\)</span> as described before,
and compute your regression coefficients <span class="math inline">\(\hat{\beta}_{0,r}^{*}, \hat{\beta}_{1,r}^{*}, \ldots, \hat{\beta}_{p,r}^{*}\)</span> from this
bootstrap sample.</p></li>
</ul>
</div>
</div>
<div id="pointwise-confidence-intervals-for-a-density-function" class="section level2">
<h2><span class="header-section-number">10.3</span> Pointwise Confidence Intervals for a Density Function</h2>
<ul>
<li><p>Recall that a kernel density estimate of an unknown probability density <span class="math inline">\(f(x)\)</span> has
the form
<span class="math display">\[\begin{equation}
\hat{f}_{h_{n}}(x) = \frac{1}{n h_{n}}\sum_{i=1}^{n} K\Big( \frac{x - X_{i}}{h_{n}} \Big) \nonumber
\end{equation}\]</span></p></li>
<li><p>We cannot naively apply the Central Limit Theorem, because <span class="math inline">\(h_{n}\)</span> is changing as <span class="math inline">\(n \longrightarrow \infty\)</span>.</p></li>
<li><p>Nevertheless, you can show (see, e.g., <span class="citation">Tsybakov (<a href="#ref-tsybakov2008">2008</a>)</span>) that
<span class="math display">\[\begin{equation}
\sqrt{nh_{n}}\Big( \hat{f}_{h_{n}}(x) - E\{ \hat{f}_{h_{n}}(x) \} \Big) \longrightarrow \textrm{Normal}\big( 0, \kappa_{2}(K) f(x) \big) \nonumber
\end{equation}\]</span>
provided that <span class="math inline">\(h_{n} \longrightarrow 0\)</span> and <span class="math inline">\(nh_{n} \longrightarrow \infty\)</span>. Here, <span class="math inline">\(\kappa_{2}(K) = \int_{-\infty}^{\infty} K^{2}(u) du\)</span>.</p></li>
</ul>
<hr />
<ul>
<li><p>This suggests that a standard error estimate for <span class="math inline">\(\hat{f}_{h_{n}}(x)\)</span> is <span class="math inline">\(\sqrt{\kappa_{2}(K)\hat{f}_{h_{n}}(x)/nh_{n}}\)</span>
and a <span class="math inline">\(95\%\)</span> confidence interval for <span class="math inline">\(E\{ \hat{f}_{h_{n}}(x) \}\)</span> is
<span class="math display">\[\begin{equation}
\Bigg[ \hat{f}_{h_{n}}(x) - 1.96 \times \sqrt{\frac{\kappa_{2}(K) \hat{f}_{h_{n}}(x) }{nh_{n}}},
\hat{f}_{h_{n}}(x) + 1.96 \times \sqrt{\frac{\kappa_{2}(K) \hat{f}_{h_{n}}(x)}{nh_{n}}} \Bigg] \nonumber
\end{equation}\]</span></p></li>
<li><p>Notice that this is a confidence interval for <span class="math inline">\(E\{ \hat{f}_{h_{n}}(x) \}\)</span> rather than <span class="math inline">\(f(x)\)</span>.</p></li>
<li><p>So, you can roughly think of this as a confidence interval for a smoothed version of <span class="math inline">\(f(x)\)</span> at <span class="math inline">\(x\)</span>:
<span class="math display">\[\begin{equation}
E\{ \hat{f}_{h_{n}}(x) \} = \frac{1}{h_{n}}\int_{-\infty}^{\infty} K\Big( \frac{x - t}{h_{n}} \Big) f(t) dt \nonumber
\end{equation}\]</span></p></li>
<li><p>Notice also that this is a pointwise confidence interval. It is not a confidence band.</p></li>
<li><p>Methods for computing âbias-correctedâ confidence intervals for <span class="math inline">\(f(x)\)</span> are
discussed, for example, in <span class="citation">Chen (<a href="#ref-chen2017tutorial">2017</a>)</span>.</p></li>
</ul>
<hr />
<ul>
<li><p>To get a bootstrap estimate of the standard deviation of <span class="math inline">\(\hat{f}_{h_{n}}(x)\)</span>, we can use the usual steps.</p></li>
<li>For <span class="math inline">\(r=1, \ldots, R\)</span>:
<ul>
<li>Draw a sample of size <span class="math inline">\(n\)</span>: <span class="math inline">\((X_{1}^{*}, \ldots, X_{n}^{*})\)</span> by sampling with replacement from <span class="math inline">\(\mathbf{X}\)</span>.</li>
<li>Compute <span class="math inline">\(T_{n,r}^{*} = \tfrac{1}{nh_{n}}\sum_{i=1}^{n} K(\tfrac{x - X_{i}^{*}}{ h_{n} } )\)</span>.</li>
</ul></li>
</ul>
<p>Then, compute the estimated standard error:
<span class="math display">\[\begin{equation}
\hat{se}_{boot} = \Big[ \frac{1}{R-1} \sum_{r=1}^{R} \Big( T_{n,r}^{*} - \frac{1}{R} \sum_{r=1}^{R} T_{n,r}^{*} )^{2} \Big]^{1/2}
\end{equation}\]</span></p>
<ul>
<li><code>R</code> code to compute these standard error estimates for the <code>sysBP</code> variable from the <code>framingham</code> dataset is given below</li>
</ul>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb214-1" data-line-number="1">framingham &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;~/Documents/STAT685Notes/Data/framingham.csv&quot;</span>)</a>
<a class="sourceLine" id="cb214-2" data-line-number="2">R &lt;-<span class="st"> </span><span class="dv">500</span></a>
<a class="sourceLine" id="cb214-3" data-line-number="3">BootMat &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span>R, <span class="dt">ncol=</span><span class="dv">4</span>)</a>
<a class="sourceLine" id="cb214-4" data-line-number="4"><span class="cf">for</span>(r <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>R) {</a>
<a class="sourceLine" id="cb214-5" data-line-number="5">    xx.boot &lt;-<span class="st"> </span><span class="kw">sample</span>(framingham<span class="op">$</span>sysBP, <span class="dt">size=</span><span class="kw">length</span>(framingham<span class="op">$</span>sysBP), <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb214-6" data-line-number="6">    kk.boot &lt;-<span class="st"> </span><span class="kw">density</span>(xx.boot)</a>
<a class="sourceLine" id="cb214-7" data-line-number="7">    tmp &lt;-<span class="st"> </span><span class="kw">approxfun</span>(kk.boot<span class="op">$</span>x, kk.boot<span class="op">$</span>y)</a>
<a class="sourceLine" id="cb214-8" data-line-number="8">    BootMat[r,] &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">tmp</span>(<span class="dv">100</span>), <span class="kw">tmp</span>(<span class="dv">125</span>), <span class="kw">tmp</span>(<span class="dv">150</span>), <span class="kw">tmp</span>(<span class="dv">175</span>))</a>
<a class="sourceLine" id="cb214-9" data-line-number="9">}</a>
<a class="sourceLine" id="cb214-10" data-line-number="10">bb &lt;-<span class="st"> </span><span class="kw">apply</span>(BootMat, <span class="dv">2</span>, sd)</a></code></pre></div>
<div class="figure"><span id="fig:unnamed-chunk-21"></span>
<img src="10-confidence-intervals_files/figure-html/unnamed-chunk-21-1.png" alt="Bootstrap confidence intervals for the density function at the points x=100, 125, 150, 175" width="672" />
<p class="caption">
Figure 10.1: Bootstrap confidence intervals for the density function at the points x=100, 125, 150, 175
</p>
</div>
</div>
<div id="when-can-the-bootstrap-fail" class="section level2">
<h2><span class="header-section-number">10.4</span> When can the Bootstrap Fail?</h2>
<ul>
<li><p>While the bootstrap is very automatic and could be used to construct confidence intervals
in nearly any situation, these bootstrap confidence intervals may fail to give
the correct coverage in some situations.</p></li>
<li>A few situations in which the bootstrap can fail include:
<ul>
<li>If we are interested in estimating a parameter <span class="math inline">\(\theta\)</span> and the support <span class="math inline">\(\{ x: f_{\theta}(x) &gt; 0\}\)</span> of the density function depends on <span class="math inline">\(\theta\)</span>.</li>
<li>If there are parameter constraints and the true value of the parameter lies on the boundary of the parameter space. For example, we estimate <span class="math inline">\(\theta\)</span> subject to the constraint that <span class="math inline">\(\theta \geq 0\)</span>, and the true value of <span class="math inline">\(\theta\)</span> is zero.</li>
<li>If <span class="math inline">\(T_{n} = g(\bar{X})\)</span> and <span class="math inline">\(g&#39;(\mu) = 0\)</span> where <span class="math inline">\(\mu = E(X_{1})\)</span>.</li>
<li>No finite mean. If <span class="math inline">\(E(|X_{1}|)\)</span> is not finite, then the bootstrap may not work well.</li>
</ul></li>
</ul>
<div id="example-the-shifted-exponential-distribution" class="section level3">
<h3><span class="header-section-number">10.4.1</span> Example: The Shifted Exponential Distribution</h3>
<ul>
<li><p>Let us consider observations <span class="math inline">\(X_{1}, \ldots, X_{n}\)</span> that follow the shifted exponential distribution whose
density function is
<span class="math display">\[\begin{equation}
f(x)
= \begin{cases}
\lambda e^{-\lambda(x - \eta)} &amp; \textrm{ if } x \geq \eta \nonumber \\
0 &amp; \textrm{otherwise}  \nonumber
\end{cases}
\end{equation}\]</span>
where <span class="math inline">\(\lambda &gt; 0\)</span> and <span class="math inline">\(\eta &gt; 0\)</span>.</p></li>
<li><p>The maximum likelihood estimates of <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\eta\)</span> are
<span class="math display">\[\begin{equation}
\hat{\lambda} = \frac{1}{\bar{X}} - X_{(1)} \qquad \hat{\eta} = X_{(1)}  \nonumber
\end{equation}\]</span>
where <span class="math inline">\(X_{(1)} = \min\{ X_{1}, \ldots, X_{n} \}\)</span> is the smallest observation.</p></li>
<li><p>Notice that this is an example where the support of the density function depends
on the parameter <span class="math inline">\(\eta\)</span>.</p></li>
<li><p>Suppose we use the bootstrap to construct confidence intervals for <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\eta\)</span>.
What will happen?</p></li>
</ul>
<hr />
<ul>
<li><p>Let us consider an example where we have i.i.d. data <span class="math inline">\(X_{1}, \ldots, X_{n}\)</span> that follow
a shifted Exponential distribution with <span class="math inline">\(\lambda = 1/3\)</span> and <span class="math inline">\(\eta = 2\)</span>.</p></li>
<li><p>The following code can estimate the coverage proportion of a bootstrap
confidence interval for <span class="math inline">\(\eta\)</span>:</p></li>
</ul>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb215-1" data-line-number="1">n &lt;-<span class="st"> </span><span class="dv">200</span></a>
<a class="sourceLine" id="cb215-2" data-line-number="2">R &lt;-<span class="st"> </span><span class="dv">500</span></a>
<a class="sourceLine" id="cb215-3" data-line-number="3">eta.true &lt;-<span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb215-4" data-line-number="4"></a>
<a class="sourceLine" id="cb215-5" data-line-number="5">nreps &lt;-<span class="st"> </span><span class="dv">500</span></a>
<a class="sourceLine" id="cb215-6" data-line-number="6">Cover.bootsd.ci &lt;-<span class="st"> </span><span class="kw">numeric</span>(nreps)</a>
<a class="sourceLine" id="cb215-7" data-line-number="7"><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nreps)  {</a>
<a class="sourceLine" id="cb215-8" data-line-number="8">  <span class="co">## Step 1: Generate the Data and compute the estimate of eta</span></a>
<a class="sourceLine" id="cb215-9" data-line-number="9">  xx &lt;-<span class="st"> </span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">rexp</span>(n, <span class="dt">rate=</span><span class="dv">1</span><span class="op">/</span><span class="dv">3</span>) </a>
<a class="sourceLine" id="cb215-10" data-line-number="10">  eta.hat &lt;-<span class="st"> </span><span class="kw">min</span>(xx)</a>
<a class="sourceLine" id="cb215-11" data-line-number="11">  </a>
<a class="sourceLine" id="cb215-12" data-line-number="12">  <span class="co">## Step 2: Find bootstrap confidence intervals using R bootstrap replications</span></a>
<a class="sourceLine" id="cb215-13" data-line-number="13">  eta.boot &lt;-<span class="st"> </span><span class="kw">numeric</span>(R)</a>
<a class="sourceLine" id="cb215-14" data-line-number="14">  <span class="cf">for</span>(r <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>R)   {</a>
<a class="sourceLine" id="cb215-15" data-line-number="15">    boot.xx &lt;-<span class="st"> </span><span class="kw">sample</span>(xx, <span class="dt">size=</span>n, <span class="dt">replace =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb215-16" data-line-number="16">    eta.boot[r] &lt;-<span class="st"> </span><span class="kw">min</span>(boot.xx)</a>
<a class="sourceLine" id="cb215-17" data-line-number="17">  }</a>
<a class="sourceLine" id="cb215-18" data-line-number="18">  boot.ci.sd &lt;-<span class="st"> </span><span class="kw">c</span>(eta.hat <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span><span class="op">*</span><span class="kw">sd</span>(eta.boot), eta.hat <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span><span class="op">*</span><span class="kw">sd</span>(eta.boot))</a>
<a class="sourceLine" id="cb215-19" data-line-number="19">  </a>
<a class="sourceLine" id="cb215-20" data-line-number="20">  <span class="co">## Step 3: Record if the true parameter is covered or not:</span></a>
<a class="sourceLine" id="cb215-21" data-line-number="21">  Cover.bootsd.ci[k] &lt;-<span class="st"> </span><span class="kw">ifelse</span>(boot.ci.sd[<span class="dv">1</span>] <span class="op">&lt;</span><span class="st"> </span>eta.true <span class="op">&amp;</span><span class="st"> </span>boot.ci.sd[<span class="dv">2</span>] <span class="op">&gt;=</span><span class="st"> </span>eta.true, </a>
<a class="sourceLine" id="cb215-22" data-line-number="22">                               <span class="dv">1</span>, <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb215-23" data-line-number="23">}</a></code></pre></div>
<ul>
<li>The estimated coverage for this bootstrap confidence interval is</li>
</ul>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb216-1" data-line-number="1"><span class="kw">mean</span>(Cover.bootsd.ci)</a></code></pre></div>
<pre><code>## [1] 0.814</code></pre>
<p><img src="10-confidence-intervals_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
</div>
</div>
<div id="the-jackknife" class="section level2">
<h2><span class="header-section-number">10.5</span> The Jackknife</h2>
<ul>
<li><p>The jackknife is also a nonparametric method for estimating standard errors.</p></li>
<li><p>Like the bootstrap, the jackknife also uses the idea of looking at
multiple subsets of the data.</p></li>
<li><p>Also like the bootstrap, the jackknife is completely automatic in the sense
that we only need to be able to compute our statistic of interest, and
we do not need to do any formal calculations to find the standard error.</p></li>
<li><p>While the jackknife was actually developed before the bootstrap, it is
used much less than the bootstrap is in applications - at least
in the context of finding confidence intervals.</p></li>
</ul>
<hr />
<ul>
<li><p>We will define <span class="math inline">\(\mathbf{X}_{-i}\)</span> to be the vector of observations
that has the <span class="math inline">\(i^{th}\)</span> observation deleted:
<span class="math display">\[\begin{equation}
\mathbf{X}_{(-i)} = (X_{1}, \ldots, X_{i-1}, X_{i+1}, \ldots, X_{n})  \nonumber
\end{equation}\]</span></p></li>
<li><p>Define <span class="math inline">\(T_{n,(-i)}\)</span> to be the value of the statistic <span class="math inline">\(T_{n}\)</span> when using
data which has the <span class="math inline">\(i^{th}\)</span> observation removed
<span class="math display">\[\begin{equation}
T_{n, (-i)} = h(X_{1}, \ldots, X_{i-1}, X_{i+1}, \ldots, X_{n})  \nonumber
\end{equation}\]</span></p></li>
<li><p>The jackknife estimate of the standard error of <span class="math inline">\(T_{n}\)</span> is
<span class="math display">\[\begin{equation}
\hat{se}_{jack} = \Big[ \frac{n-1}{n} \sum_{i=1}^{n} ( T_{n, (-i)} - \bar{T}_{n, jack} )^{2}  \Big]^{1/2}, \nonumber 
\end{equation}\]</span>
where <span class="math inline">\(\bar{T}_{n,jack} = \tfrac{1}{n} \sum_{i=1}^{n} T_{n, (-i)}\)</span>.</p></li>
</ul>
<hr />
<ul>
<li><p>An advantage of the jackknife is that, like the bootstrap, it does not
make any particular parametric assumptions about the distribution of the data.</p></li>
<li><p>However, the jackknife is more dependent on a smoothness assumption (that is smoothness
across slightly perturbed datasets) than the bootstrap. An example of this is the
sample median where, if we delete one observation, the sample median has
a different definition due to the sample size being even vs.Â odd.</p></li>
</ul>

</div>
</div>



<h3>References</h3>
<div id="refs" class="references">
<div id="ref-buhlmann2002">
<p>BÃ¼hlmann, Peter. 2002. âBootstraps for Time Series.â <em>Statistical Science</em>, 52â72.</p>
</div>
<div id="ref-chen2017tutorial">
<p>Chen, Yen-Chi. 2017. âA Tutorial on Kernel Density Estimation and Recent Advances.â <em>Biostatistics &amp; Epidemiology</em> 1 (1). Taylor &amp; Francis: 161â87.</p>
</div>
<div id="ref-davison1997">
<p>Davison, Anthony Christopher, and David Victor Hinkley. 1997. <em>Bootstrap Methods and Their Application</em>. Vol. 1. Cambridge university press.</p>
</div>
<div id="ref-tsybakov2008">
<p>Tsybakov, Alexandre B. 2008. <em>Introduction to Nonparametric Estimation</em>. Springer Science &amp; Business Media.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bootstrap-main.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="kernel-regression-and-local-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ElementsNonparStat.pdf"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
