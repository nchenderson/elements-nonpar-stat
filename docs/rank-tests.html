<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Rank and Sign Statistics | Elements of Nonparametric Statistics</title>
  <meta name="description" content="Course notes for Biostatistics 685/Statistics 560 (Winter 2020)." />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Rank and Sign Statistics | Elements of Nonparametric Statistics" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://nchenderson.github.io/elements-nonpar-stat/" />
  
  <meta property="og:description" content="Course notes for Biostatistics 685/Statistics 560 (Winter 2020)." />
  <meta name="github-repo" content="nchenderson/elements-nonpar-stat" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Rank and Sign Statistics | Elements of Nonparametric Statistics" />
  
  <meta name="twitter:description" content="Course notes for Biostatistics 685/Statistics 560 (Winter 2020)." />
  

<meta name="author" content="Nicholas Henderson" />


<meta name="date" content="2020-01-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="getting-started.html"/>
<link rel="next" href="tidy.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Biostat 685/Stat 560</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#sec:whatisnonpar"><i class="fa fa-check"></i><b>1.1</b> What is Nonparametric Statistics?</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#sec:course-outline"><i class="fa fa-check"></i><b>1.2</b> Outline of Course</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#sec:example-nonpar-tests"><i class="fa fa-check"></i><b>1.3</b> Example 1: Nonparametric vs. Parametric Two-Sample Testing</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#sec:example-nonpar-estimation"><i class="fa fa-check"></i><b>1.4</b> Example 2: Nonparametric Estimation</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#sec:example-nonpar-confint"><i class="fa fa-check"></i><b>1.5</b> Example 3: Confidence Intervals</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#sec:example-nonpar-regress1"><i class="fa fa-check"></i><b>1.6</b> Example 4: Nonparametric Regression with a Single Covariate</a></li>
<li class="chapter" data-level="1.7" data-path="intro.html"><a href="intro.html#sec:example-nonpar-regress2"><i class="fa fa-check"></i><b>1.7</b> Example 5: Classification and Regression Trees (CART)</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>2</b> Working with R</a></li>
<li class="part"><span><b>I Nonparametric Testing</b></span></li>
<li class="chapter" data-level="3" data-path="rank-tests.html"><a href="rank-tests.html"><i class="fa fa-check"></i><b>3</b> Rank and Sign Statistics</a><ul>
<li class="chapter" data-level="3.1" data-path="rank-tests.html"><a href="rank-tests.html#ranks"><i class="fa fa-check"></i><b>3.1</b> Ranks</a><ul>
<li class="chapter" data-level="3.1.1" data-path="rank-tests.html"><a href="rank-tests.html#definition"><i class="fa fa-check"></i><b>3.1.1</b> Definition</a></li>
<li class="chapter" data-level="3.1.2" data-path="rank-tests.html"><a href="rank-tests.html#handling-ties"><i class="fa fa-check"></i><b>3.1.2</b> Handling Ties</a></li>
<li class="chapter" data-level="3.1.3" data-path="rank-tests.html"><a href="rank-tests.html#properties-of-ranks"><i class="fa fa-check"></i><b>3.1.3</b> Properties of Ranks</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="rank-tests.html"><a href="rank-tests.html#the-wilcoxon-rank-sum-wrs-test-a-two-sample-test"><i class="fa fa-check"></i><b>3.2</b> The Wilcoxon Rank Sum (WRS) Test: A Two-Sample Test</a><ul>
<li class="chapter" data-level="3.2.1" data-path="rank-tests.html"><a href="rank-tests.html#goal-of-the-test"><i class="fa fa-check"></i><b>3.2.1</b> Goal of the Test</a></li>
<li class="chapter" data-level="3.2.2" data-path="rank-tests.html"><a href="rank-tests.html#definition-of-the-wrs-test-statistic"><i class="fa fa-check"></i><b>3.2.2</b> Definition of the WRS Test Statistic</a></li>
<li class="chapter" data-level="3.2.3" data-path="rank-tests.html"><a href="rank-tests.html#computing-p-values-for-the-wrs-test"><i class="fa fa-check"></i><b>3.2.3</b> Computing p-values for the WRS Test</a></li>
<li class="chapter" data-level="3.2.4" data-path="rank-tests.html"><a href="rank-tests.html#computing-the-wrs-test-in-r"><i class="fa fa-check"></i><b>3.2.4</b> Computing the WRS test in R</a></li>
<li class="chapter" data-level="3.2.5" data-path="rank-tests.html"><a href="rank-tests.html#additional-notes-for-the-wrs-test"><i class="fa fa-check"></i><b>3.2.5</b> Additional Notes for the WRS test</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="rank-tests.html"><a href="rank-tests.html#one-sample-tests"><i class="fa fa-check"></i><b>3.3</b> One Sample Tests</a><ul>
<li class="chapter" data-level="3.3.1" data-path="rank-tests.html"><a href="rank-tests.html#the-sign-test"><i class="fa fa-check"></i><b>3.3.1</b> The Sign Test</a></li>
<li class="chapter" data-level="3.3.2" data-path="rank-tests.html"><a href="rank-tests.html#the-wilcoxon-signed-rank-test"><i class="fa fa-check"></i><b>3.3.2</b> The Wilcoxon Signed Rank Test</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="rank-tests.html"><a href="rank-tests.html#power-and-comparisons-with-parametric-tests"><i class="fa fa-check"></i><b>3.4</b> Power and Comparisons with Parametric Tests</a><ul>
<li class="chapter" data-level="3.4.1" data-path="rank-tests.html"><a href="rank-tests.html#power-of-tests"><i class="fa fa-check"></i><b>3.4.1</b> Power of Tests</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="rank-tests.html"><a href="rank-tests.html#thinking-about-rank-statistics-more-generally"><i class="fa fa-check"></i><b>3.5</b> Thinking about Rank statistics more generally</a></li>
<li class="chapter" data-level="3.6" data-path="rank-tests.html"><a href="rank-tests.html#notes"><i class="fa fa-check"></i><b>3.6</b> Notes</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="tidy.html"><a href="tidy.html"><i class="fa fa-check"></i><b>4</b> Rank Tests for Multiple Groups</a></li>
<li class="chapter" data-level="5" data-path="permutation.html"><a href="permutation.html"><i class="fa fa-check"></i><b>5</b> Permutation Tests</a></li>
<li class="chapter" data-level="6" data-path="ustat.html"><a href="ustat.html"><i class="fa fa-check"></i><b>6</b> U-Statistics</a><ul>
<li class="chapter" data-level="6.1" data-path="ustat.html"><a href="ustat.html#examples"><i class="fa fa-check"></i><b>6.1</b> Examples</a></li>
<li class="chapter" data-level="6.2" data-path="ustat.html"><a href="ustat.html#mann-whitney-statistic"><i class="fa fa-check"></i><b>6.2</b> Mann-Whitney Statistic</a></li>
<li class="chapter" data-level="6.3" data-path="ustat.html"><a href="ustat.html#kendalls-tau"><i class="fa fa-check"></i><b>6.3</b> Kendall’s tau</a></li>
<li class="chapter" data-level="6.4" data-path="ustat.html"><a href="ustat.html#distance-covariance"><i class="fa fa-check"></i><b>6.4</b> Distance Covariance</a></li>
</ul></li>
<li class="part"><span><b>II Nonparametric Estimation</b></span></li>
<li class="chapter" data-level="7" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>7</b> The Empirical Distribution Function</a><ul>
<li class="chapter" data-level="7.1" data-path="regression.html"><a href="regression.html#empirical-distribution-functions"><i class="fa fa-check"></i><b>7.1</b> Empirical Distribution Functions</a><ul>
<li class="chapter" data-level="7.1.1" data-path="regression.html"><a href="regression.html#definition-and-basic-properties"><i class="fa fa-check"></i><b>7.1.1</b> Definition and Basic Properties</a></li>
<li class="chapter" data-level="7.1.2" data-path="regression.html"><a href="regression.html#confidence-intervals-for-fhat"><i class="fa fa-check"></i><b>7.1.2</b> Confidence intervals for Fhat</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="regression.html"><a href="regression.html#the-empirical-distribution-function-in-r"><i class="fa fa-check"></i><b>7.2</b> The Empirical Distribution Function in R</a></li>
<li class="chapter" data-level="7.3" data-path="regression.html"><a href="regression.html#the-empirical-distribution-functino-and-statistical-functionals"><i class="fa fa-check"></i><b>7.3</b> The empirical distribution functino and statistical functionals</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="density-estimation.html"><a href="density-estimation.html"><i class="fa fa-check"></i><b>8</b> Density Estimation</a><ul>
<li class="chapter" data-level="8.1" data-path="density-estimation.html"><a href="density-estimation.html#introduction"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="density-estimation.html"><a href="density-estimation.html#histograms"><i class="fa fa-check"></i><b>8.2</b> Histograms</a><ul>
<li class="chapter" data-level="8.2.1" data-path="density-estimation.html"><a href="density-estimation.html#definition-1"><i class="fa fa-check"></i><b>8.2.1</b> Definition</a></li>
<li class="chapter" data-level="8.2.2" data-path="density-estimation.html"><a href="density-estimation.html#histograms-in-r"><i class="fa fa-check"></i><b>8.2.2</b> Histograms in R</a></li>
<li class="chapter" data-level="8.2.3" data-path="density-estimation.html"><a href="density-estimation.html#performance-of-the-histogram-estimate"><i class="fa fa-check"></i><b>8.2.3</b> Performance of the Histogram Estimate</a></li>
<li class="chapter" data-level="8.2.4" data-path="density-estimation.html"><a href="density-estimation.html#choosing-the-histogram-bin-width"><i class="fa fa-check"></i><b>8.2.4</b> Choosing the Histogram Bin Width</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="density-estimation.html"><a href="density-estimation.html#kernel-density-estimation"><i class="fa fa-check"></i><b>8.3</b> Kernel Density Estimation</a><ul>
<li class="chapter" data-level="8.3.1" data-path="density-estimation.html"><a href="density-estimation.html#histograms-and-a-naive-density-estimate"><i class="fa fa-check"></i><b>8.3.1</b> Histograms and a “Naive” Density Estimate</a></li>
<li class="chapter" data-level="8.3.2" data-path="density-estimation.html"><a href="density-estimation.html#kernels-bandwidth-and-smooth-density-estimation"><i class="fa fa-check"></i><b>8.3.2</b> Kernels, Bandwidth, and Smooth Density Estimation</a></li>
<li class="chapter" data-level="8.3.3" data-path="density-estimation.html"><a href="density-estimation.html#bias-and-variance-of-kernel-density-estimates"><i class="fa fa-check"></i><b>8.3.3</b> Bias and Variance of Kernel Density Estimates</a></li>
<li class="chapter" data-level="8.3.4" data-path="density-estimation.html"><a href="density-estimation.html#bandwidth-selection"><i class="fa fa-check"></i><b>8.3.4</b> Bandwidth Selection</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="density-estimation.html"><a href="density-estimation.html#kernel-density-estimation-in-practice"><i class="fa fa-check"></i><b>8.4</b> Kernel Density Estimation in Practice</a></li>
</ul></li>
<li class="part"><span><b>III Uncertainty Measures</b></span></li>
<li class="chapter" data-level="9" data-path="sampling.html"><a href="sampling.html"><i class="fa fa-check"></i><b>9</b> The Jackknife</a><ul>
<li class="chapter" data-level="9.1" data-path="sampling.html"><a href="sampling.html#introduction-1"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ci.html"><a href="ci.html"><i class="fa fa-check"></i><b>10</b> The Bootstrap and Confidence Intervals</a><ul>
<li class="chapter" data-level="10.1" data-path="ci.html"><a href="ci.html#bootstrapping"><i class="fa fa-check"></i><b>10.1</b> Bootstrapping</a></li>
</ul></li>
<li class="part"><span><b>IV Nonparametric Regression: Part I</b></span></li>
<li class="chapter" data-level="11" data-path="kernel-regression.html"><a href="kernel-regression.html"><i class="fa fa-check"></i><b>11</b> Kernel Regression</a><ul>
<li class="chapter" data-level="11.1" data-path="kernel-regression.html"><a href="kernel-regression.html#introduction-2"><i class="fa fa-check"></i><b>11.1</b> Introduction</a><ul>
<li class="chapter" data-level="11.1.1" data-path="kernel-regression.html"><a href="kernel-regression.html#an-example"><i class="fa fa-check"></i><b>11.1.1</b> An Example</a></li>
<li class="chapter" data-level="11.1.2" data-path="kernel-regression.html"><a href="kernel-regression.html#linear-smoothers-and-naive-nonparametric-estimates"><i class="fa fa-check"></i><b>11.1.2</b> Linear Smoothers and Naive Nonparametric Estimates</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="kernel-regression.html"><a href="kernel-regression.html#the-nadaraya-watson-estimator"><i class="fa fa-check"></i><b>11.2</b> The Nadaraya-Watson estimator</a></li>
<li class="chapter" data-level="11.3" data-path="kernel-regression.html"><a href="kernel-regression.html#local-linear-and-polynomial-regression"><i class="fa fa-check"></i><b>11.3</b> Local Linear and Polynomial Regression</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="inference-for-regression.html"><a href="inference-for-regression.html"><i class="fa fa-check"></i><b>12</b> Splines and Penalized Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="inference-for-regression.html"><a href="inference-for-regression.html#introduction-3"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="inference-for-regression.html"><a href="inference-for-regression.html#spline-basis-functions"><i class="fa fa-check"></i><b>12.2</b> Spline Basis Functions</a></li>
<li class="chapter" data-level="12.3" data-path="inference-for-regression.html"><a href="inference-for-regression.html#smoothing-splinespenalized-regression"><i class="fa fa-check"></i><b>12.3</b> Smoothing Splines/Penalized Regression</a><ul>
<li class="chapter" data-level="12.3.1" data-path="inference-for-regression.html"><a href="inference-for-regression.html#selection-of-smoothing-parameter"><i class="fa fa-check"></i><b>12.3.1</b> Selection of Smoothing Parameter</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>V Nonparametric Regression: Part II</b></span></li>
<li class="chapter" data-level="13" data-path="decision-tree.html"><a href="decision-tree.html"><i class="fa fa-check"></i><b>13</b> Decision Trees and CART</a></li>
<li class="chapter" data-level="14" data-path="ensemble.html"><a href="ensemble.html"><i class="fa fa-check"></i><b>14</b> Ensemble Methods for Prediction</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Elements of Nonparametric Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="rank-tests" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Rank and Sign Statistics</h1>
<!--   ## Introduction

Start with t-test example, difference in means is sufficient for superiority

Give example of type of tests we are interested in.

Why ranks and why nonparametric testing?

(reduce influence of outliers)
-->
<div id="ranks" class="section level2">
<h2><span class="header-section-number">3.1</span> Ranks</h2>
<div id="definition" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Definition</h3>
<ul>
<li>Suppose we have <span class="math inline">\(n\)</span> observations <span class="math inline">\(\mathbf{X} = (X_{1}, \ldots, X_{n})\)</span>. The <strong>rank</strong> of the <span class="math inline">\(i^{th}\)</span> observation <span class="math inline">\(R_{i}\)</span> is defined as
<span class="math display" id="eq:rankdef">\[\begin{equation}
R_{i} = R_{i}(\mathbf{X}) = \sum_{j=1}^{n} I( X_{i} \geq X_{j}) 
\tag{3.1}
\end{equation}\]</span>
where
<span class="math display">\[\begin{equation}
I(X_{i} \geq X_{j}) 
= \begin{cases}
1 &amp; \text{ if } X_{i} \geq X_{j} \\
0 &amp; \text{ if } X_{i} &lt; X_{j}
\end{cases}
\end{equation}\]</span></li>
<li>The largest observation has a rank of <span class="math inline">\(n\)</span>.</li>
<li>The smallest observation has a rank of <span class="math inline">\(1\)</span> (if there are no ties).</li>
<li><p>I’m using the notation <span class="math inline">\(R_{i}(\mathbf{X})\)</span> to emphasize that the rank
of the <span class="math inline">\(i^{th}\)</span> observations depends on the entire vector of observations
rather than only the value of <span class="math inline">\(X_{i}\)</span>.</p></li>
<li><p>You can compute ranks in <strong>R</strong> using the <strong>rank</strong> function:</p></li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1">x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">7</span>, <span class="dv">1</span>, <span class="dv">12</span>, <span class="dv">6</span>)  <span class="co">## 5 observations</span></a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="kw">rank</span>(x)</a></code></pre></div>
<pre><code>## [1] 2 4 1 5 3</code></pre>
</div>
<div id="handling-ties" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Handling Ties</h3>
<ul>
<li>In the definition of ranks shown in <a href="rank-tests.html#eq:rankdef">(3.1)</a>, tied observations
receive their maximum possible rank.</li>
<li>For example, suppose that <span class="math inline">\((X_{1}, X_{2}, X_{3}, X_{4}) = (0, 1, 1, 2)\)</span>.
In this case, one could argue whether both observations 2 and 3 should be ranked
<span class="math inline">\(2^{nd}\)</span> or <span class="math inline">\(3^{rd}\)</span> while observations <span class="math inline">\(1\)</span> and <span class="math inline">\(4\)</span> should unambiguously receive
ranks of <span class="math inline">\(1\)</span> and <span class="math inline">\(4\)</span> respectively.</li>
<li><p>Under definition <a href="rank-tests.html#eq:rankdef">(3.1)</a>, both observations <span class="math inline">\(2\)</span> and <span class="math inline">\(3\)</span> receive a rank of <span class="math inline">\(3\)</span>.</p></li>
<li><p>In <strong>R</strong>, handling ties that is consistent with definition <a href="rank-tests.html#eq:rankdef">(3.1)</a> is done using the <strong>ties.method = “max”</strong> argument</p></li>
</ul>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1">x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>)  </a>
<a class="sourceLine" id="cb3-2" data-line-number="2"><span class="kw">rank</span>(x, <span class="dt">ties.method=</span><span class="st">&quot;max&quot;</span>)</a></code></pre></div>
<pre><code>## [1] 1 3 3 4</code></pre>
<ul>
<li>The default in <strong>R</strong> is to replace the ranks of tied observations with their “average” rank</li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1">x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>)  </a>
<a class="sourceLine" id="cb5-2" data-line-number="2"><span class="kw">rank</span>(x)</a></code></pre></div>
<pre><code>## [1] 1.0 2.5 2.5 4.0</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" data-line-number="1">y &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">9</span>, <span class="dv">7</span>, <span class="dv">7</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb7-2" data-line-number="2"><span class="kw">rank</span>(y, <span class="dt">ties.method=</span><span class="st">&quot;max&quot;</span>)</a></code></pre></div>
<pre><code>## [1] 3 7 6 6 4 3 1</code></pre>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" data-line-number="1"><span class="kw">rank</span>(y)</a></code></pre></div>
<pre><code>## [1] 2.5 7.0 5.5 5.5 4.0 2.5 1.0</code></pre>
<hr />
<ul>
<li><p>When defining ranks using the “average” or “midrank” approach to handling ties, replaces
tied ranks with the average of the two “adjacent” ranks.</p></li>
<li><p>For example, if we have a vector of ranks <span class="math inline">\((R_{1}, R_{2}, R_{3}, R_{4})\)</span> where <span class="math inline">\(R_{2} = R_{3} =3\)</span> and <span class="math inline">\(R_{1} = 4\)</span> and <span class="math inline">\(R_{4} = 1\)</span>, then the vector of modified ranks using the “average” approach to handling ties
would be
<span class="math display">\[\begin{equation}
(R_{1}&#39;, R_{2}&#39;, R_{3}&#39;, R_{4}&#39;) = \Big( 4, \frac{4 + 1}{2}, \frac{4 + 1}{2}, 1 \Big)
\end{equation}\]</span></p></li>
<li><p>The “average” approach is the most common way of handling ties when computing the
Wilcoxon rank sum statistic.</p></li>
</ul>
</div>
<div id="properties-of-ranks" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Properties of Ranks</h3>
<p>Suppose <span class="math inline">\((X_{1}, \ldots, X_{n})\)</span> is random sample from a continuous distribution <span class="math inline">\(F\)</span> (so that the probability
of ties is zero). Then, the following properties hold for the associated ranks <span class="math inline">\(R_{1}, \ldots, R_{n}\)</span>.</p>
<ul>
<li>Each <span class="math inline">\(R_{i}\)</span> follows a discrete uniform distribution
<span class="math display">\[\begin{equation}
P(R_{i} = j) = 1/n, \quad \text{for any } j = 1, \ldots,n.
\end{equation}\]</span></li>
<li>The expectation of <span class="math inline">\(R_{i}\)</span> is
<span class="math display">\[\begin{equation}
E( R_{i} ) = \sum_{j=1}^{n} j P(R_{i} = j) = \frac{1}{n}\sum_{j=1}^{n} j = \frac{(n+1)}{2}
\end{equation}\]</span></li>
<li>The variance of <span class="math inline">\(R_{i}\)</span> is
<span class="math display">\[\begin{equation}
\text{Var}( R_{i} ) = E( R_{i}^{2} ) - E(R_{i})^{2}
= \frac{1}{n}\sum_{j=1}^{n} j^{2}  - \Big( \frac{n+1}{2} \Big)^{2}
= \frac{ n^{2} - 1}{12}
\end{equation}\]</span></li>
<li>The random variables <span class="math inline">\(R_{1}, \ldots, R_{n}\)</span> are <strong>not</strong> independent (why?). However,
the vector <span class="math inline">\(\mathbf{R}_{n} = (R_{1}, \ldots, R_{n})\)</span> is uniformly distributed
on the set of <span class="math inline">\(n!\)</span> permutations of <span class="math inline">\((1,2,\ldots,n)\)</span>.</li>
</ul>
<hr />
<p><strong>Exercise 3.1</strong>: Suppose <span class="math inline">\(X_{1}, X_{2}, X_{3}\)</span> are i.i.d. observations from a continuous
distribution function <span class="math inline">\(F_{X}\)</span>. Compute the covariance matrix of the vector
of ranks <span class="math inline">\(\big( R_{1}(\mathbf{X}), R_{2}(\mathbf{X}), R_{3}( \mathbf{X} ) \big)\)</span>.</p>
<p><strong>Exercise 3.2</strong>: Again, suppose that <span class="math inline">\(X_{1}, X_{2}, X_{3}, X_{4}\)</span> are i.i.d. observations from a continuous
distribution function <span class="math inline">\(F_{X}\)</span>. Let <span class="math inline">\(T= R_{1}( \mathbf{X} ) + R_{2}(\mathbf{X})\)</span>. Compute <span class="math inline">\(P( T = j )\)</span>
for <span class="math inline">\(j = 3, 4, 5, 6, 7\)</span>.</p>
<hr />
</div>
</div>
<div id="the-wilcoxon-rank-sum-wrs-test-a-two-sample-test" class="section level2">
<h2><span class="header-section-number">3.2</span> The Wilcoxon Rank Sum (WRS) Test: A Two-Sample Test</h2>
<div id="goal-of-the-test" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Goal of the Test</h3>
<ul>
<li><p>The Wilcoxon Rank Sum (WRS) test (sometimes referred to as the Wilcoxon-Mann-Whitney test) is a popular,
rank-based two-sample test.</p></li>
<li><p>The WRS test is used to test whether or not observations from one group tend to be larger (or smaller) than observations
from the other group.</p></li>
<li><p>Suppose we have observations from two groups: <span class="math inline">\(X_{1}, \ldots, X_{n} \sim F_{X}\)</span> and <span class="math inline">\(Y_{1}, \ldots, Y_{m} \sim F_{Y}\)</span>.</p></li>
<li><p>Roughly speaking, the WRS tests the following hypothesis
<span class="math display" id="eq:general-wilcoxon-hypothesis">\[\begin{eqnarray}
H_{0}: &amp;&amp; F_{X} = F_{Y} \quad \textrm{ versus }  \\
H_{A}: &amp;&amp; \textrm{Observations from } F_{X} \textrm{ tend to be larger than observations from } F_{Y} \nonumber
\tag{3.2}
\end{eqnarray}\]</span></p></li>
</ul>
<hr />
<ul>
<li><p>What is meant by “tend to be larger” in the alternative hypothesis?</p></li>
<li>Two common ways of stating the alternative hypothesis for the WRS include
<ol style="list-style-type: decimal">
<li>The stochastic dominance alternative
<span class="math display" id="eq:stochasticlarger-formulation">\[\begin{eqnarray}
H_{0}: &amp; &amp; F_{X} = F_{Y} \quad \textrm{ versus } \nonumber \\
H_{A}: &amp; &amp; F_{X} \textrm{ is stochastically larger than } F_{Y} 
\tag{3.3}
\end{eqnarray}\]</span></li>
<li>The “shift” alternative
<span class="math display" id="eq:shift-formulation">\[\begin{eqnarray}
H_{0}: &amp; &amp; F_{X} = F_{Y} \quad \textrm{ versus } \nonumber \\
H_{A}: &amp; &amp; F_{X}(t) = F_{Y}(t - \Delta), \Delta &gt; 0.
\tag{3.4}
\end{eqnarray}\]</span></li>
</ol></li>
<li><p>A distribution function <span class="math inline">\(F_{X}\)</span> is said to be stochastically larger than
<span class="math inline">\(F_{Y}\)</span> if <span class="math inline">\(F_{X}(t) \leq F_{Y}(t)\)</span> for all <span class="math inline">\(t\)</span> with <span class="math inline">\(F_{X}(t) &lt; F_{Y}(t)\)</span>
for at least one value of <span class="math inline">\(t\)</span>.</p></li>
<li><p>Note that the “shift alternative” implies stochastic dominance.</p></li>
<li><p>Why do we need to specify an alternative?</p></li>
</ul>
<hr />
<ul>
<li><p>It is often stated that the WRS test is a test
of equal medians.</p></li>
<li><p>This is true under the assumption that the
relevant alternative is of the form <span class="math inline">\(F_{X}(t) = F_{Y}(t - \Delta)\)</span>.</p></li>
<li><p>However, one could have a scenario where the two groups have equal medians, but
the WRS test has a very high probability of rejecting <span class="math inline">\(H_{0}\)</span>.</p></li>
<li><p>In addition, in many applications, it is difficult to justify
that the “shift alternative” is a reasonable model.</p></li>
<li><p>An alternative is to view the WRS test as performing the following
hypothesis test:
<span class="math display" id="eq:mw-formulation">\[\begin{eqnarray}
H_{0}: &amp;&amp; P(X_{i} &gt; Y_{j}) + \tfrac{1}{2}P(X_{i} = Y_{j}) = 1/2 \quad \textrm{ versus } \\
H_{A}: &amp;&amp; P(X_{i} &gt; Y_{j}) + \tfrac{1}{2}P(X_{i} = Y_{j}) &gt; 1/2
\tag{3.5}
\end{eqnarray}\]</span>
See <span class="citation">Divine et al. (<a href="#ref-divine2018">2018</a>)</span> for more discussion around this formulation of the
WRS test.</p></li>
<li><p>The hypothesis test <a href="rank-tests.html#eq:mw-formulation">(3.5)</a> makes fewer assumptions
about how <span class="math inline">\(F_{X}\)</span> and <span class="math inline">\(F_{Y}\)</span> are related and is, in many cases, more interpretable.</p></li>
<li><p>For example, in medical applications, it is often more natural to
answer the question: what is the probability that the outcome
under treatment 1 is better than the outcome under treatment 2.</p></li>
<li><p>The justification of hypothesis test <a href="rank-tests.html#eq:mw-formulation">(3.5)</a> comes through
the close connection between the WRS test statistic <span class="math inline">\(W\)</span> and the Mann-Whitney statistic <span class="math inline">\(U_{MW}\)</span>.
Specifically, <span class="math inline">\(W = U_{MW} + n(n+1)/2\)</span>. (Although, often <span class="math inline">\(U_{MW}\)</span> is defined as
<span class="math inline">\(U_{MW} = mn + n(n+1)/2 - W\)</span>).</p></li>
<li><p>The Mann-Whitney statistic divided by <span class="math inline">\(mn\)</span> is an estimate of the probability:
<span class="math display">\[\begin{equation}
P(X_{i} &gt; Y_{j}) + \tfrac{1}{2}P(X_{i} = Y_{j}) = 1/2.
\end{equation}\]</span></p></li>
</ul>
</div>
<div id="definition-of-the-wrs-test-statistic" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Definition of the WRS Test Statistic</h3>
<ul>
<li><p>The WRS test statistic is based on computing the sum of ranks (ranks based on the pooled sample)
in one group.</p></li>
<li><p>If observations from group 1 tend to be larger than those from group 2, the average rank from group 1 should exceed the
average rank from group 2.</p></li>
<li><p>A sufficiently large value of the average rank from group 1 will allow us to reject <span class="math inline">\(H_{0}\)</span>
in favor of <span class="math inline">\(H_{A}\)</span>.</p></li>
</ul>
<hr />
<ul>
<li><p>We will define the pooled data vector <span class="math inline">\(\mathbf{Z}\)</span> as
<span class="math display">\[\begin{equation}
\mathbf{Z} = (X_{1}, \ldots, X_{n}, Y_{1}, \ldots, Y_{m})
\end{equation}\]</span>
This is a vector with length <span class="math inline">\(n + m\)</span>.</p></li>
<li><p>The Wilcoxon rank-sum test statistic <span class="math inline">\(W\)</span> for testing hypotheses of the form <a href="rank-tests.html#eq:general-wilcoxon-hypothesis">(3.2)</a>
is then defined as
<span class="math display" id="eq:wrs-formula">\[\begin{equation}
W = \sum_{i=1}^{n} R_{i}( \mathbf{Z} )
\tag{3.6}
\end{equation}\]</span></p></li>
<li><p>In other words, the WRS test statistic is the sum of the ranks for those observations coming
from group 1 (i.e., the group with the <span class="math inline">\(X_{i}\)</span> as observations).</p></li>
<li><p>If the group 1 observations tend to, in fact, be larger than the group 2 observations,
then we should expect the sum of the ranks in this group to be larger than the sum of the
ranks from group 2.</p></li>
</ul>
<hr />
<ul>
<li><p>Under <span class="math inline">\(H_{0}\)</span>, we can treat both <span class="math inline">\(X_{i}\)</span> and <span class="math inline">\(Y_{i}\)</span> as being observations coming from
a common distribution function <span class="math inline">\(F\)</span>.</p></li>
<li><p>Hence, the expectation of <span class="math inline">\(R_{i}(\mathbf{Z})\)</span> under the null hypothesis is
<span class="math display">\[\begin{equation}
E_{H_{0}}\{ R_{i}(\mathbf{Z}) \} = \frac{n + m + 1}{2}
\end{equation}\]</span>
and thus the expectation of <span class="math inline">\(W\)</span> under <span class="math inline">\(H_{0}\)</span>
<span class="math display">\[\begin{equation}
E_{H_{0}}( W ) = \sum_{i=1}^{n} E_{H_{0}}\{ R_{i}( \mathbf{Z} ) \}
= \frac{ n(n + m + 1)  }{ 2 }
\end{equation}\]</span></p></li>
<li><p>It can be shown that the variance of <span class="math inline">\(W\)</span> under the null hypothesis is
<span class="math display">\[\begin{equation}
\textrm{Var}_{H_{0}}( W ) = \frac{mn(m + n + 1)}{12}
\end{equation}\]</span></p></li>
</ul>
</div>
<div id="computing-p-values-for-the-wrs-test" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Computing p-values for the WRS Test</h3>
<p><strong>Exact Distribution</strong></p>
<ul>
<li><p>The p-value is found by computing the probability
<span class="math display">\[\begin{equation}
\textrm{p-value} = P_{H_{0}}( W \geq w_{obs})
\end{equation}\]</span>
where <span class="math inline">\(w_{obs}\)</span> is the observed WRS test statistic that
we get from our data.</p></li>
<li><p>Computing p-values for the WRS test requires us to
work with the <strong>null distribution</strong> of <span class="math inline">\(W\)</span>. That is,
the distribution of <span class="math inline">\(W\)</span> under the assumption that
<span class="math inline">\(F_{X} = F_{Y}\)</span>.</p></li>
<li><p>The exact null distribution is found by using the fact
that each possible ordering of the ranks has the same probability.
That is,
<span class="math display">\[\begin{equation}
P\{ R_{1}(\mathbf{Z}) = r_{1}, \ldots, R_{n+m}(\mathbf{Z}) =  r_{n+m} \} = \frac{1}{(n + m)!},
\end{equation}\]</span>
where <span class="math inline">\((r_{1}, \ldots, r_{n+m})\)</span> is any permutation of the set <span class="math inline">\(\{1, 2, \ldots, n + m\}\)</span>.
Note that the null distribution only depends on <span class="math inline">\(n\)</span> and <span class="math inline">\(m\)</span>.</p></li>
<li><p>Also, there are <span class="math inline">\({n + m \choose n}\)</span> possible ways to assign distinct ranks to group 1.</p></li>
<li><p>Consider an example with <span class="math inline">\(n = m = 2\)</span>. In this case, there are <span class="math inline">\({4 \choose 2} = 6\)</span> distinct
ways to assign 2 ranks to group 1.
What is the null distribution of the WRS test statistic? Try to verify that
<span class="math display">\[\begin{eqnarray}
P_{H_{0}}( W = 7) &amp;=&amp; 1/6 \nonumber \\
P_{H_{0}}( W = 6 ) &amp;=&amp; 1/6 \nonumber \\
P_{H_{0}}(W = 5) &amp;=&amp; 1/3  \nonumber \\
P_{H_{0}}( W = 4 ) &amp;=&amp; 1/6  \nonumber \\
P_{H_{0}}(W = 3) &amp;=&amp; 1/6. \nonumber 
\end{eqnarray}\]</span></p></li>
</ul>
<hr />
<p><strong>Large-Sample Approximate Distribution</strong></p>
<ul>
<li><p>Looking at <a href="rank-tests.html#eq:wrs-formula">(3.6)</a>, we can see that the
WRS test statistic is a sum of nearly independent random variables
(at least nearly independent for large <span class="math inline">\(n\)</span> and <span class="math inline">\(m\)</span>).</p></li>
<li><p>Thus, we can expect that an appropriately centered and scaled
version of <span class="math inline">\(W\)</span> should be approximately Normally distributed (recall the Central Limit Theorem).</p></li>
<li><p>The standardized version <span class="math inline">\(\tilde{W}\)</span> of the WRS is defined as
<span class="math display">\[\begin{equation}
\tilde{W} = \frac{W - E_{H_{0}}(W)}{ \sqrt{\textrm{Var}_{H_{0}}(W) }  }
= \frac{W - n(n+m+1)/2}{ \sqrt{ mn(n + m + 1)/12 }  }
\end{equation}\]</span></p></li>
<li><p>Under <span class="math inline">\(H_{0}\)</span>, <span class="math inline">\(\tilde{W}\)</span> converges in distribution to a Normal<span class="math inline">\((0,1)\)</span> random variable.</p></li>
<li><p>A p-value using this large-sample approximation would then be computed in the following
way
<span class="math display">\[\begin{eqnarray}
\textrm{p-value} &amp;=&amp; P_{H_{0}}( W \geq w_{obs}) 
= P\Bigg( \frac{W - n(n+m+1)/2}{ \sqrt{ mn(n + m + 1)/12 }  } \geq \frac{w_{obs} - n(n+m+1)/2}{ \sqrt{ mn(n + m + 1)/12 }  }\Bigg)
\nonumber \\
&amp;=&amp; P_{H_{0}}\Big( \tilde{W} \geq \frac{w_{obs} - n(n+m+1)/2}{ \sqrt{ mn(n + m + 1)/12 }  }\Big)
= 1 - \Phi\Bigg( \frac{w_{obs} - n(n+m+1)/2}{ \sqrt{ mn(n + m + 1)/12 }  }  \Bigg), \nonumber
\end{eqnarray}\]</span>
where <span class="math inline">\(\Phi(t)\)</span> denotes the cumulative distribution function of a standard Normal random variable.</p></li>
<li><p>Often, in practice, a continuity correction is applied when using this large-sample approximation.
For example, we would compute the probability <span class="math inline">\(P_{H_{0}}(W \geq w_{obs} - 0.5)\)</span> with the Normal approximation
rather than <span class="math inline">\(P_{H_{0}}(W \geq w_{obs})\)</span> directly.</p></li>
</ul>
<hr />
<ul>
<li><p>Many statistical software packages (including <strong>R</strong>) will not compute p-values using the exact distribution in
the presence of ties.</p></li>
<li><p>The <strong>coin</strong> package in <strong>R</strong> does allow you to perform a permutation test in the presence of ties.</p></li>
<li><p>A “two-sided” Wilcoxon rank sum test can also be performed. The two-sided
hypothesis tests could either be stated as
<span class="math display">\[\begin{eqnarray}
H_{0}: &amp; &amp; F_{X} = F_{Y} \quad \textrm{ versus } \nonumber \\
H_{A}: &amp; &amp; F_{X} \textrm{ is stochastically larger or smaller than } F_{Y} 
\end{eqnarray}\]</span>
or
<span class="math display">\[\begin{eqnarray}
H_{0}: &amp; &amp; F_{X} = F_{Y} \quad \textrm{ versus } \nonumber \\
H_{A}: &amp; &amp; F_{X}(t) = F_{Y}(t - \Delta), \Delta \neq 0.
\end{eqnarray}\]</span>
or
<span class="math display">\[\begin{eqnarray}
H_{0}: &amp;&amp; P(X_{i} &gt; Y_{i}) + \tfrac{1}{2}P(X_{i} = Y_{i}) = 1/2 \quad \textrm{ versus } \\
H_{A}: &amp;&amp; P(X_{i} &gt; Y_{i}) + \tfrac{1}{2}P(X_{i} = Y_{i}) \neq 1/2
\end{eqnarray}\]</span></p></li>
</ul>
<hr />
<p><strong>Exercise 3.3.</strong> Using the exact distribution, what is the smallest
possible one-sided p-value associated with the WRS test
for a fixed value of <span class="math inline">\(n\)</span> and <span class="math inline">\(m\)</span> (assuming the probability of ties is zero)?</p>
<hr />
<!-- * Give exercise, compute p-values for Wilcoxon test where
we have two populations. both are Normally distributed
with mean zero but different variances. -->
</div>
<div id="computing-the-wrs-test-in-r" class="section level3">
<h3><span class="header-section-number">3.2.4</span> Computing the WRS test in R</h3>
<ul>
<li>To illustrate performing the WRS test in <strong>R</strong>, we can use the <strong>wine</strong> dataset from the <strong>rattle.data</strong> package.
This dataset is also available from the UCI Machine Learning Repository.</li>
</ul>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" data-line-number="1"><span class="kw">library</span>(rattle.data)</a>
<a class="sourceLine" id="cb11-2" data-line-number="2"><span class="kw">head</span>(wine)</a></code></pre></div>
<pre><code>##   Type Alcohol Malic  Ash Alcalinity Magnesium Phenols Flavanoids
## 1    1   14.23  1.71 2.43       15.6       127    2.80       3.06
## 2    1   13.20  1.78 2.14       11.2       100    2.65       2.76
## 3    1   13.16  2.36 2.67       18.6       101    2.80       3.24
## 4    1   14.37  1.95 2.50       16.8       113    3.85       3.49
## 5    1   13.24  2.59 2.87       21.0       118    2.80       2.69
## 6    1   14.20  1.76 2.45       15.2       112    3.27       3.39
##   Nonflavanoids Proanthocyanins Color  Hue Dilution Proline
## 1          0.28            2.29  5.64 1.04     3.92    1065
## 2          0.26            1.28  4.38 1.05     3.40    1050
## 3          0.30            2.81  5.68 1.03     3.17    1185
## 4          0.24            2.18  7.80 0.86     3.45    1480
## 5          0.39            1.82  4.32 1.04     2.93     735
## 6          0.34            1.97  6.75 1.05     2.85    1450</code></pre>
<ul>
<li>This dataset contains three types of wine. We will only consider the first two.</li>
</ul>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb13-1" data-line-number="1">wine2 &lt;-<span class="st"> </span><span class="kw">subset</span>(wine, Type<span class="op">==</span><span class="dv">1</span> <span class="op">|</span><span class="st"> </span>Type<span class="op">==</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb13-2" data-line-number="2">wine2<span class="op">$</span>Type &lt;-<span class="st"> </span><span class="kw">factor</span>(wine2<span class="op">$</span>Type)</a></code></pre></div>
<ul>
<li><p>Let’s consider the difference in the level of magnesium across the two types of wine.
<img src="03-rankstat_files/figure-html/unnamed-chunk-6-1.png" width="672" /><img src="03-rankstat_files/figure-html/unnamed-chunk-6-2.png" width="672" /></p></li>
<li><p>Suppose we are interested in testing whether or not magnesium levels in
Type 1 wine are generally larger than magnesium levels in Type 2 wine.
This can be done with the following code</p></li>
</ul>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb14-1" data-line-number="1"><span class="kw">wilcox.test</span>(<span class="dt">x=</span>wine2<span class="op">$</span>Magnesium[wine2<span class="op">$</span>Type<span class="op">==</span><span class="dv">1</span>], <span class="dt">y=</span>wine2<span class="op">$</span>Magnesium[wine2<span class="op">$</span>Type<span class="op">==</span><span class="dv">2</span>], </a>
<a class="sourceLine" id="cb14-2" data-line-number="2">            <span class="dt">alternative=</span><span class="st">&quot;greater&quot;</span>)</a></code></pre></div>
<pre><code>## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  wine2$Magnesium[wine2$Type == 1] and wine2$Magnesium[wine2$Type == 2]
## W = 3381.5, p-value = 8.71e-10
## alternative hypothesis: true location shift is greater than 0</code></pre>
<p>You could also use the following code (just be careful about the ordering of the levels of <strong>Type</strong>)</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb16-1" data-line-number="1"><span class="kw">wilcox.test</span>(Magnesium <span class="op">~</span><span class="st"> </span>Type, <span class="dt">data=</span>wine2, <span class="dt">alternative=</span><span class="st">&quot;greater&quot;</span>)</a></code></pre></div>
<pre><code>## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  Magnesium by Type
## W = 3381.5, p-value = 8.71e-10
## alternative hypothesis: true location shift is greater than 0</code></pre>
<ul>
<li>What is the value of the WRS test statistic? We can code this directly
with the following steps:</li>
</ul>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb18-1" data-line-number="1">W &lt;-<span class="st"> </span><span class="kw">wilcox.test</span>(<span class="dt">x=</span>wine2<span class="op">$</span>Magnesium[wine2<span class="op">$</span>Type<span class="op">==</span><span class="dv">1</span>], <span class="dt">y=</span>wine2<span class="op">$</span>Magnesium[wine2<span class="op">$</span>Type<span class="op">==</span><span class="dv">2</span>])</a>
<a class="sourceLine" id="cb18-2" data-line-number="2"></a>
<a class="sourceLine" id="cb18-3" data-line-number="3">n &lt;-<span class="st"> </span><span class="kw">sum</span>(wine2<span class="op">$</span>Type<span class="op">==</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb18-4" data-line-number="4">m &lt;-<span class="st"> </span><span class="kw">sum</span>(wine2<span class="op">$</span>Type<span class="op">==</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb18-5" data-line-number="5">zz &lt;-<span class="st"> </span><span class="kw">rank</span>(wine2<span class="op">$</span>Magnesium) <span class="co">## vector of pooled ranks</span></a>
<a class="sourceLine" id="cb18-6" data-line-number="6"><span class="kw">sum</span>(zz[wine2<span class="op">$</span>Type<span class="op">==</span><span class="dv">1</span>])  <span class="co">## The WRS test statistic</span></a></code></pre></div>
<pre><code>## [1] 5151.5</code></pre>
<ul>
<li>The statistic returned by the <strong>wilcox.test</strong> function is actually equal to <span class="math inline">\(W - n(n+1)/2\)</span> not <span class="math inline">\(W\)</span></li>
</ul>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb20-1" data-line-number="1"><span class="kw">sum</span>(zz[wine2<span class="op">$</span>Type<span class="op">==</span><span class="dv">1</span>]) <span class="op">-</span><span class="st"> </span>n<span class="op">*</span>(n <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)<span class="op">/</span><span class="dv">2</span></a></code></pre></div>
<pre><code>## [1] 3381.5</code></pre>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb22-1" data-line-number="1">W<span class="op">$</span>statistic</a></code></pre></div>
<pre><code>##      W 
## 3381.5</code></pre>
<ul>
<li><span class="math inline">\(\{ W - n(n+1)/2 \}\)</span> is equal to the Mann-Whitney statistic. Thus, <strong>W$statistic/(mn)</strong> is
an estimate of the probability <span class="math inline">\(P(X_{i} &gt; Y_{j}) + P(X_{i} = Y_{j})/2\)</span>.</li>
</ul>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb24-1" data-line-number="1">W<span class="op">$</span>statistic<span class="op">/</span>(m<span class="op">*</span>n)</a></code></pre></div>
<pre><code>##         W 
## 0.8072332</code></pre>
<ul>
<li>Let’s check how the Mann-Whitney statistic matches a simulation-based estimate of this probability</li>
</ul>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb26-1" data-line-number="1">ind1 &lt;-<span class="st"> </span><span class="kw">which</span>(wine2<span class="op">$</span>Type<span class="op">==</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb26-2" data-line-number="2">ind2 &lt;-<span class="st"> </span><span class="kw">which</span>(wine2<span class="op">$</span>Type<span class="op">==</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb26-3" data-line-number="3">xgreater &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">100</span>)</a>
<a class="sourceLine" id="cb26-4" data-line-number="4"><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>) {</a>
<a class="sourceLine" id="cb26-5" data-line-number="5">    xi &lt;-<span class="st"> </span><span class="kw">sample</span>(ind1, <span class="dt">size=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb26-6" data-line-number="6">    yi &lt;-<span class="st"> </span><span class="kw">sample</span>(ind2, <span class="dt">size=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb26-7" data-line-number="7">    xgreater[k] &lt;-<span class="st"> </span><span class="kw">ifelse</span>(wine2<span class="op">$</span>Magnesium[xi] <span class="op">&gt;</span><span class="st"> </span>wine2<span class="op">$</span>Magnesium[yi], <span class="dv">1</span>, <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb26-8" data-line-number="8">}</a>
<a class="sourceLine" id="cb26-9" data-line-number="9"><span class="kw">mean</span>(xgreater)  <span class="co">## estimate of this probability</span></a></code></pre></div>
<pre><code>## [1] 0.83</code></pre>
</div>
<div id="additional-notes-for-the-wrs-test" class="section level3">
<h3><span class="header-section-number">3.2.5</span> Additional Notes for the WRS test</h3>
<div id="comparing-ordinal-data" class="section level4">
<h4><span class="header-section-number">3.2.5.1</span> Comparing Ordinal Data</h4>
<ul>
<li><p>The WRS test is often suggested when comparing categorical data which are <strong>ordinal</strong>.</p></li>
<li>For example, we might have 4 categories:
<ul>
<li>Poor</li>
<li>Fair</li>
<li>Good</li>
<li>Excellent</li>
</ul></li>
<li><p>In this case, there is a natural ordering of the categories
but any numerical values assigned to these categories would be arbitrary.</p></li>
<li><p>In such cases, we might be interested in testing whether or not outcomes tend to be
better in one group than the other rather than simply comparing whether or not
the distribution is different between the two groups.</p></li>
<li><p>A WRS test is useful here since we can still compute ranks without having to
choose aribtrary numbers for each category.</p></li>
<li><p>Thinking of the “probability greater than alternative <a href="rank-tests.html#eq:mw-formulation">(3.5)</a>”
or “stochastically larger than alternative <a href="rank-tests.html#eq:stochasticlarger-formulation">(3.3)</a>” interpretation
of the WRS test is probably more reasonable than the “shift alternative <a href="rank-tests.html#eq:shift-formulation">(3.4)</a>” interpretation.</p></li>
<li><p>Note that there will probably be many ties when comparing ordinal data.</p></li>
</ul>
<hr />
<ul>
<li><p>The Hodges-Lehmann Estimator <span class="math inline">\(\hat{\Delta}\)</span> is an estimator of <span class="math inline">\(\Delta\)</span> in the location-shift model
<span class="math display">\[\begin{equation}
F_{X}(t) = F_{Y}(t - \Delta) \nonumber
\end{equation}\]</span></p></li>
<li><p>The Hodges-Lehmann is defined as the median difference among all possible (group 1, group 2) pairs.
Specifically,
<span class="math display">\[\begin{equation}
\hat{\Delta} = \textrm{median}\{ (X_{i} - Y_{j}); i=1,\ldots,n; j=1,\ldots,m \} \nonumber
\end{equation}\]</span></p></li>
<li><p>We won’t discuss the Hodges-Lehmann estimator in detail in this course, but in
many statistical software packages, the
Hodges-Lehmann is often reported when computing the WRS test.</p></li>
<li><p>In <strong>R</strong>, the Hodges-Lehmann estimator can be obtained by using the <strong>conf.int=TRUE</strong>
argument in the <strong>wilcox.test</strong> function</p></li>
</ul>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb28-1" data-line-number="1">WC &lt;-<span class="st"> </span><span class="kw">wilcox.test</span>(<span class="dt">x=</span>wine2<span class="op">$</span>Magnesium[wine2<span class="op">$</span>Type<span class="op">==</span><span class="dv">1</span>], <span class="dt">y=</span>wine2<span class="op">$</span>Magnesium[wine2<span class="op">$</span>Type<span class="op">==</span><span class="dv">2</span>],</a>
<a class="sourceLine" id="cb28-2" data-line-number="2">                  <span class="dt">conf.int=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb28-3" data-line-number="3">WC<span class="op">$</span>estimate     <span class="co">## The Hodges-Lehmann estimate</span></a></code></pre></div>
<pre><code>## difference in location 
##               14.00005</code></pre>
</div>
</div>
</div>
<div id="one-sample-tests" class="section level2">
<h2><span class="header-section-number">3.3</span> One Sample Tests</h2>
<div id="the-sign-test" class="section level3">
<h3><span class="header-section-number">3.3.1</span> The Sign Test</h3>
<div id="motivation-and-definition" class="section level4">
<h4><span class="header-section-number">3.3.1.1</span> Motivation and Definition</h4>
<ul>
<li><p>Suppose we have observations <span class="math inline">\(D_{1}, \ldots, D_{n}\)</span> which arise from the following model
<span class="math display">\[\begin{equation}
D_{i} = \theta + \varepsilon_{i}, \nonumber 
\end{equation}\]</span>
where <span class="math inline">\(\varepsilon_{i}\)</span> are iid random variables each with distribution function <span class="math inline">\(F_{\epsilon}\)</span>
that is assumed to have a median of zero.</p></li>
<li><p>The distribution function of <span class="math inline">\(D_{i}\)</span> is then
<span class="math display">\[\begin{equation}
F_{D}(t) = P(D_{i} \leq t) = P(\varepsilon_{i} \leq t - \theta) = F_{\epsilon}(t - \theta)
\end{equation}\]</span></p></li>
<li><p>Likewise the density function <span class="math inline">\(f_{D}(t)\)</span> of <span class="math inline">\(D_{i}\)</span> is given by
<span class="math display">\[\begin{equation}
f_{D}(t) = f_{\epsilon}(t - \theta)
\end{equation}\]</span></p></li>
<li><p>In this context, <span class="math inline">\(\theta\)</span> is usually referred to as a <strong>location parameter</strong>.</p></li>
<li><p>The goal here is to test <span class="math inline">\(H_{0}: \theta = \theta_{0}\)</span>. (Often, <span class="math inline">\(\theta_{0} = 0\)</span>).</p></li>
</ul>
<hr />
<ul>
<li>This sort of test usually comes up in the context of <strong>paired data</strong>.
Common examples include
<ul>
<li>patients compared “pre and post treatment”</li>
<li>students before and after the introduction of a new teaching method</li>
<li>comparison of “matched” individuals who are similar (e.g., same age, sex, education, etc.)</li>
</ul></li>
</ul>
<table border="1">
<tr>
<th>
</th>
<th>
Baseline_Measure
</th>
<th>
Post_Treatment_Measure
</th>
</tr>
<tr>
<td align="center">
Patient 1
</td>
<td align="center">
X1
</td>
<td align="center">
Y1
</td>
</tr>
<tr>
<td align="center">
Patient 2
</td>
<td align="center">
X2
</td>
<td align="center">
Y2
</td>
</tr>
<tr>
<td align="center">
Patient 3
</td>
<td align="center">
X3
</td>
<td align="center">
Y3
</td>
</tr>
<tr>
<td align="center">
Patient 4
</td>
<td align="center">
X4
</td>
<td align="center">
Y4
</td>
</tr>
</table>
<ul>
<li><p>In such cases, we have observations <span class="math inline">\(X_{i}\)</span> and <span class="math inline">\(Y_{i}\)</span> for <span class="math inline">\(i = 1,\ldots n\)</span> where
it is not necessarily reasonable to think of <span class="math inline">\(X_{i}\)</span> and <span class="math inline">\(Y_{i}\)</span> as independent.</p></li>
<li><p>We can define <span class="math inline">\(D_{i} = X_{i} - Y_{i}\)</span> as the difference in the <span class="math inline">\(i^{th}\)</span> pair.</p></li>
<li><p>With this setup, a natural question is whether or not the differences <span class="math inline">\(D_{i}\)</span> tend to be
greater than zero or not.</p></li>
</ul>
<hr />
<ul>
<li><p>The <strong>sign</strong> statistic <span class="math inline">\(S_{n}\)</span> is defined as
<span class="math display" id="eq:sign-statistic">\[\begin{equation}
S = \sum_{i=1}^{n} I( D_{i} &gt; 0)
\tag{3.7}
\end{equation}\]</span></p></li>
<li><p>If the null hypothesis <span class="math inline">\(H_{0}: \theta = 0\)</span> is true, then we should expect that roughly half
of the observations will be positive.</p></li>
<li><p>This suggests that we will reject <span class="math inline">\(H_{0}\)</span> if <span class="math inline">\(S \geq c\)</span> where <span class="math inline">\(c\)</span> is a
number that is greater than <span class="math inline">\(n/2\)</span>.</p></li>
</ul>
</div>
<div id="null-distribution-and-p-values" class="section level4">
<h4><span class="header-section-number">3.3.1.2</span> Null Distribution and p-values</h4>
<ul>
<li><p>Notice that the sign statistic defined in <a href="rank-tests.html#eq:sign-statistic">(3.7)</a> is the sum of independent
Bernoulli random variable.</p></li>
<li><p>That is, we can think of <span class="math inline">\(Z_{i} = I(D_{i} &gt; 0)\)</span> as a random variable with success probability
<span class="math inline">\(p( \theta )\)</span> where the formula for <span class="math inline">\(p( \theta )\)</span> is
<span class="math display">\[\begin{equation}
p(\theta) = P(Z_{i} &gt; 0) = 1 - F_{D}(0) = 1 - F_{\epsilon}( -\theta )
\end{equation}\]</span></p></li>
<li><p>This implies that <span class="math inline">\(S_{n}\)</span> is a binomial random variable
with <span class="math inline">\(n\)</span> trials and success probability <span class="math inline">\(p(\theta)\)</span>.
That is,
<span class="math display">\[\begin{equation}
S \sim \textrm{Binomial}(n, p(\theta) )
\end{equation}\]</span></p></li>
<li><p>Because <span class="math inline">\(p(0) = 1/2\)</span>, <span class="math inline">\(S_{n} \sim \textrm{Binomial}(n, 1/2 )\)</span> under <span class="math inline">\(H_{0}\)</span>.</p></li>
<li><p>Notice that the “null distribution” of the sign statistic is “distribution free”
in the sense that the distribution does not depend on the distribution of <span class="math inline">\(D_{i}\)</span>.</p></li>
<li><p>The p-value for the sign test can be computed by
<span class="math display">\[\begin{equation}
\textrm{p-value} = P_{H_{0}}(S \geq s_{obs}) = \sum_{j=s_{obs}}^{n} {n \choose j} \frac{1}{2^{n}},
\end{equation}\]</span>
where <span class="math inline">\(s_{obs}\)</span> is the observed value of the sign statistic.</p></li>
</ul>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb30-1" data-line-number="1"><span class="co">### How to compute the p-value for the sign test using R</span></a>
<a class="sourceLine" id="cb30-2" data-line-number="2">xx &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>)</a>
<a class="sourceLine" id="cb30-3" data-line-number="3">sign.stat &lt;-<span class="st"> </span><span class="kw">sum</span>(xx <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb30-4" data-line-number="4"><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pbinom</span>(sign.stat <span class="op">-</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">size=</span><span class="dv">100</span>, <span class="dt">prob=</span><span class="dv">1</span><span class="op">/</span><span class="dv">2</span>) <span class="co">## p-value for sign test</span></a></code></pre></div>
<pre><code>## [1] 0.4602054</code></pre>
<ul>
<li>The reason that this is the right expression using <strong>R</strong> is that for any positive integer <span class="math inline">\(w\)</span>
<span class="math display">\[\begin{equation}
P_{H_{0}}(S \geq w) = 1 - P_{H_{0}}(S &lt; w) = 1 - P_{H_{0}}(S \leq w - 1)
\end{equation}\]</span>
and the <strong>R</strong> function <strong>pbinom(t, n, prob)</strong> computes <span class="math inline">\(P(X \leq t)\)</span> where <span class="math inline">\(X\)</span> is
a binomial random variable with <span class="math inline">\(n\)</span> trials and success probability <strong>prob</strong>.</li>
</ul>
</div>
</div>
<div id="the-wilcoxon-signed-rank-test" class="section level3">
<h3><span class="header-section-number">3.3.2</span> The Wilcoxon Signed Rank Test</h3>
<ul>
<li><p>The Wilcoxon signed rank test can be applied
under the same scenario that we used the sign test.</p></li>
<li><p>One criticism of the sign test is that it ignores the magnitude
of the observations.</p></li>
<li><p>For example, the sign test statistic <span class="math inline">\(S\)</span> treats observations
<span class="math inline">\(D_{i} = 0.2\)</span> and <span class="math inline">\(D_{i}=3\)</span> the same.</p></li>
<li><p>The <strong>Wilcoxon signed rank statistic</strong> <span class="math inline">\(T^{+}\)</span> weights the positive
indicators <span class="math inline">\(I( D_{i} &gt; 0)\)</span> by the rank of its absolute value.</p></li>
<li><p>Specifically, the Wilcoxon signed rank statistic is defined as
<span class="math display">\[\begin{equation}
T^{+} = \sum_{i=1}^{n} I( D_{i} &gt; 0)R_{i}( |\mathbf{D}| )
\end{equation}\]</span></p></li>
<li><p>Here, <span class="math inline">\(R_{i}( \mathbf{D})\)</span> is the rank of the <span class="math inline">\(i^{th}\)</span> element from the vector
<span class="math inline">\(|\mathbf{D}| = (|D_{1}|, |D_{2}|, \ldots, |D_{n})\)</span>.</p></li>
</ul>
<hr />
<p><strong>Exercise 3.4.</strong> Suppose we had data <span class="math inline">\((-2, 1, -1/2, 3/2, 3)\)</span>. What would
be the value of the Wilcoxon signed rank statistic?</p>
<hr />
<ul>
<li>Expectation under the null hypothesis..</li>
</ul>
<div id="exact-distribution" class="section level4">
<h4><span class="header-section-number">3.3.2.1</span> Exact Distribution</h4>
</div>
<div id="asymptotic-distribution" class="section level4">
<h4><span class="header-section-number">3.3.2.2</span> Asymptotic Distribution</h4>
</div>
</div>
</div>
<div id="power-and-comparisons-with-parametric-tests" class="section level2">
<h2><span class="header-section-number">3.4</span> Power and Comparisons with Parametric Tests</h2>
<div id="power-of-tests" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Power of Tests</h3>
<ul>
<li>The <strong>power</strong> of a test is the probability
that a test rejects the null hypothesis when the
alternative hypothesis is true.</li>
</ul>
</div>
</div>
<div id="thinking-about-rank-statistics-more-generally" class="section level2">
<h2><span class="header-section-number">3.5</span> Thinking about Rank statistics more generally</h2>
</div>
<div id="notes" class="section level2">
<h2><span class="header-section-number">3.6</span> Notes</h2>
<ul>
<li>Additional reading which covers the material discussed in this chapter includes:
<ul>
<li>Chapters 3-4 from <span class="citation">Hollander, Wolfe, and Chicken (<a href="#ref-hollander2013">2013</a>)</span></li>
</ul></li>
</ul>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-divine2018">
<p>Divine, George W, H James Norton, Anna E Barón, and Elizabeth Juarez-Colunga. 2018. “The Wilcoxon–Mann–Whitney Procedure Fails as a Test of Medians.” <em>The American Statistician</em> 72 (3). Taylor &amp; Francis: 278–86.</p>
</div>
<div id="ref-hollander2013">
<p>Hollander, Myles, Douglas A Wolfe, and Eric Chicken. 2013. <em>Nonparametric Statistical Methods</em>. Vol. 751. John Wiley &amp; Sons.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="getting-started.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="tidy.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["bookdown-start.pdf"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
