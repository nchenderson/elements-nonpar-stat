[
["index.html", "Elements of Nonparametric Statistics Preface", " Elements of Nonparametric Statistics Nicholas Henderson 2020-04-21 Preface This book will serve as the main source of course notes for Biostatistics 685/Statistics 560, Winter 2020. "],
["intro.html", "Chapter 1 Introduction 1.1 What is Nonparametric Statistics? 1.2 Outline of Course 1.3 Example 1: Nonparametric vs. Parametric Two-Sample Testing 1.4 Example 2: Nonparametric Estimation 1.5 Example 3: Confidence Intervals 1.6 Example 4: Nonparametric Regression with a Single Covariate 1.7 Example 5: Classification and Regression Trees (CART)", " Chapter 1 Introduction 1.1 What is Nonparametric Statistics? What is Parametric Statistics? Parametric models refer to probability distributions that can be fully described by a fixed number of parameters that do not change with the sample size. Typical examples include Gaussian Poisson Exponential Beta Could also refer to a regression setting where the mean function is described by a fixed number of parameters. What is Nonparametric Statistics? It is difficult to give a concise, all-encompassing definition, but nonparametric statistics generally refers to statistical methods where there is not a clear parametric component. A more practical definition is that nonparametric statistics refers to flexible statistical procedures where very few assumptions are made regarding the distribution of the data or the form of a regression model. The uses of nonparametric methods in several common statistical contexts are described in Sections 1.3 - 1.7. 1.2 Outline of Course This course is roughly divided into the following 5 categories. Nonparametric Testing Rank-based Tests Permutation Tests Estimation of Basic Nonparametric Quantities The Empirical Distribution Function Density Estimation Nonparametric Confidence Intervals Bootstrap Jacknife Nonparametric Regression Part I (Smoothing Methods) Kernel Methods Splines Local Regression Nonparametric Regression Part II (Machine Learning Methods) Decision Trees/CART Ensemble Methods 1.3 Example 1: Nonparametric vs. Parametric Two-Sample Testing Suppose we have data from two groups. For example, outcomes from two different treatments. Group 1 outcomes: \\(X_{1}, \\ldots, X_{n}\\) an i.i.d (independent and identically distributed) sample from distribution function \\(F_{X}\\). This means that \\[\\begin{equation} F_{X}(t) = P( X_{i} \\leq t) \\quad \\textrm{ for any } 1 \\leq i \\leq n \\nonumber \\end{equation}\\] Group 2 outcomes: \\(Y_{1}, \\ldots, Y_{m}\\) an i.i.d. sample from distribution function \\(F_{Y}\\). \\[\\begin{equation} F_{Y}(t) = P( Y_{i} \\leq t) \\quad \\textrm{ for any } 1 \\leq i \\leq n \\nonumber \\end{equation}\\] To test the impact of a new treatment, we usually want to test whether or not \\(F_{X}\\) differs from \\(F_{Y}\\) in some way. This can be stated in hypothesis testing language as \\[\\begin{eqnarray} H_{0}&amp;:&amp; F_{X} = F_{Y} \\quad \\textrm{( populations are the same)} \\nonumber \\\\ H_{A}&amp;:&amp; F_{X} \\neq F_{Y} \\quad \\textrm{( populations are different)} \\tag{1.1} \\end{eqnarray}\\] Parametric Tests Perhaps the most common parametric test for (1.1) is the t-test. The t-test assumes that \\[\\begin{equation} F_{X} = \\textrm{Normal}(\\mu_{x}, \\sigma^{2}) \\quad \\textrm{ and } \\quad F_{Y} = \\textrm{Normal}(\\mu_{y}, \\sigma^{2}) \\end{equation}\\] Under this parametric assumption, the hypothesis test (1.1) reduces to \\[\\begin{equation} H_{0}: \\mu_{x} = \\mu_{y} \\quad \\textrm{ vs. } \\quad H_{A}: \\mu_{x} \\neq \\mu_{y} \\end{equation}\\] The standard t-statistic (with a pooled estimate of \\(\\sigma^{2}\\)) is the following \\[\\begin{equation} T = \\frac{\\bar{X} - \\bar{Y}}{ s_{p}\\sqrt{\\frac{1}{n} + \\frac{1}{m}} }, \\end{equation}\\] where \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}\\) and \\(\\bar{Y} = \\frac{1}{m}\\sum_{i=1}^{m} Y_{i}\\) are the group-specific sample means and \\(s_{p}^{2}\\) is the pooled estimate of \\(\\sigma^{2}\\) \\[\\begin{equation} s_{p}^{2} = \\frac{1}{m + n - 2}\\Big\\{ \\sum_{i=1}^{n} (X_{i} - \\bar{X})^{2} + \\sum_{i=1}^{m} (Y_{i} - \\bar{Y})^{2} \\Big\\} \\end{equation}\\] The t-test is based on the null distribution of \\(T\\) - the distribution of \\(T\\) under the null hypothesis. Under the assumption of normality, the null distribution of \\(T\\) is a t distribution with \\(n + m - 2\\) degrees of freedom. Notice that the null distribution of \\(T\\) depends on the parametric assumption that both \\(F_{X} = \\textrm{Normal}(\\mu_{x}, \\sigma^{2})\\) and \\(F_{Y} = \\textrm{Normal}(\\mu_{y}, \\sigma^{2})\\). Appealing to the Central Limit Theorem, one could argue that is a quite reasonable assumption. In addition to using the assumption that \\(F_{X} = \\textrm{Normal}(\\mu_{x}, \\sigma^{2})\\) and \\(F_{Y} = \\textrm{Normal}(\\mu_{y}, \\sigma^{2})\\), we used this parametric assumption (at least implicitly) in the formulation of the hypothesis test itself because we assumed that any difference between \\(F_{X}\\) and \\(F_{Y}\\) would be fully described by difference in \\(\\mu_{x}\\) and \\(\\mu_{y}\\). So, in a sense, you are using the assumption of normality twice in the construction of the two-sample t-test. Nonparametric Tests Two-sample nonparametric tests are meant to be “distribution-free”. This means the null distribution of the test statistic does not depend on any parametric assumptions about the two populations \\(F_{X}\\) and \\(F_{Y}\\). Many such tests are based on ranks. The distribution of the ranks under the assumption that \\(F_{X} = F_{Y}\\) do not depend on the form of \\(F_{X}\\) (assuming \\(F_{X}\\) is continuous). Also, the statements of hypotheses tests for nonparametric tests should not rely on any parametric assumptions about \\(F_{X}\\) and \\(F_{Y}\\). For example, \\(H_{A}: F_{X} \\neq F_{Y}\\) or \\(H_{A}: F_{X} \\geq F_{Y}\\). Nonparametric tests usually tradeoff power for greater robustness. In general, if the parametric assumptions are correct, a nonparametric test will have less power than its parametric counterpart. If the parametric assumptions are not correct, parametric tests might have inappropriate type-I error control or lose power. 1.4 Example 2: Nonparametric Estimation Suppose we have \\(n\\) observations \\((X_{1}, \\ldots, X_{n})\\) which are assumed to be i.i.d. (independent and identically distributed). The distribution function of \\(X_{i}\\) is \\(F_{X}\\). Suppose we are interested in estimating the entire distribution function \\(F_{X}\\) rather than specific features of the distribution of \\(X_{i}\\) such as the mean or standard deviation. In a parametric approach to estimating \\(F_{X}\\), we would assume the distribution of \\(X_{i}\\) belongs to some parametric family of distributions. For example, \\(X_{i} \\sim \\textrm{Normal}(\\mu, \\sigma^{2})\\) \\(X_{i} \\sim \\textrm{Exponential}(\\lambda)\\) \\(X_{i} \\sim \\textrm{Beta}(\\alpha, \\beta)\\) If we assume that \\(X_{i} \\sim \\textrm{Normal}( \\mu, \\sigma^{2} )\\), we only need to estimate 2 parameters to fully describe the distribution of \\(X_{i}\\), and the number of parameters will not depend on the sample size. In a nonparametric approach to characterizing the distribution of \\(X_{i}\\), we need to instead estimate the entire distribution function \\(F_{X}\\) or density function \\(f_{X}\\). The distribution function \\(F_{X}\\) is usually estimated by the empirical distribution function \\[\\begin{equation} \\hat{F}_{n}(t) = \\frac{1}{n}\\sum_{i=1}^{n} I( X_{i} \\leq t), \\end{equation}\\] where \\(I()\\) denotes the indicator function. That is, \\(I( X_{i} \\leq t) = 1\\) if \\(X_{i} \\leq t\\), and \\(I(X_{i} \\leq t) = 0\\) if \\(X_{i} &gt; t\\). The empirical distribution function is a discrete distribution function, and it can be thought of as an estimate having \\(n\\) “parameters”. Kernel density estimation is probably the most common nonparametric method for estimating a probability distribution function \\(f_{X}(t) = F_{X}&#39;(t)\\). The density function of \\(X_{i}\\) is often estimated by a kernel density estimator (KDE). This is defined as \\[\\begin{equation} \\hat{f}_{n}(t) = \\frac{1}{n h_{n}} \\sum_{i=1}^{n} K\\Big( \\frac{t - X_{i}}{ h_{n} } \\Big). \\end{equation}\\] \\(K()\\) - the kernel function \\(h_{n}\\) - the bandwidth The KDE is a type of smoothing procedure. 1.5 Example 3: Confidence Intervals Inference for a wide range of statistical procedures is based on the following argument \\[\\begin{equation} \\hat{\\theta}_{n} \\textrm{ has an approximate Normal}\\Big( \\theta, \\widehat{\\textrm{Var}(\\hat{\\theta}_{n})} \\Big) \\textrm{ distribution } \\tag{1.2} \\end{equation}\\] Above, \\(\\hat{\\theta}_{n}\\) is an estimate of a parameter \\(\\theta\\), and \\(\\widehat{\\textrm{Var}(\\hat{\\theta}_{n})}\\) is an estimate of the variance of \\(\\hat{\\theta}_{n}\\). \\(se_{n} = \\sqrt{\\widehat{\\textrm{Var}(\\hat{\\theta}_{n})}}\\) is usually referred to as the standard error. \\(95\\%\\) confidence intervals are reported using the following formula \\[\\begin{equation} [\\hat{\\theta}_{n} - 1.96 se_{n}, \\hat{\\theta}_{n} + 1.96 se_{n} ] \\end{equation}\\] Common examples of this include: \\(\\hat{\\theta}_{n} = \\bar{X}_{n}\\). In this case, appeals to the Central Limit Theorem would justify approximation (1.2). The variance of \\(\\hat{\\theta}_{n}\\) would be \\(\\sigma^{2}/n\\), and the standard error would typically be \\(se_{n} = \\hat{\\sigma}/\\sqrt{n}\\). \\(\\hat{\\theta}_{n} = \\textrm{Maximum Likelihood Estimate of } \\theta\\). In this case, asymptotics would justify the approximate distribution \\(\\hat{\\theta}_{n} \\sim \\textrm{Normal}(\\theta, \\frac{1}{nI(\\theta)} )\\), where \\(I(\\theta)\\) denotes the Fisher information. The standard error in this context is often \\(se_{n} = \\{ n I(\\hat{\\theta}_{n}) \\}^{-1/2}\\). Confidence intervals using (1.2) rely on a parametric approximation to the sampling distribution of the statistic \\(\\hat{\\theta}_{n}\\). Moreover, even if one wanted to use something like (1.2), working out standard error formulas can be a great challenge in more complicated situations. The bootstrap is a simulation-based approach for computing standard errors and confidence intervals. The bootstrap does not rely on any particular parametric assumptions and can be applied in almost any context (though bootstrap confidence intervals can fail to work as desired in some situations). Through resampling from the original dataset, the bootstrap uses many possible alternative datasets to assess the variability in \\(\\hat{\\theta}_{n}\\). OriginalDat Dat1 Dat2 Dat3 Dat4 Obs. 1 0.20 0.20 0.80 0.20 0.30 Obs. 2 0.50 0.20 0.80 0.20 0.70 Obs. 3 0.30 0.30 0.50 0.80 0.20 Obs. 4 0.80 0.30 0.70 0.50 0.50 Obs. 5 0.70 0.70 0.20 0.30 0.20 theta.hat 0.50 0.34 0.60 0.40 0.38 In the above example, we have 4 boostrap replications for the statistic \\(\\hat{\\theta}\\): \\[\\begin{eqnarray} \\hat{\\theta}^{(1)} &amp;=&amp; 0.34 \\\\ \\hat{\\theta}^{(2)} &amp;=&amp; 0.60 \\\\ \\hat{\\theta}^{(3)} &amp;=&amp; 0.40 \\\\ \\hat{\\theta}^{(4)} &amp;=&amp; 0.38 \\end{eqnarray}\\] In the above example, the bootstrap standard error for \\(\\hat{\\theta}_{n}\\) would be the standard deviation of the bootstrap replications \\[\\begin{eqnarray} se_{boot} &amp;=&amp; \\Big( \\frac{1}{3} \\sum_{b=1}^{4} \\{ \\hat{\\theta}^{(b)} - \\hat{\\theta}^{(-)} \\}^{2} \\Big)^{1/2} \\nonumber \\\\ &amp;=&amp; \\Big( (0.34 - 0.43)^{2}/3 + (0.60 - 0.43)^{2}/3 + (0.40 - 0.43)^{2}/3 + (0.38 - 0.43)^{2}/3 \\Big)^{1/2} \\nonumber \\\\ &amp;=&amp; 0.116 \\end{eqnarray}\\] where \\(\\hat{\\theta}^{(-)} = 0.43\\) is the average of the bootstrap replications. One would then report the confidence interval \\([\\hat{\\theta} - 1.96 \\times 0.116, \\hat{\\theta} + 1.96 \\times 0.116]\\). In practice, the number of bootstrap replications is typically much larger than \\(4\\). It is often better to construct confidence intervals using the percentiles from the bootstrap distribution of \\(\\hat{\\theta}\\) rather than use a confidence interval of the form: \\(\\hat{\\theta} \\pm 1.96 \\times se_{boot}\\). Figure 1.1: Bootstrap distribution of the sample standard deviation for the age variable from the kidney fitness data. Dasjed vertical lines are placed at the 2.5 and 97.5 percentiles of the bootstrap distribution. 1.6 Example 4: Nonparametric Regression with a Single Covariate Regression is a common way of modeling the relationship between two different variables. Suppose we have \\(n\\) pairs of observations \\((y_{1}, x_{1}), \\ldots, (y_{n}, x_{n})\\) where \\(y_{i}\\) and \\(x_{i}\\) are suspected to have some association. Linear regression would assume that these \\(y_{i}\\) and \\(x_{i}\\) are related by the following \\[\\begin{equation} y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\varepsilon_{i} \\end{equation}\\] with the assumption \\(\\varepsilon_{i} \\sim \\textrm{Normal}(0, \\sigma^{2})\\) often made. In this model, there are only 3 parameters: \\((\\beta_{0}, \\beta_{1}, \\sigma^{2})\\), and the number of parameters stays fixed for all \\(n\\). The nonparametric counterpart to linear regression is usually formulated in the following way \\[\\begin{equation} y_{i} = m( x_{i} ) + \\varepsilon_{i} \\end{equation}\\] Typically, one makes very few assumptions about the form of the mean function \\(m\\), and it is not assumed \\(m\\) can be described by a finite number of parameters. There are a large number of nonparametric methods for estimating \\(m\\). One popular method is the use of smoothing splines. With smoothing splines, one considers mean functions of the form \\[\\begin{equation} m(x) = \\sum_{j=1}^{n} \\beta_{j}g_{j}(x) \\tag{1.3} \\end{equation}\\] where \\(g_{1}, \\ldots, g_{n}(x)\\) are a collection of spline basis functions. Because of the large number of parameters in (1.3), one should estimate the basis function weights \\(\\beta_{j}\\) through penalized regression \\[\\begin{equation} \\textrm{minimize} \\quad \\sum_{i=1}^{n} \\Big( y_{i} - \\sum_{j=1}^{n} \\beta_{j}g_{j}( x_{i} ) \\Big)^{2} + \\lambda \\sum_{i=1}^{n}\\sum_{j=1}^{n} \\Omega_{ij}\\beta_{i}\\beta_{j} \\tag{1.4} \\end{equation}\\] where \\(\\Omega_{ij} = \\int g_{i}&#39;&#39;(t)g_{j}&#39;&#39;(t) dt\\). Using coefficient estimates \\(\\hat{\\beta}_{1}, \\ldots, \\hat{\\beta}_{n}\\) found from solving (1.3), the nonparametric estimate of the mean function is defined as \\[\\begin{equation} \\hat{m}(x) = \\sum_{j=1}^{n} \\hat{\\beta}_{j}g_{j}(x) \\end{equation}\\] While the estimation in (1.4) resembles parametric estimation for linear regression, notice that the number of parameters to be estimated will change with the sample size. Allowing the number of basis functions to grow with \\(n\\) is important. For a sufficiently large number of basis functions, one should be able to approximate the true mean function \\(m(x)\\) arbitrarily closely. 1.7 Example 5: Classification and Regression Trees (CART) Suppose we now have observations \\((y_{1}, \\mathbf{x}_{1}), \\ldots, (y_{n}, \\mathbf{x}_{n})\\) where \\(y_{i}\\) is a continuous response and \\(\\mathbf{x}_{i}\\) is a p-dimensional vector of covariates. Regression trees are a nonparametric approach for predicting \\(y_{i}\\) from \\(\\mathbf{x}_{i}\\). Here, the regression function is a decision tree rather than some fitted curve. With a decision tree, a final prediction from a covariate vector \\(\\mathbf{x}_{i}\\) is obtained by answering a sequence of “yes or no” questions. When the responses \\(y_{i}\\) are binary, such trees are referred to as classification trees. Hence, the name: classification and regression trees (CART). Classification and regression trees are constructed through recursive partitioning. Recursive partitioning is the process of deciding if and how to split a given node into two child nodes. Tree splits are usually chosen to minimize the “within-node” sum of squares. The size of the final tree is determined by a process of “pruning” the tree with cross-validation determining the best place to stop pruning. Regression trees are an example of a more algorithmic approach to constructing predictions (as opposed to probability modeling in more traditional statistical methods) with a strong emphasis on predictive performance as measured through cross-validation. While single regression trees have the advantage of being directly interpretable, their prediction performance is often not that great. However, using collections of trees can be very effective for prediction and has been used in many popular learning methods. Examples include: random forests, boosting, and Bayesian additive regression trees (BART). Methods such as these can perform well on much larger datasets. We will discuss additional methods if time allows. "],
["getting-started.html", "Chapter 2 Working with R", " Chapter 2 Working with R You can download R by visiting https://www.r-project.org/ and clicking on the download R link. Follow the instructions to complete installation. THe most recent version is version 3.6.2. It is not necessary to use this, but I find RStudio to be a very useful integrated development environment (IDE) for computing with R. RStudio may be downloaded and installed by visiting https://rstudio.com/ "],
["rank-tests.html", "Chapter 3 Rank and Sign Statistics 3.1 Ranks 3.2 The Wilcoxon Rank Sum (WRS) Test: A Two-Sample Test 3.3 One Sample Tests 3.4 Power and Comparisons with Parametric Tests 3.5 Linear Rank Statistics in General 3.6 Additional Reading 3.7 Exercises", " Chapter 3 Rank and Sign Statistics 3.1 Ranks 3.1.1 Definition Suppose we have \\(n\\) observations \\(\\mathbf{X} = (X_{1}, \\ldots, X_{n})\\). The rank \\(R_{i}\\) of the \\(i^{th}\\) observation is defined as \\[\\begin{equation} R_{i} = R_{i}(\\mathbf{X}) = \\sum_{j=1}^{n} I( X_{i} \\geq X_{j}) \\tag{3.1} \\end{equation}\\] where \\[\\begin{equation} I(X_{i} \\geq X_{j}) = \\begin{cases} 1 &amp; \\text{ if } X_{i} \\geq X_{j} \\\\ 0 &amp; \\text{ if } X_{i} &lt; X_{j} \\end{cases} \\end{equation}\\] The largest observation has a rank of \\(n\\). The smallest observation has a rank of \\(1\\) (if there are no ties). I am using the notation \\(R_{i}(\\mathbf{X})\\) to emphasize that the rank of the \\(i^{th}\\) observations depends on the entire vector of observations rather than only on the value of \\(X_{i}\\). You can compute ranks in R using the rank function: x &lt;- c(3, 7, 1, 12, 6) ## 5 observations rank(x) ## [1] 2 4 1 5 3 3.1.2 Handling Ties In the definition of ranks shown in (3.1), tied observations receive their maximum possible rank. For example, suppose that \\((X_{1}, X_{2}, X_{3}, X_{4}) = (0, 1, 1, 2)\\). In this case, one could argue whether both observations 2 and 3 should be ranked \\(2^{nd}\\) or \\(3^{rd}\\) while observations \\(1\\) and \\(4\\) should unambiguously receive ranks of \\(1\\) and \\(4\\) respectively. Under definition (3.1), both observations \\(2\\) and \\(3\\) receive a rank of \\(3\\). In R, handling ties in a way that is consistent with definition (3.1) is done using the ties.method = “max” argument x &lt;- c(0, 1, 1, 2) rank(x, ties.method=&quot;max&quot;) ## [1] 1 3 3 4 The default in R is to replace the ranks of tied observations with their “average” rank x &lt;- c(0, 1, 1, 2) rank(x) ## [1] 1.0 2.5 2.5 4.0 As another example of the “average” definition of ranks, consider the following example: y &lt;- c(2, 9, 7, 7, 3, 2, 1) rank(y, ties.method=&quot;max&quot;) ## [1] 3 7 6 6 4 3 1 rank(y) ## [1] 2.5 7.0 5.5 5.5 4.0 2.5 1.0 When defining ranks using the “average” or “midrank” approach to handling ties, we replace tied ranks with the average of the two “adjacent” ranks. For example, if we have a vector of ranks \\((R_{1}, R_{2}, R_{3}, R_{4})\\) where \\(R_{2} = R_{3} =3\\) and \\(R_{1} = 4\\) and \\(R_{4} = 1\\), then the vector of modified ranks using the “average” approach to handling ties would be \\[\\begin{equation} (R_{1}&#39;, R_{2}&#39;, R_{3}&#39;, R_{4}&#39;) = \\Big( 4, \\frac{4 + 1}{2}, \\frac{4 + 1}{2}, 1 \\Big) \\end{equation}\\] The “average” approach is the most common way of handling ties when computing the Wilcoxon rank sum statistic. 3.1.3 Properties of Ranks Suppose \\((X_{1}, \\ldots, X_{n})\\) is random sample from a continuous distribution \\(F\\) (so that the probability of ties is zero). Then, the following properties hold for the associated ranks \\(R_{1}, \\ldots, R_{n}\\). Each \\(R_{i}\\) follows a discrete uniform distribution \\[\\begin{equation} P(R_{i} = j) = 1/n, \\quad \\text{for any } j = 1, \\ldots,n. \\end{equation}\\] The expectation of \\(R_{i}\\) is \\[\\begin{equation} E( R_{i} ) = \\sum_{j=1}^{n} j P(R_{i} = j) = \\frac{1}{n}\\sum_{j=1}^{n} j = \\frac{(n+1)}{2} \\tag{3.2} \\end{equation}\\] The variance of \\(R_{i}\\) is \\[\\begin{equation} \\text{Var}( R_{i} ) = E( R_{i}^{2} ) - E(R_{i})^{2} = \\frac{1}{n}\\sum_{j=1}^{n} j^{2} - \\Big( \\frac{n+1}{2} \\Big)^{2} = \\frac{ n^{2} - 1}{12} \\end{equation}\\] The random variables \\(R_{1}, \\ldots, R_{n}\\) are not independent (why?). However, the vector \\(\\mathbf{R}_{n} = (R_{1}, \\ldots, R_{n})\\) is uniformly distributed on the set of \\(n!\\) permutations of \\((1,2,\\ldots,n)\\). 3.2 The Wilcoxon Rank Sum (WRS) Test: A Two-Sample Test 3.2.1 Goal of the Test The Wilcoxon Rank Sum (WRS) test (sometimes referred to as the Wilcoxon-Mann-Whitney test) is a popular, rank-based two-sample test. The one-sided WRS test is used to test whether or not observations from one group tend to be larger (or smaller) than observations from the other group. Suppose we have observations from two groups: \\(X_{1}, \\ldots, X_{n} \\sim F_{X}\\) and \\(Y_{1}, \\ldots, Y_{m} \\sim F_{Y}\\). Roughly speaking, the one-sided WRS tests the following hypothesis \\[\\begin{eqnarray} H_{0}: &amp;&amp; F_{X} = F_{Y} \\quad \\textrm{ versus } \\\\ H_{A}: &amp;&amp; \\textrm{Observations from } F_{X} \\textrm{ tend to be larger than observations from } F_{Y} \\nonumber \\tag{3.3} \\end{eqnarray}\\] What is meant by “tend to be larger” in the alternative hypothesis? Two common ways of stating the alternative hypothesis for the WRS include The stochastic dominance alternative \\[\\begin{eqnarray} H_{0}: &amp; &amp; F_{X} = F_{Y} \\quad \\textrm{ versus } \\nonumber \\\\ H_{A}: &amp; &amp; F_{X} \\textrm{ is stochastically larger than } F_{Y} \\tag{3.4} \\end{eqnarray}\\] The “shift” alternative \\[\\begin{eqnarray} H_{0}: &amp; &amp; F_{X} = F_{Y} \\quad \\textrm{ versus } \\nonumber \\\\ H_{A}: &amp; &amp; F_{X}(t) = F_{Y}(t - \\Delta), \\Delta &gt; 0. \\tag{3.5} \\end{eqnarray}\\] A distribution function \\(F_{X}\\) is said to be stochastically larger than \\(F_{Y}\\) if \\(F_{X}(t) \\leq F_{Y}(t)\\) for all \\(t\\) with \\(F_{X}(t) &lt; F_{Y}(t)\\) for at least one value of \\(t\\). Note that the “shift alternative” implies stochastic dominance. Why do we need to specify an alternative? It is often stated that the WRS test is a test of equal medians. This is true under the assumption that the relevant alternative is of the form \\(F_{X}(t) = F_{Y}(t - \\Delta)\\). However, one could have a scenario where the two groups have equal medians, but the WRS test has a very high probability of rejecting \\(H_{0}\\). In addition, in many applications, it is difficult to justify that the “shift alternative” is a reasonable assumption. An alternative is to view the WRS test as performing the following hypothesis test: \\[\\begin{eqnarray} H_{0}: &amp;&amp; P(X_{i} &gt; Y_{j}) + \\tfrac{1}{2}P(X_{i} = Y_{j}) = 1/2 \\quad \\textrm{ versus } \\nonumber \\\\ H_{A}: &amp;&amp; P(X_{i} &gt; Y_{j}) + \\tfrac{1}{2}P(X_{i} = Y_{j}) &gt; 1/2 \\tag{3.6} \\end{eqnarray}\\] See Divine et al. (2018) for more discussion around this formulation of the WRS test. The hypothesis test (3.6) makes fewer assumptions about how \\(F_{X}\\) and \\(F_{Y}\\) are related and is, in many cases, more interpretable. For example, in medical applications, it is often more natural to answer the question: what is the probability that the outcome under treatment 1 is better than the outcome under treatment 2. The justification of hypothesis test (3.6) comes through the close connection between the WRS test statistic \\(W\\) and the Mann-Whitney statistic \\(M\\). Specifically, \\(W = M + n(n+1)/2\\). (Although, often \\(M\\) is defined as \\(M = mn + n(n+1)/2 - W\\)). The Mann-Whitney statistic divided by \\(mn\\) is an estimate of the probability: \\[\\begin{equation} P(X_{i} &gt; Y_{j}) + \\tfrac{1}{2}P(X_{i} = Y_{j}) = 1/2. \\nonumber \\end{equation}\\] The reason for stating \\(H_{0}\\) in (3.6) as \\[\\begin{equation} H_{0}: P(X_{i} &gt; Y_{j}) + \\tfrac{1}{2}P(X_{i} = Y_{j}) = 1/2 \\nonumber \\\\ \\end{equation}\\] is to cover the case of either a continuous or discrete distribution. When both \\(X_{i}\\) and \\(Y_{j}\\) are samples from a continuous distribution we will have \\(P(X_{i} = Y_{j}) = 0\\), and we should then think of the null hypothesis as \\(H_{0}: P(X_{i} &gt; Y_{j})\\). For the case when both \\(X_{i}\\) and \\(Y_{j}\\) have a discrete distribution, consider an example where \\(X_{i}\\) and \\(Y_{j}\\) have the same discrete distribution with probabilities \\(P(X_{i} = 0) = p_{0}, P(X_{i} = 1) = p_{1}\\), and \\(P(X_{i} = 2) = 1 - p_{0} - p_{2}\\). With this common discrete distribution on \\(\\{0, 1, 2\\}\\), we can see that \\(P(X_{i} &gt; Y_{j}) + \\tfrac{1}{2}P(X_{i} = Y_{j}) = 1/2\\) because \\[\\begin{eqnarray} P(X_{i} &gt; Y_{j}) + \\frac{1}{2}P(X_{i} = Y_{j}&amp;)&amp; = P(X_{i}=1, Y_{j}=0) + P(X_{i} = 2, Y_{j}=0) + P(X_{i}=2, Y_{j}=1) \\nonumber \\\\ &amp;+&amp; \\frac{1}{2}\\Big[P(X_{i}=0, Y_{j}=0) + P(X_{i} = 1, Y_{j}=1) + P(X_{i}=2, Y_{j}=2) \\Big] \\nonumber \\\\ &amp;=&amp; p_{1}p_{0} + (1 - p_{1} - p_{0})p_{0} + (1 - p_{1} - p_{0})p_{1} \\nonumber \\\\ &amp;+&amp; p_{0}^{2} + p_{1}^{2} + \\frac{1}{2} - p_{0} - p_{1} + p_{0}p_{1} \\nonumber \\\\ &amp;=&amp; 1/2 \\nonumber \\end{eqnarray}\\] 3.2.2 Definition of the WRS Test Statistic The WRS test statistic is based on computing the sum of ranks (ranks based on the pooled sample) in one group. The motivation for the WRS test statistic is the following: if observations from group 1 tend to be larger than those from group 2, the average rank from group 1 should exceed the average rank from group 2. A sufficiently large value of the average rank from group 1 will allow us to reject \\(H_{0}\\) in favor of \\(H_{A}\\). We will define the pooled data vector \\(\\mathbf{Z}\\) as \\[\\begin{equation} \\mathbf{Z} = (X_{1}, \\ldots, X_{n}, Y_{1}, \\ldots, Y_{m}) \\nonumber \\end{equation}\\] This is a vector with length \\(n + m\\). The Wilcoxon rank-sum test statistic \\(W\\) for testing hypotheses of the form (3.3) is then defined as \\[\\begin{equation} W = \\sum_{i=1}^{n} R_{i}( \\mathbf{Z} ) \\tag{3.7} \\end{equation}\\] In other words, the WRS test statistic is the sum of the ranks for those observations coming from group 1 (i.e., the group with the \\(X_{i}\\) as observations). If the group 1 observations tend to, in fact, be larger than the group 2 observations, then we should expect the sum of the ranks in this group to be larger than the sum of the ranks from group 2. Under \\(H_{0}\\), we can treat both \\(X_{i}\\) and \\(Y_{i}\\) as being observations coming from a common distribution function \\(F\\). Hence, the expectation of \\(R_{i}(\\mathbf{Z})\\) under the null hypothesis is \\[\\begin{equation} E_{H_{0}}\\{ R_{i}(\\mathbf{Z}) \\} = \\frac{n + m + 1}{2} \\nonumber \\end{equation}\\] and thus the expectation of \\(W\\) under \\(H_{0}\\) \\[\\begin{equation} E_{H_{0}}( W ) = \\sum_{i=1}^{n} E_{H_{0}}\\{ R_{i}( \\mathbf{Z} ) \\} \\nonumber = \\frac{ n(n + m + 1) }{ 2 } \\end{equation}\\] It can be shown that the variance of \\(W\\) under the null hypothesis is \\[\\begin{equation} \\textrm{Var}_{H_{0}}( W ) = \\frac{mn(m + n + 1)}{12} \\nonumber \\end{equation}\\] 3.2.3 Computing p-values for the WRS Test Exact Distribution The p-value for the WRS test is found by computing the probability \\[\\begin{equation} \\textrm{p-value} = P_{H_{0}}( W \\geq w_{obs}) \\end{equation}\\] where \\(w_{obs}\\) is the observed WRS test statistic that we get from our data. Computing p-values for the WRS test requires us to work with the null distribution of \\(W\\). That is, the distribution of \\(W\\) under the assumption that \\(F_{X} = F_{Y}\\). The exact null distribution is found by using the fact that each possible ordering of the ranks has the same probability. That is, \\[\\begin{equation} P\\{ R_{1}(\\mathbf{Z}) = r_{1}, \\ldots, R_{n+m}(\\mathbf{Z}) = r_{n+m} \\} = \\frac{1}{(n + m)!}, \\end{equation}\\] where \\((r_{1}, \\ldots, r_{n+m})\\) is any permutation of the set \\(\\{1, 2, \\ldots, n + m\\}\\). Note that the null distribution only depends on \\(n\\) and \\(m\\). Also, there are \\({n + m \\choose n}\\) possible ways to assign distinct ranks to group 1. Consider an example with \\(n = m = 2\\). In this case, there are \\({4 \\choose 2} = 6\\) distinct ways to assign 2 ranks to group 1. What is the null distribution of the WRS test statistic? Try to verify that \\[\\begin{eqnarray} P_{H_{0}}( W = 7) &amp;=&amp; 1/6 \\nonumber \\\\ P_{H_{0}}( W = 6 ) &amp;=&amp; 1/6 \\nonumber \\\\ P_{H_{0}}(W = 5) &amp;=&amp; 1/3 \\nonumber \\\\ P_{H_{0}}( W = 4 ) &amp;=&amp; 1/6 \\nonumber \\\\ P_{H_{0}}(W = 3) &amp;=&amp; 1/6. \\nonumber \\end{eqnarray}\\] Large-Sample Approximate Distribution Looking at (3.7), we can see that the WRS test statistic is a sum of nearly independent random variables (at least nearly independent for large \\(n\\) and \\(m\\)). Thus, we can expect that an appropriately centered and scaled version of \\(W\\) should be approximately Normally distributed (recall the Central Limit Theorem). The standardized version \\(\\tilde{W}\\) of the WRS is defined as \\[\\begin{equation} \\tilde{W} = \\frac{W - E_{H_{0}}(W)}{ \\sqrt{\\textrm{Var}_{H_{0}}(W) } } = \\frac{W - n(n+m+1)/2}{ \\sqrt{ mn(n + m + 1)/12 } } \\end{equation}\\] Under \\(H_{0}\\), \\(\\tilde{W}\\) converges in distribution to a Normal\\((0,1)\\) random variable. A p-value using this large-sample approximation would then be computed in the following way \\[\\begin{eqnarray} \\textrm{p-value} &amp;=&amp; P_{H_{0}}( W \\geq w_{obs}) = P\\Bigg( \\frac{W - n(n+m+1)/2}{ \\sqrt{ mn(n + m + 1)/12 } } \\geq \\frac{w_{obs} - n(n+m+1)/2}{ \\sqrt{ mn(n + m + 1)/12 } }\\Bigg) \\nonumber \\\\ &amp;=&amp; P_{H_{0}}\\Big( \\tilde{W} \\geq \\frac{w_{obs} - n(n+m+1)/2}{ \\sqrt{ mn(n + m + 1)/12 } }\\Big) = 1 - \\Phi\\Bigg( \\frac{w_{obs} - n(n+m+1)/2}{ \\sqrt{ mn(n + m + 1)/12 } } \\Bigg), \\nonumber \\end{eqnarray}\\] where \\(\\Phi(t)\\) denotes the cumulative distribution function of a standard Normal random variable. Often, in practice, a continuity correction is applied when using this large-sample approximation. For example, we would compute the probability \\(P_{H_{0}}(W \\geq w_{obs} - 0.5)\\) with the Normal approximation rather than \\(P_{H_{0}}(W \\geq w_{obs})\\) directly. Many statistical software packages (including R) will not compute p-values using the exact distribution in the presence of ties. The coin package in R does allow you to perform a permutation test in the presence of ties. A “two-sided” Wilcoxon rank sum test can also be performed. The two-sided hypothesis tests could either be stated as \\[\\begin{eqnarray} H_{0}: &amp; &amp; F_{X} = F_{Y} \\quad \\textrm{ versus } \\nonumber \\\\ H_{A}: &amp; &amp; F_{X} \\textrm{ is stochastically larger or smaller than } F_{Y} \\nonumber \\end{eqnarray}\\] or \\[\\begin{eqnarray} H_{0}: &amp; &amp; F_{X} = F_{Y} \\quad \\textrm{ versus } \\nonumber \\\\ H_{A}: &amp; &amp; F_{X}(t) = F_{Y}(t - \\Delta), \\Delta \\neq 0. \\nonumber \\end{eqnarray}\\] or \\[\\begin{eqnarray} H_{0}: &amp;&amp; P(X_{i} &gt; Y_{i}) + \\tfrac{1}{2}P(X_{i} = Y_{i}) = 1/2 \\quad \\textrm{ versus } \\nonumber \\\\ H_{A}: &amp;&amp; P(X_{i} &gt; Y_{i}) + \\tfrac{1}{2}P(X_{i} = Y_{i}) \\neq 1/2 \\nonumber \\end{eqnarray}\\] 3.2.4 Computing the WRS test in R To illustrate performing the WRS test in R, we can use the wine dataset from the rattle.data package. This dataset is also available from the UCI Machine Learning Repository. library(rattle.data) head(wine) ## Type Alcohol Malic Ash Alcalinity Magnesium Phenols Flavanoids Nonflavanoids ## 1 1 14.23 1.71 2.43 15.6 127 2.80 3.06 0.28 ## 2 1 13.20 1.78 2.14 11.2 100 2.65 2.76 0.26 ## 3 1 13.16 2.36 2.67 18.6 101 2.80 3.24 0.30 ## 4 1 14.37 1.95 2.50 16.8 113 3.85 3.49 0.24 ## 5 1 13.24 2.59 2.87 21.0 118 2.80 2.69 0.39 ## 6 1 14.20 1.76 2.45 15.2 112 3.27 3.39 0.34 ## Proanthocyanins Color Hue Dilution Proline ## 1 2.29 5.64 1.04 3.92 1065 ## 2 1.28 4.38 1.05 3.40 1050 ## 3 2.81 5.68 1.03 3.17 1185 ## 4 2.18 7.80 0.86 3.45 1480 ## 5 1.82 4.32 1.04 2.93 735 ## 6 1.97 6.75 1.05 2.85 1450 This dataset contains three types of wine. We will only consider the first two. wine2 &lt;- subset(wine, Type==1 | Type==2) wine2$Type &lt;- factor(wine2$Type) Let us consider the difference in the level of magnesium across the two types of wine. Suppose we are interested in testing whether or not magnesium levels in Type 1 wine are generally larger than magnesium levels in Type 2 wine. This can be done with the following code wilcox.test(x=wine2$Magnesium[wine2$Type==1], y=wine2$Magnesium[wine2$Type==2], alternative=&quot;greater&quot;) ## ## Wilcoxon rank sum test with continuity correction ## ## data: wine2$Magnesium[wine2$Type == 1] and wine2$Magnesium[wine2$Type == 2] ## W = 3381.5, p-value = 8.71e-10 ## alternative hypothesis: true location shift is greater than 0 You could also use the following code to perform this test (just be careful about the ordering of the levels of Type) wilcox.test(Magnesium ~ Type, data=wine2, alternative=&quot;greater&quot;) ## ## Wilcoxon rank sum test with continuity correction ## ## data: Magnesium by Type ## W = 3381.5, p-value = 8.71e-10 ## alternative hypothesis: true location shift is greater than 0 What is the value of the WRS test statistic? We can code this directly with the following steps: W &lt;- wilcox.test(x=wine2$Magnesium[wine2$Type==1], y=wine2$Magnesium[wine2$Type==2]) n &lt;- sum(wine2$Type==1) m &lt;- sum(wine2$Type==2) zz &lt;- rank(wine2$Magnesium) ## vector of pooled ranks sum(zz[wine2$Type==1]) ## The WRS test statistic ## [1] 5151.5 The statistic returned by the wilcox.test function is actually equal to \\(W - n(n+1)/2\\) not \\(W\\) sum(zz[wine2$Type==1]) - n*(n + 1)/2 ## [1] 3381.5 W$statistic ## W ## 3381.5 \\(\\{ W - n(n+1)/2 \\}\\) is equal to the Mann-Whitney statistic. Thus, W$statistic/(mn) is an estimate of the probability \\(P(X_{i} &gt; Y_{j}) + P(X_{i} = Y_{j})/2\\). W$statistic/(m*n) ## W ## 0.8072332 Let’s check how the Mann-Whitney statistic matches a simulation-based estimate of this probability ind1 &lt;- which(wine2$Type==1) ind2 &lt;- which(wine2$Type==2) xgreater &lt;- rep(0, 100) for(k in 1:100) { xi &lt;- sample(ind1, size=1) yi &lt;- sample(ind2, size=1) xgreater[k] &lt;- ifelse(wine2$Magnesium[xi] &gt; wine2$Magnesium[yi], 1, 0) + ifelse(wine2$Magnesium[xi] == wine2$Magnesium[yi], 1/2, 0) } mean(xgreater) ## estimate of this probability ## [1] 0.77 This simulation-based estimate of \\(P(X_{i} &gt; Y_{j}) + P(X_{i} = Y_{j})/2\\) is quite close to the value of the Mann-Whitney statistic divided by \\(mn\\). 3.2.5 Additional Notes for the WRS test 3.2.5.1 Comparing Ordinal Data The WRS test is often suggested when comparing categorical data which are ordinal. For example, we might have 4 categories: Poor Fair Good Excellent In this case, there is a natural ordering of the categories but any numerical values assigned to these categories would be arbitrary. In such cases, we might be interested in testing whether or not outcomes tend to be better in one group than the other rather than simply comparing whether or not the distribution is different between the two groups. A WRS test is useful here since we can still compute ranks without having to choose aribtrary numbers for each category. Thinking of the “probability greater than alternative (3.6)” or “stochastically larger than alternative (3.4)” interpretation of the WRS test is probably more reasonable than the “shift alternative (3.5)” interpretation. Note that there will probably be many ties when comparing ordinal data. 3.2.5.2 The Hodges-Lehmann Estimator The Hodges-Lehmann Estimator \\(\\hat{\\Delta}\\) is an estimator of \\(\\Delta\\) in the location-shift model \\[\\begin{equation} F_{X}(t) = F_{Y}(t - \\Delta) \\nonumber \\end{equation}\\] The Hodges-Lehmann is defined as the median difference among all possible (group 1, group 2) pairs. Specifically, \\[\\begin{equation} \\hat{\\Delta} = \\textrm{median}\\{ (X_{i} - Y_{j}); i=1,\\ldots,n; j=1,\\ldots,m \\} \\nonumber \\end{equation}\\] We won’t discuss the Hodges-Lehmann estimator in detail in this course, but in many statistical software packages, the Hodges-Lehmann is often reported when computing the WRS test. In R, the Hodges-Lehmann estimator can be obtained by using the conf.int=TRUE argument in the wilcox.test function WC &lt;- wilcox.test(x=wine2$Magnesium[wine2$Type==1], y=wine2$Magnesium[wine2$Type==2], conf.int=TRUE) WC$estimate ## The Hodges-Lehmann estimate ## difference in location ## 14.00005 3.3 One Sample Tests 3.3.1 The Sign Test 3.3.1.1 Motivation and Definition The sign test can be thought of as a test of whether or not the median of a distribution is greater than zero (or greater than some other fixed value \\(\\theta_{0}\\)). Frequently, the sign test is applied in the following context: Suppose we have observations \\(D_{1}, \\ldots, D_{n}\\) which arise from the model \\[\\begin{equation} D_{i} = \\theta + \\varepsilon_{i}, \\tag{3.8} \\end{equation}\\] where \\(\\varepsilon_{i}\\) are iid random variables each with distribution function \\(F_{\\epsilon}\\) that is assumed to have a median of zero. Moreover, we will assume the density function \\(f_{\\varepsilon}(t)\\) of \\(\\varepsilon_{i}\\) is symmetric around zero. The distribution function of \\(D_{i}\\) is then \\[\\begin{equation} F_{D}(t) = P(D_{i} \\leq t) = P(\\varepsilon_{i} \\leq t - \\theta) = F_{\\epsilon}(t - \\theta) \\end{equation}\\] Likewise, the density function \\(f_{D}(t)\\) of \\(D_{i}\\) is given by \\[\\begin{equation} f_{D}(t) = f_{\\epsilon}(t - \\theta) \\end{equation}\\] In this context, \\(\\theta\\) is usually referred to as a location parameter. The goal here is to test \\(H_{0}: \\theta = \\theta_{0}\\) vs. \\(H_{A}: \\theta &gt; \\theta_{0}\\). (Often, \\(\\theta_{0} = 0\\)). This sort of test usually comes up in the context of paired data. Common examples include patients compared “pre and post treatment” students before and after the introduction of a new teaching method comparison of “matched” individuals who are similar (e.g., same age, sex, education, etc.) comparing consistency of measurements made on the same objects Baseline_Measure Post_Treatment_Measure Patient 1 Y1 X1 Patient 2 Y2 X2 Patient 3 Y3 X3 Patient 4 Y4 X4 In such cases, we have observations \\(X_{i}\\) and \\(Y_{i}\\) for \\(i = 1,\\ldots n\\) where it is not necessarily reasonable to think of \\(X_{i}\\) and \\(Y_{i}\\) as independent. We can define \\(D_{i} = X_{i} - Y_{i}\\) as the difference in the \\(i^{th}\\) pair. With this setup, a natural question is whether or not the differences \\(D_{i}\\) tend to be greater than zero or not. The sign statistic \\(S_{n}\\) is defined as \\[\\begin{equation} S_{n} = \\sum_{i=1}^{n} I( D_{i} &gt; 0) \\tag{3.9} \\end{equation}\\] If the null hypothesis \\(H_{0}: \\theta = 0\\) is true, then we should expect that roughly half of the observations will be positive. This suggests that we will reject \\(H_{0}\\) if \\(S_{n} \\geq c\\), where \\(c\\) is a number that is greater than \\(n/2\\). 3.3.1.2 Null Distribution and p-values for the Sign Test Notice that the sign statistic defined in (3.9) is the sum of independent Bernoulli random variable. That is, we can think of \\(Z_{i} = I(D_{i} &gt; 0)\\) as a random variable with success probability \\(p( \\theta )\\) where the formula for \\(p( \\theta )\\) is \\[\\begin{equation} p(\\theta) = P(Z_{i} = 1) = P(D_{i} &gt; 0) = 1 - F_{D}(0) = 1 - F_{\\epsilon}( -\\theta ) \\nonumber \\end{equation}\\] This implies that \\(S_{n}\\) is a binomial random variable with \\(n\\) trials and success probability \\(p(\\theta)\\). That is, \\[\\begin{equation} S_{n} \\sim \\textrm{Binomial}(n, p(\\theta) ) \\tag{3.10} \\end{equation}\\] Because \\(p(0) = 1/2\\), \\(S_{n} \\sim \\textrm{Binomial}(n, 1/2 )\\) under \\(H_{0}\\). Notice that the “null distribution” of the sign statistic is “distribution free” in the sense that the null distribution of \\(S_{n}\\) does not depend on the distribution of \\(D_{i}\\). The p-value for the one-sided sign test can be computed by \\[\\begin{equation} \\textrm{p-value} = P_{H_{0}}(S_{n} \\geq s_{obs}) = \\sum_{j=s_{obs}}^{n} P_{H_{0}}(S_{n} = j) = \\sum_{j=s_{obs}}^{n} {n \\choose j} \\frac{1}{2^{n}}, \\nonumber \\end{equation}\\] where \\(s_{obs}\\) is the observed value of the sign statistic. ### How to compute the p-value for the sign test using R xx &lt;- rnorm(100) sign.stat &lt;- sum(xx &gt; 0) ## This is the value of the sign statistic 1 - pbinom(sign.stat - 1, size=100, prob=1/2) ## p-value for sign test ## [1] 0.955687 The reason that this is the right expression using R is that for any positive integer \\(w\\) \\[\\begin{equation} P_{H_{0}}(S_{n} \\geq w) = 1 - P_{H_{0}}(S_{n} &lt; w) = 1 - P_{H_{0}}(S_{n} \\leq w - 1) \\end{equation}\\] and the R function pbinom(t, n, prob) computes \\(P(X \\leq t)\\) where \\(X\\) is a binomial random variable with \\(n\\) trials and success probability prob. You can also perform the one-sided sign test by using the binom.test function in R. btest &lt;- binom.test(sign.stat, n=100, p=0.5, alternative=&quot;greater&quot;) btest$p.value ## [1] 0.955687 3.3.1.3 Two-sided Sign Test Notice that the number of negative values of \\(D_{i}\\) can be expressed as \\[\\begin{equation} \\sum_{i=1}^{n} I(D_{i} &lt; 0) = n - S_{n} \\end{equation}\\] if there are no observations that equal zero exactly. Large value of \\(n - S_{n}\\) would be used in favor of another possible one-sided alternative \\(H_{A}: \\theta &lt; 0\\). If we now want to test the two-sided alternative \\[\\begin{equation} H_{0}: \\theta = 0 \\quad \\textrm{ vs. } \\quad H_{A}: \\theta \\neq 0 \\nonumber \\end{equation}\\] you would need to compute the probability under the null hypothesis of observing a “more extreme” observation than the one that was actually observed. Extreme is defined by thinking about the fact that we would have rejected \\(H_{0}\\) if either \\(S_{n}\\) or \\(n - S_{n}\\) were very large. For example, if \\(n = 12\\), then the expected value of the sign statistic would be \\(6\\). If \\(s_{obs} = 10\\), then the collection of “more extreme” events would be \\(\\leq 2\\) or \\(\\geq 10\\). The two-sided p-value is determined by looking at the tail probabilities on both sides \\[\\begin{equation} \\textrm{p-value} = \\begin{cases} P_{H_{0}}(S_{n} \\geq s_{obs}) + P_{H_{0}}(S_{n} \\leq n - s_{obs}) &amp; \\textrm{ if } s_{obs} \\geq n/2 \\nonumber \\\\ P_{H_{0}}(S_{n} \\leq s_{obs}) + P_{H_{0}}(S_{n} \\geq n - s_{obs}) &amp; \\textrm{ if } s_{obs} &lt; n/2 \\nonumber \\end{cases} \\end{equation}\\] It actually works out that \\[\\begin{equation} \\textrm{p-value} = \\begin{cases} 2 P_{H_{0}}(S_{n} \\geq s_{obs}) &amp; \\textrm{ if } s_{obs} \\geq n/2 \\nonumber \\\\ 2 P_{H_{0}}(S_{n} \\leq s_{obs}) &amp; \\textrm{ if } s_{obs} &lt; n/2 \\nonumber \\end{cases} \\end{equation}\\] Also, you can note that this p-value would be the same that you would get from performing the test \\(H_{0}: p = 1/2\\) vs. \\(H_{A}: p \\neq 1/2\\) when it is assumed that \\(S_{n} \\sim \\textrm{Binomial}(n, p)\\). Another note: It is often suggested that one should drop observations which are exactly zero when performing the sign test. 3.3.2 The Wilcoxon Signed Rank Test The Wilcoxon signed rank test can be applied under the same scenario that we used the sign test. One criticism of the sign test is that it ignores the magnitude of the observations. For example, the sign test statistic \\(S\\) treats observations \\(D_{i} = 0.2\\) and \\(D_{i}=3\\) the same. The Wilcoxon signed rank statistic \\(T_{n}\\) weights the signs of \\(D_{i}\\) by the rank of its absolute value. Specifically, the Wilcoxon signed rank statistic is defined as \\[\\begin{equation} T_{n} = \\sum_{i=1}^{n} \\textrm{sign}( D_{i}) R_{i}( |\\mathbf{D}| ) \\nonumber \\end{equation}\\] where the \\(\\textrm{sign}\\) function is defined as \\[\\begin{equation} \\textrm{sign}(x) = \\begin{cases} 1 &amp; \\textrm{if } x &gt; 0 \\\\ 0 &amp; \\textrm{if } x = 0 \\\\ -1 &amp; \\textrm{if } x &lt; 0 \\end{cases} \\nonumber \\end{equation}\\] Here, \\(R_{i}( |\\mathbf{D}| )\\) is the rank of the \\(i^{th}\\) element from the vector \\(|\\mathbf{D}| = (|D_{1}|, |D_{2}|, \\ldots, |D_{n}|)\\). Intuitively, the Wilcoxon signed rank statistic is measuring whether or not large values of \\(|D_{i}|\\) tend to be associated with positive vs. negative values of \\(D_{i}\\). 3.3.2.1 Asymptotic Distribution As mentioned in the above exercise, the expectation of \\(T_{n}\\) under \\(H_{0}\\) is zero. It can be shown that the variance under the null hypothesis is \\[\\begin{equation} \\textrm{Var}_{H_{0}}( T_{n} ) = \\frac{n(2n + 1)(n + 1)}{6} \\nonumber \\end{equation}\\] Similar, to the large-sample approximation we used for the WRS test, we have the following asymptotic result for the Wilcoxon signed-rank test \\[\\begin{equation} \\frac{T_{n}}{\\sqrt{\\textrm{Var}_{H_{0}}(T_{n}) }} \\longrightarrow \\textrm{Normal}(0,1) \\quad \\textrm{as } n \\longrightarrow \\infty \\nonumber \\end{equation}\\] Because the variance of \\(T\\) is dominated by the term \\(n^{3}/3\\) for very large \\(n\\), we could also say that under \\(H_{0}\\) that \\[\\begin{equation} \\frac{T_{n}}{\\sqrt{n^{3}/3} } \\longrightarrow \\textrm{Normal}(0,1) \\quad \\textrm{as } n \\longrightarrow \\infty \\end{equation}\\] In other words, we can say that \\(T_{n}\\) has an approximately \\(\\textrm{Normal}(0, n^{3}/3)\\) for large \\(n\\). 3.3.2.2 Exact Distribution The exact distribution of the Wilcoxon signed rank statistic \\(T_{n}\\) is somewhat more complicated than the exact distribution of the WRS test statistic. Nevertheless, there exists functions in R for working with this exact distribution. 3.3.3 Using R to Perform the Sign and Wilcoxon Tests Let’s first look at the Meat data from the PairedData R package. This data set contains 20 observations with each observation corresponding to a single piece of meat. For each observation, we have two measures of fat percentage that were obtained different measuring techniques. library(PairedData, quietly=TRUE, warn.conflicts=FALSE) ## loading PairedData package data(Meat) ## loading Meat data head(Meat) ## AOAC Babcock MeatType ## 1 22.0 22.3 Wiener ## 2 22.1 21.8 Wiener ## 3 22.1 22.4 Wiener ## 4 22.2 22.5 Wiener ## 5 24.6 24.9 ChoppedHam ## 6 25.3 25.6 ChooppedPork Define the differences \\(D_{i}\\) as the Babcock measurements minus the AOAC measures. We will drop the single observation that equals zero. DD &lt;- Meat[,2] - Meat[,1] DD &lt;- DD[DD!=0] hist(DD, main=&quot;Meat Data&quot;, xlab=&quot;Difference in Measured Fat Percentage&quot;, las=1) summary(DD) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.60000 -0.25000 0.30000 0.04211 0.40000 1.10000 The Sign Test in R Let’s first test the hypothesis \\(H_{0}: \\theta = 0\\) vs. \\(H_{A}: \\theta \\neq 0\\) using the two-sided sign test. This can be done using the binom.test function binom.test(sum(DD &gt; 0), n = length(DD), p=0.5)$p.value ## [1] 0.6476059 Wilcoxon Signed Rank Test in R You can actually use the function wilcox.test to perform the Wilcoxon signed rank test in addition to the Wilcoxon rank sum test. To perform the Wilcoxon signed rank test in R, you just need to enter data for the x argument and leave the y argument empty. wilcox.test(x=DD) ## Warning in wilcox.test.default(x = DD): cannot compute exact p-value with ties ## ## Wilcoxon signed rank test with continuity correction ## ## data: DD ## V = 118.5, p-value = 0.3534 ## alternative hypothesis: true location is not equal to 0 You will note that the p-value for the Wilcoxon signed rank test is lower than that of the sign test. In general, the Wilcoxon signed rank test is somewhat more “sensitive” than the sign test meaning that it will have a greater tendency to reject \\(H_{0}\\) for small deviations from \\(H_{0}\\). We can explore this sensitivity comparison with a small simulation study. We will consider a scenario where \\(D_{i} = 0.4 + \\varepsilon_{i}\\) with \\(\\varepsilon_{i}\\) having a t distribution with \\(3\\) degrees of freedom. set.seed(1327) n.reps &lt;- 500 ## number of simulation replications samp.size &lt;- 50 ## the sample size wilcox.reject &lt;- rep(0, n.reps) sign.reject &lt;- rep(0, n.reps) for(k in 1:n.reps) { dsim &lt;- .4 + rt(samp.size, df=3) wilcox.reject[k] &lt;- ifelse(wilcox.test(x=dsim)$p.value &lt; 0.05, 1, 0) sign.reject[k] &lt;- ifelse(binom.test(sum(dsim &gt; 0), n=samp.size, p=0.5)$p.value &lt; 0.05, 1, 0) } mean(wilcox.reject) ## proportion of times Wilcoxon signed rank rejected H0 ## [1] 0.614 mean(sign.reject) ## proportion of times Wilcoxon signed rank rejected H0 ## [1] 0.488 3.4 Power and Comparisons with Parametric Tests 3.4.1 The Power Function of a Test The power of a test is the probability that a test rejects the null hypothesis when the alternative hypothesis is true. The alternative hypothesis \\(H_{A}\\) is usually characterized by a large range of values of the parameter of interest. For example, \\(H_{A}: \\theta &gt; 0\\) or \\(H_{A}: \\theta \\neq 0\\). For this reason, it is better to think of power as a function that varies across the range of the alternative hypothesis. To be more precise, we will define the power function as a function of some parameter \\(\\theta\\) where the null hypothesis corresponds to \\(\\theta = \\theta_{0}\\) and the alternative hypothesis represents a range of alternative values of \\(\\theta\\). The power function \\(\\gamma_{n}(\\cdot)\\) of a testing procedure is defined as \\[\\begin{equation} \\gamma_{n}(\\delta) = P_{\\theta=\\delta}\\{ \\textrm{reject } H_{0} \\} \\qquad \\textrm{ for } \\delta \\in H_{A}. \\nonumber \\end{equation}\\] The notation \\(P_{\\theta=\\delta}\\{ \\textrm{reject } H_{0} \\}\\) means that we are computing this probability under the assumption that the parameter of interest \\(\\theta\\) equals \\(\\delta\\). The Approximate Power Function of the Sign Test Let us consider the sign test for testing \\(H_{0}: \\theta = 0\\) vs. \\(\\theta &gt; 0\\). The sign test is based on the value of the sign statistic \\(S_{n}\\). Recalling (3.10), we know that \\(S_{n} \\sim \\textrm{Binomial}(n, p(\\theta))\\). Hence, \\[\\begin{equation} \\sqrt{n}(\\tfrac{S_{n}}{n} - p(\\theta)) \\longrightarrow \\textrm{Normal}\\Big( 0, p(\\theta)(1 - p(\\theta)) \\Big) \\quad \\textrm{as } n \\longrightarrow \\infty \\tag{3.11} \\end{equation}\\] The sign test will reject \\(H_{0}\\) when \\(S_{n} \\geq c_{\\alpha,n}\\) where the constant \\(c_{\\alpha,n}\\) is chosen so that \\(P_{H_{0}}( S_{n} \\geq c_{\\alpha,n} ) = \\alpha\\). Using the large-sample approximation (3.11), you can show that \\[\\begin{equation} c_{\\alpha, n} = \\frac{n + \\sqrt{n}z_{1-\\alpha}}{2}, \\tag{3.12} \\end{equation}\\] where \\(z_{1-\\alpha}\\) denotes the upper \\(1 - \\alpha\\) quantile of the standard normal distribution. In other words, \\(\\Phi( z_{1-\\alpha}) = 1-\\alpha\\). Also, when using large-sample approximation (3.11), the power of this test to detect a value of \\(\\theta = \\delta\\) is given by \\[\\begin{eqnarray} \\gamma_{n}(\\delta) &amp;=&amp; P_{\\theta=\\delta}\\{ S_{n} \\geq c_{\\alpha,n} \\} = P_{\\theta=\\delta}\\Bigg\\{ \\frac{\\sqrt{n}(S_{n}/n - p(\\delta))}{\\sqrt{ p(\\delta)(1 - p(\\delta)) } } \\geq \\frac{ \\sqrt{n}(c_{\\alpha, n}/n - p(\\delta)) }{ \\sqrt{p(\\delta)(1 - p(\\delta))} } \\Bigg\\} \\nonumber \\\\ &amp;=&amp; 1 - \\Phi\\Bigg( \\frac{ \\sqrt{n}(c_{\\alpha,n}/n - p(\\delta)) }{ \\sqrt{p(\\delta)(1 - p(\\delta))} } \\Bigg) \\nonumber \\\\ &amp;=&amp; 1 - \\Phi\\Bigg( \\frac{ z_{1-\\alpha} }{ 2\\sqrt{p(\\delta)(1 - p(\\delta))} } - \\frac{ \\sqrt{n}(p(\\delta) - 1/2) }{ \\sqrt{p(\\delta)(1 - p(\\delta))} }\\Bigg) \\tag{3.13} \\end{eqnarray}\\] Notice that the power of the test depends more directly on the term \\(p(\\delta) = P_{\\theta = \\delta}(D_{i} &gt; 0)\\). Recall from Section 3.3.1 that \\(p(\\delta) = 1 - F_{\\epsilon}(-\\delta)\\), where \\(F_{\\epsilon}\\) is the distribution function of \\(\\varepsilon_{i}\\) in the model \\(D_{i} = \\theta + \\varepsilon_{i}\\). So, in any power or sample size calculation, it would be more sensible to think about plausible values for \\(p(\\delta)\\) rather than \\(\\delta\\) itself. Plus, \\(p(\\delta)\\) has the direct interpretation \\(p(\\delta) = P_{\\theta=\\delta}( D_{i} &gt; 0)\\). 3.4.2 Power Comparisons and Asymptotic Relative Efficiency Notice that for the sign statistic power function shown in (3.13), we have that \\[\\begin{equation} \\lim_{n \\longrightarrow \\infty} \\gamma_{n}(\\delta) = \\begin{cases} \\alpha &amp; \\textrm{ if } \\delta = 0 \\\\ 1 &amp; \\textrm{ if } \\delta &gt; 0 \\end{cases} \\tag{3.14} \\end{equation}\\] The above type of limit for the power function is will be true for most “reasonable” tests. Indeed, a test whose power function satisfies (3.14) is typically called a consistent tests. If nearly all reasonable tests are consistent, then how can we compare tests with respect to their power? One approach is to use simulations to compare power for several plausible alternatives. While this can be useful for a specific application, it limits our ability to make more general statements about power comparisons. Another approach might be to determine for which values of \\((\\delta, n)\\) one test has greater power than another. However, this could be tough to interpret (no test will be uniformly more powerful for all distributions) or even difficult to compute. One way to think about power is to think about the relative efficiency of two testing procedures. The efficiency of a test in this context is the sample size required to achieve a certain level of power. To find the asymptotic relative efficiency, we first need to derive the asymptotic power function. For our hypothesis \\(H_{0}: \\theta = \\theta_{0}\\) vs. \\(H_{A}: \\theta &gt; \\theta_{0}\\), this is defined as \\[\\begin{equation} \\tilde{\\gamma}(\\delta) = \\lim_{n \\longrightarrow \\infty} \\gamma_{n}( \\theta_{0} + \\delta/\\sqrt{n}) \\nonumber \\end{equation}\\] Considering the sequence of “local alternatives” \\(\\theta_{n} = \\theta_{0} + \\delta/\\sqrt{n}\\), we avoid the problem of the power always converging to \\(1\\). It can be shown that \\[\\begin{equation} \\tilde{\\gamma}(\\delta) = 1 - \\Phi\\Bigg( z_{1-\\alpha} - \\delta \\frac{\\mu&#39;(\\theta_{0})}{\\sigma(\\theta_{0})} \\Bigg) \\end{equation}\\] as long as we can find functions \\(\\mu(\\cdot)\\) and \\(\\sigma(\\cdot)\\) such that \\[\\begin{equation} \\frac{\\sqrt{n}(V_{n} - \\mu(\\theta_{n}))}{ \\sigma(\\theta_{n})} \\longrightarrow \\textrm{Normal}(0, 1) \\tag{3.15} \\end{equation}\\] where the test of \\(H_{0}:\\theta = \\theta_{0}\\) vs. \\(H_{A}: \\theta &gt; \\theta_{0}\\) is based on the test statistic \\(V_{n}\\) with rejection of \\(H_{0}\\) occurring whenever \\(V_{n} \\geq c_{\\alpha, n}\\). Statement (3.15) asssumes that the distribution of \\(V_{n}\\) is governed by \\(\\theta_{n}\\) for each \\(n\\). The ratio \\(e(\\theta_{0}) = \\mu&#39;(\\theta_{0})/\\sigma(\\theta_{0})\\) is the asymptotic efficiency of the test. When comparing two tests with efficiency \\(e_{1}(\\theta_{0})\\) and \\(e_{2}(\\theta_{0})\\), the asymptotic relative efficiency of test 1 vs. test 2 is defined as \\[\\begin{equation} ARE_{12}(\\theta_{0}) = \\Big( \\frac{e_{1}(\\theta_{0})}{e_{2}(\\theta_{0})} \\Big)^{2} \\end{equation}\\] Interpretation of Asymptotic Efficiency of Tests Roughly speaking, the asymptotic relative efficiency \\(ARE_{12}( \\theta_{0} )\\) approximately equals \\(n_{2}/n_{1}\\) where \\(n_{1}\\) is the sample size needed for test 1 to achieve power \\(\\beta\\) and \\(n_{2}\\) is the sample size needed for test 2 to achieve power \\(\\beta\\). This is true for an arbitrary \\(\\beta\\). To further justify this interpretation notice that, for large \\(n\\), we should have \\[\\begin{equation} c_{\\alpha, n} \\approx \\mu(\\theta_{0}) + \\frac{ \\sigma(\\theta_{0})z_{1-\\alpha} }{\\sqrt{n}} \\end{equation}\\] (This approximation for \\(c_{\\alpha, n}\\) comes from the asymptotic statement in (3.15)) Now, consider the power for detecting \\(H_{A}: \\theta = \\theta_{A}\\) (where we will assume that \\(\\theta_{A}\\) is “close” to \\(\\theta_{0}\\)). Using (3.15), the approximate power in this setting is \\[\\begin{eqnarray} P_{\\theta_{A}}\\Big( V_{n} \\geq c_{\\alpha, n} \\Big) &amp;=&amp; P_{\\theta_{A} }\\Bigg( \\frac{\\sqrt{n}(V_{n} - \\mu(\\theta_{A} ))}{ \\sigma(\\theta_{A} )} \\geq \\frac{\\sqrt{n}(c_{\\alpha,n} - \\mu(\\theta_{A}))}{ \\sigma(\\theta_{A})} \\Bigg) \\nonumber \\\\ &amp;\\approx&amp; 1 - \\Phi\\Bigg( \\frac{\\sqrt{n}(c_{\\alpha,n} - \\mu(\\theta_{A}))}{ \\sigma(\\theta_{A})} \\Bigg) \\nonumber \\\\ &amp;=&amp; 1 - \\Phi\\Bigg( \\frac{\\sqrt{n}(\\mu(\\theta_{0}) - \\mu(\\theta_{A}))}{ \\sigma(\\theta_{A})} + \\frac{z_{1-\\alpha}\\sigma(\\theta_{0})}{ \\sigma(\\theta_{A})}\\Bigg) \\end{eqnarray}\\] Hence, if we want to achieve a power level of \\(\\beta\\) for the alternative \\(H_{A}: \\theta = \\theta_{A}\\), we need the corresponding sample size \\(n_{\\beta}( \\theta_{A} )\\) to satisfy \\[\\begin{equation} \\frac{\\sqrt{n_{\\beta}(\\theta_{A})}(\\mu(\\theta_{0}) - \\mu(\\theta_{A}))}{ \\sigma(\\theta_{A})} + \\frac{z_{1-\\alpha}\\sigma(\\theta_{0})}{ \\sigma(\\theta_{A})} = z_{1-\\beta} \\end{equation}\\] which reduces to \\[\\begin{equation} n_{\\beta}(\\theta_{A}) = \\Bigg( \\frac{ z_{1-\\beta}\\sigma(\\theta_{A}) - z_{1-\\alpha}\\sigma(\\theta_{0}) }{ \\mu(\\theta_{0}) - \\mu(\\theta_{A}) } \\Bigg)^{2} \\approx \\Bigg( \\frac{ [z_{1-\\beta} - z_{1-\\alpha}]\\sigma(\\theta_{0}) }{ (\\theta_{A} - \\theta_{0})\\mu&#39;(\\theta_{0})} \\Bigg)^{2} \\tag{3.16} \\end{equation}\\] So, if we were comparing two testing procedures and we computed the approximate sample sizes \\(n_{\\beta}^{1}(\\theta_{A})\\) and \\(n_{\\beta}^{2}(\\theta_{A})\\) needed to reach \\(\\beta\\) power for the alternative \\(H_{A}: \\theta = \\theta_{A}\\), the sample size ratio (using approximation (3.16)) would be \\[\\begin{equation} \\frac{ n_{\\beta}^{2}(\\theta_{A}) }{n_{\\beta}^{1}(\\theta_{A}) } = \\Bigg( \\frac{ \\mu_{1}&#39;(\\theta_{0})\\sigma_{2}(\\theta_{0}) }{ \\mu_{2}&#39;(\\theta_{0})\\sigma_{1}(\\theta_{0})} \\Bigg)^{2} = \\textrm{ARE}_{12}(\\theta_{0}) \\end{equation}\\] Notice that \\(\\textrm{ARE}_{12}(\\theta_{0}) &gt; 1\\) indicates that the test \\(1\\) is better than test \\(2\\) because the sample size required for test \\(1\\) would be less than the sample size required for test \\(2\\). It is also worth noting that our justification for the interpretation of \\(\\textrm{ARE}_{12}(\\theta_{0})\\) was not very rigorous or precise, but it is possible to make a more rigorous statement. See, for example, Chapter 13 of Lehmann and Romano (2006) for a more rigorous treatment of relative efficiency. In Lehmann and Romano (2006), they have a result that states (under appropriate assumptions) that \\[\\begin{equation} \\lim_{\\theta \\downarrow \\theta_{0}} \\frac{N_{2}(\\theta)}{N_{1}(\\theta)} = ARE_{12}(\\theta_{0}) \\end{equation}\\] where \\(N_{1}(\\theta)\\) and \\(N_{2}(\\theta)\\) are the sample sizes required to have power \\(\\beta\\) against alternative \\(\\theta\\). 3.4.3 Efficiency Examples The Sign Test Let us return to the example of the sign statistic \\(S_{n}\\) and its use in testing the hypothesis \\(H_{0}: \\theta = 0\\) vs. \\(H_{A}: \\theta &gt; 0\\). Notice that the sign test rejects \\(H_{0}:\\theta=0\\) for \\(V_{n} &gt; c_{\\alpha,n}\\) where \\(V_{n} = S_{n}/n\\) and \\(S_{n}\\) is the sign statistic. When \\(V_{n}\\) is defined this way (3.15) is satisfied when \\(\\mu(\\theta) = p(\\theta)\\) and \\(\\sigma(\\theta) = \\sqrt{p(\\theta)(1 - p(\\theta) )}\\) where \\(p(\\theta) = 1 - F_{\\epsilon}( -\\theta )\\). Thus, the efficiency of the sign test for testing \\(H_{0}: \\theta = 0\\) vs. \\(H_{A}: \\theta &gt; 0\\) is \\[\\begin{equation} \\frac{\\mu&#39;(0)}{\\sigma(0)} = \\frac{p&#39;(0)}{\\sqrt{p(0)(1 - p(0))}} = 2f_{\\epsilon}(0) \\nonumber \\end{equation}\\] where \\(f_{\\epsilon}(t) = F_{\\epsilon}&#39;(t)\\). The One-Sample t-test Assume that we have data \\(D_{1}, \\ldots, D_{n}\\) generated under the same assumption as in our discussion of the sign test and the Wilcoxon signed-rank test. That is, \\[\\begin{equation} D_{i} = \\theta + \\varepsilon_{i}, \\nonumber \\end{equation}\\] where \\(\\varepsilon_{i}\\) are assumed to have median \\(0\\) with \\(\\varepsilon_{i}\\) having p.d.f. \\(f_{\\varepsilon}\\) The one-sample t-test will reject \\(H_{0}: \\theta = 0\\) whenever \\(V_{n} &gt; c_{\\alpha, n}\\), where \\(V_{n}\\) is defined to be \\[\\begin{equation} V_{n} = \\frac{\\bar{D}}{ \\hat{\\sigma} } \\nonumber \\end{equation}\\] Note that (3.15) will apply if we choose \\[\\begin{eqnarray} \\mu(\\theta) &amp;=&amp; E_{\\theta}(D_{i}) = \\theta \\nonumber \\\\ \\sigma(\\theta) &amp;=&amp; \\sqrt{\\textrm{Var}_{\\theta}(D_{i})} = \\sqrt{\\textrm{Var}(\\varepsilon_{i})} = \\sigma_{\\epsilon} \\nonumber \\end{eqnarray}\\] These choices of \\(\\mu(\\theta)\\) and \\(\\sigma(\\theta)\\) work because \\[\\begin{eqnarray} \\frac{\\sqrt{n}(V_{n} - \\mu(\\theta_{n}))}{\\sigma(\\theta_{n})} &amp;=&amp; \\frac{\\sqrt{n}(\\bar{D} - \\theta_{n})}{\\sigma_{e}} + \\sqrt{n}\\theta_{n}\\Big( \\frac{1}{\\hat{\\sigma}} - \\frac{1}{\\sigma_{e}} \\Big) \\nonumber \\\\ &amp;=&amp; \\frac{\\sqrt{n}(\\bar{D} - \\theta_{n})}{\\sigma_{e}} + \\delta\\Big( \\frac{1}{\\hat{\\sigma}} - \\frac{1}{\\sigma_{e}} \\Big) \\nonumber \\\\ &amp;\\longrightarrow&amp; \\textrm{Normal}(0, 1) \\nonumber \\end{eqnarray}\\] So, the efficiency of the one-sample t-test is given by \\[\\begin{equation} \\frac{\\mu&#39;(0)}{\\sigma(0)} = \\frac{1}{ \\sigma_{e} } \\nonumber \\end{equation}\\] The Wilcoxon Rank Sum Test Using the close relation between the WRS test statistic and the Mann-Whitney statistic, the WRS test can be represented as rejecting \\(H_{0}\\) when \\(V_{N} \\geq c_{\\alpha, N}\\) where \\(V_{N}\\) is \\[\\begin{equation} V_{N} = \\frac{1}{mn} \\sum_{i=1}^{n}\\sum_{j=1}^{m} I(X_{i} \\geq Y_{j}) \\nonumber \\end{equation}\\] and \\(N = n + m\\). The power of the WRS test is usually analyzed in the context of the “shift alternative”. Namely, we are assuming that \\(F_{X}(t) = F_{Y}(t - \\theta)\\) and test \\(H_{0}: \\theta=0\\) vs. \\(H_{A}: \\theta &gt; 0\\). The natural choice for \\(\\mu(\\theta)\\) is the expectation of \\(V_{N}\\) when \\(\\theta\\) is the true shift parameter. So, let \\(\\mu(\\theta) = P_{\\theta}(X_{i} \\geq Y_{j})\\). This can be written in terms of \\(F_{Y}\\) and \\(f_{Y}\\): \\[\\begin{eqnarray} \\mu(\\theta) &amp;=&amp; \\int_{-\\infty}^{\\infty} P_{\\theta}( X_{i} \\geq Y_{j} | Y_{j}=t) f_{Y}(t) dt = \\int_{-\\infty}^{\\infty} P_{\\theta}( X_{i} \\geq t) f_{Y}(t) dt \\nonumber \\\\ &amp;=&amp; \\int_{-\\infty}^{\\infty} \\{1 - F_{X}(t) \\} f_{Y}(t) dt = 1 - \\int_{-\\infty}^{\\infty} F_{Y}(t - \\theta) f_{Y}(t) dt \\end{eqnarray}\\] You can show that (3.15) holds (see e.g, Chapter 14 of Van der Vaart (2000)) if you choose \\(\\sigma^{2}(\\theta)\\) to be \\[\\begin{eqnarray} \\sigma^{2}(\\theta) &amp;=&amp; \\frac{1}{1 - \\lambda}\\textrm{Var}\\{ F_{Y}(X_{i}) \\} + \\frac{1}{\\lambda} \\textrm{Var}\\{ F_{Y}(Y_{i} - \\theta) \\} \\end{eqnarray}\\] Here, \\(n/(m + n) \\longrightarrow \\lambda\\). Thus, the efficiency of testing \\(H_{0}: \\theta = 0\\) for the WRS test is \\[\\begin{equation} e(0) = \\frac{\\mu&#39;(0)}{\\sigma(0)} = \\frac{\\int_{-\\infty}^{\\infty} f^{2}(t) dt}{\\sigma(0)} \\end{equation}\\] 3.4.4 Efficiency Comparisons for Several Distributions Sign Test vs. One-Sample t-test Comparisons of the Efficiency of the sign and one-sample t-test only require us to find \\(f_{\\epsilon}(0)\\) and \\(\\sigma_{e}^{2}\\) for different assumptions about the residual density \\(f_{\\epsilon}\\). For the Logistic(0,1) distribution, \\(f_{\\epsilon}(0) = 1/4\\) and the standard deviation is \\(\\pi/\\sqrt{3}\\). Hence, the asymptotic relative efficiency of the sign test vs. the one-sample t-test is \\((\\pi/2\\sqrt{3})^{2} = \\pi^{2}/12\\). The relative efficiencies for the sign vs. t-test for other distributions are shown below \\[\\begin{eqnarray} \\textrm{Distribution} &amp; &amp; \\quad \\textrm{Efficiency} \\\\ \\textrm{Normal}(0,1) &amp; &amp; \\qquad 2/\\pi \\\\ \\textrm{Logistic}(0,1) &amp; &amp; \\qquad \\pi^{2}/12 \\\\ \\textrm{Laplace}(0,1) &amp; &amp; \\qquad 2 \\\\ \\textrm{Uniform}(-1, 1) &amp; &amp; \\qquad 1/3 \\\\ \\textrm{t-dist}_{\\nu} &amp; &amp; \\qquad [4(\\nu/(\\nu-2))\\Gamma^{2}\\{ (\\nu + 1)/2\\}]/[ \\Gamma^{2}(\\nu/2)\\nu \\pi ] \\end{eqnarray}\\] WRS Test vs. Two-Sample t-test The relative efficiencies for the WRS test vs. the two-sample t-test for several distributions are shown below. \\[\\begin{eqnarray} \\textrm{Distribution} &amp; &amp; \\quad \\textrm{Efficiency} \\\\ \\textrm{Normal}(0,1) &amp; &amp; \\qquad 3/\\pi \\\\ \\textrm{Logistic}(0,1) &amp; &amp; \\qquad \\pi^{2}/9 \\\\ \\textrm{Laplace}(0,1) &amp; &amp; \\qquad 3/2 \\\\ \\textrm{Uniform}(-1, 1) &amp; &amp; \\qquad 1 \\\\ \\textrm{t-dist}_{3} &amp; &amp; \\qquad 1.24 \\\\ \\textrm{t-dist}_{5} &amp; &amp; \\qquad 1.90 \\\\ \\end{eqnarray}\\] 3.4.5 A Power “Contest” To compare power for specific sample sizes, effect sizes, and distributional assumptions, a simulation study can be more helpful than statements about asymptotic relative efficiency. Below shows the results of a simulation study in R which compares power for the one-sample testing problem. This simulation study compares the sign test, Wilcoxon signed rank test, and the one-sample t-test. It is assumed that \\(n = 200\\) and that responses \\(D_{i}\\) are generated from the following model: \\[\\begin{equation} D_{i} = 0.2 + \\varepsilon_{i} \\nonumber \\end{equation}\\] Three choices for the distribution of \\(\\varepsilon_{i}\\) were considered: \\(\\varepsilon_{i} \\sim \\textrm{Logistic}(0, 1)\\) \\(\\varepsilon_{i} \\sim \\textrm{Normal}(0, 1)\\) \\(\\varepsilon_{i} \\sim \\textrm{Uniform}(-3/2, 3/2)\\) The R code and simulation results are shown below. set.seed(148930) theta &lt;- 0.2 n &lt;- 200 nreps &lt;- 500 RejectSign &lt;- RejectWilcoxonSign &lt;- RejectT &lt;- matrix(NA, nrow=nreps, ncol=4) for(k in 1:nreps) { xx &lt;- theta + rlogis(n) yy &lt;- theta + rnorm(n) zz &lt;- theta + runif(n, min=-3/2, max=3/2) ww &lt;- theta + (rexp(n, rate=1) - rexp(n, rate=1))/sqrt(2) RejectSign[k,1] &lt;- ifelse(binom.test(x=sum(xx &gt; 0), n=n, p=0.5)$p.value &lt; 0.05, 1, 0) RejectWilcoxonSign[k,1] &lt;- ifelse(wilcox.test(xx)$p.value &lt; 0.05, 1, 0) RejectT[k,1] &lt;- ifelse(t.test(xx)$p.value &lt; 0.05, 1, 0) RejectSign[k,2] &lt;- ifelse(binom.test(x=sum(yy &gt; 0), n=n, p=0.5)$p.value &lt; 0.05, 1, 0) RejectWilcoxonSign[k,2] &lt;- ifelse(wilcox.test(yy)$p.value &lt; 0.05, 1, 0) RejectT[k,2] &lt;- ifelse(t.test(yy)$p.value &lt; 0.05, 1,0) RejectSign[k,3] &lt;- ifelse(binom.test(x=sum(zz &gt; 0), n=n, p=0.5)$p.value &lt; 0.05, 1, 0) RejectWilcoxonSign[k,3] &lt;- ifelse(wilcox.test(zz)$p.value &lt; 0.05, 1, 0) RejectT[k,3] &lt;- ifelse(t.test(zz)$p.value &lt; 0.05,1,0) RejectSign[k,4] &lt;- ifelse(binom.test(x=sum(ww &gt; 0), n=n, p=0.5)$p.value &lt; 0.05, 1, 0) RejectWilcoxonSign[k,4] &lt;- ifelse(wilcox.test(ww)$p.value &lt; 0.05, 1, 0) RejectT[k,4] &lt;- ifelse(t.test(ww)$p.value &lt; 0.05, 1, 0) } power.results &lt;- data.frame(Distribution=c(&quot;Logistic&quot;, &quot;Normal&quot;, &quot;Uniform&quot;, &quot;Laplace&quot;), SignTest=colMeans(RejectSign), WilcoxonSign=colMeans(RejectWilcoxonSign), tTest=colMeans(RejectT)) Estimated power for three one-sample tests and three distributions. 500 simulation replications were used. Distribution SignTest WilcoxonSign tTest Logistic 0.25 0.37 0.34 Normal 0.59 0.77 0.81 Uniform 0.44 0.87 0.90 Laplace 0.93 0.92 0.81 3.5 Linear Rank Statistics in General 3.5.1 Definition The Wilcoxon rank sum statistic is an example of a statistic that belongs to a more general class of rank statistics. This is the class of linear rank statistics. Suppose we have observations \\(\\mathbf{Z} = (Z_{1}, \\ldots, Z_{N})\\). A linear rank statistic is a statistic \\(T_{N}\\) that can be expressed as \\[\\begin{equation} T_{N} = \\sum_{i=1}^{N} c_{iN} a_{N}\\big( R_{i}( \\mathbf{Z} ) \\big) \\tag{3.17} \\end{equation}\\] The terms \\(c_{1N}, \\ldots, c_{NN}\\) are usually called coefficients. These are fixed numbers and are not random variables. The terms \\(a_{N}(R_{i}( \\mathbf{Z} ) )\\) are commonly referred to as scores. Typically, the scores are generated from a given function \\(\\psi\\) in the following way \\[\\begin{equation} a_{N}(i) = \\psi\\Big( \\frac{i}{N+1} \\Big) \\nonumber \\end{equation}\\] Example: WRS statistic For the Wilcoxon rank sum test, we separated the data \\(\\mathbf{Z} = (Z_{1}, \\ldots, Z_{N})\\) into two groups. The first \\(n\\) observations were from group 1 while the last \\(m\\) observations were from group 2. The WRS statistic was then defined as \\[\\begin{equation} W = \\sum_{i=1}^{n} R_{i}(\\mathbf{Z}) \\nonumber \\end{equation}\\] In this case, the WRS statistic can be expressed in the form (3.17) if we choose the coefficients to be the following \\[\\begin{equation} c_{iN} = \\begin{cases} 1 &amp; \\textrm{ if } i \\leq n \\\\ 0 &amp; \\textrm{ if } i &gt; n \\end{cases} \\nonumber \\end{equation}\\] and we choose the scores to be \\[\\begin{equation} a_{N}(i) = i \\nonumber \\end{equation}\\] 3.5.2 Properties of Linear Rank Statistics The expected value of the linear rank statistic (if the distribution of the \\(Z_{i}\\) is continuous) is \\[\\begin{equation} E(T_{N}) = N\\bar{c}_{N}\\bar{a}_{N}, \\tag{3.18} \\end{equation}\\] where \\(\\bar{c}_{N} = \\frac{1}{N} \\sum_{j=1}^{N} c_{jN}\\) and \\(\\bar{a}_{N} = \\frac{1}{N}\\sum_{j=1}^{N} a_{N}(j)\\) The formula (3.18) for the expectation only uses the fact that \\(R_{i}(\\mathbf{Z})\\) has a discrete uniform distribution. So, \\[\\begin{equation} E\\{ a_{N}( R_{i}(\\mathbf{Z} ) \\} = \\sum_{j=1}^{N} a_{N}(j)P\\{ R_{i}( \\mathbf{Z}) = j \\} = \\sum_{j=1}^{N} \\frac{ a_{N}(j) }{N} = \\bar{a}_{N} \\nonumber \\end{equation}\\] Using this, we can then see that \\[\\begin{equation} E( T_{N} ) = \\sum_{j=1}^{N} c_{jN} E\\{ a_{N}(R_{i}(\\mathbf{Z})) \\} = \\sum_{j=1}^{N} c_{jN}\\bar{a}_{N} = N\\bar{c}_{N}\\bar{a}_{N} \\nonumber \\end{equation}\\] A similar argument can show that the variance of \\(T_{N}\\) is \\[\\begin{equation} \\textrm{Var}( T_{N} ) = \\frac{N^{2}}{n-1} \\sigma_{a}^{2}\\sigma_{c}^{2}, \\nonumber \\end{equation}\\] where \\(\\sigma_{c}^{2} = \\frac{1}{N}\\sum_{j=1}^{N} (c_{jN} - \\bar{c}_{N})^{2}\\) and \\(\\sigma_{a}^{2} = \\frac{1}{N}\\sum_{j=1}^{N} (a_{N}(j) - \\bar{a}_{N})^{2}\\) To perform hypothesis testing when using a general linear rank statistics, working with the exact distribution or performing permutation tests can often be computationally demanding. Using a large-sample approximation is often easier. As long as a few conditions for the coefficients and scores are satisfied, one can state the following \\[\\begin{equation} \\frac{T_{N} - E( T_{N})}{\\sqrt{\\textrm{Var}(T_{N})}} \\longrightarrow \\textrm{Normal}(0, 1), \\nonumber \\end{equation}\\] where, as we showed, both \\(E(T_{N})\\) and \\(\\textrm{Var}(T_{N})\\) both have closed-form expressions for an arbitrary linear rank statistic. 3.5.3 Other Examples of Linear Rank Statistics 3.5.3.1 The van der Waerden statistic and the normal scores test Van der Waerden’s rank statistic is used for two-sample problems where the first \\(n\\) observations come from group 1 while the last \\(m\\) observations come from group 2. Van der Waerden’s rank statistic \\(VW_{N}\\) is defined as \\[\\begin{equation} VW_{N} = \\sum_{j=1}^{n} \\Phi^{-1}\\Bigg( \\frac{\\mathbf{R}_{i}( \\mathbf{Z})}{N+1} \\Bigg) \\nonumber \\end{equation}\\] The function \\(\\Phi^{-1}\\) denotes the inverse of the cumulative distribution function of a standard Normal random variable. The statistic \\(VW_{N}\\) is a linear rank statistic with coefficients \\[\\begin{equation} c_{iN} = \\begin{cases} 1 &amp; \\textrm{ if } i \\leq n \\\\ 0 &amp; \\textrm{ if } i &gt; n \\end{cases} \\nonumber \\end{equation}\\] and scores determined by \\[\\begin{equation} a_{N}(i) = \\Phi^{-1}\\Big( \\frac{i}{N+1} \\Big) \\nonumber \\end{equation}\\] A test based on van der Waerden’s statistic is often referred to as the normal scores test. The normal scores test is often suggested as an attractive test when the underlying data has an approximately normal distribution. If you plot a histogram of the van der Waerden scores \\(a_{N}(i)\\) it should look roughly like a Gaussian distribution (if there are not too many ties). 3.5.3.2 The median test The median test is also a two-sample rank test. While the Wilcoxon rank sum test looks at the average rank within group \\(1\\), the median test instead looks at how many of the ranks from group \\(1\\) are less than the median rank (which should equal \\((N+1)/2\\)). The test statistic \\(M_{N}\\) for the median test is defined as \\[\\begin{equation} M_{N} = \\sum_{i=1}^{n} I\\Big( R_{i}(\\mathbf{Z}) \\leq \\frac{N+1}{2} \\Big) \\nonumber \\end{equation}\\] because \\((N+1)/2\\) will be the median rank. This is a linear rank statistic with coefficients \\[\\begin{equation} c_{iN} = \\begin{cases} 1 &amp; \\textrm{ if } i \\leq n \\\\ 0 &amp; \\textrm{ if } i &gt; n \\end{cases} \\nonumber \\end{equation}\\] and scores \\[\\begin{equation} a_{N}(i) = \\begin{cases} 1 &amp; \\textrm{ if } i \\leq (N+1)/2 \\\\ 0 &amp; \\textrm{ if } i &gt; (N+1)/2 \\end{cases} \\nonumber \\end{equation}\\] The median test could be used to test whether or not observations from group 1 tend to be smaller than those from group 2. 3.5.4 Choosing the scores \\(a_{N}(i)\\) The rank tests we have discussed so far are nonparametric in the sense that their null distribution does not depend on any particular parametric assumptions about the distributions from which the observations arise. For power calculations, we often think of some parameter or “effect size” modifying the base distribution in some way. For example, we often think of the shift alternative \\(F_{X}(t) = F_{Y}(t - \\theta)\\) in the two-sample problem. In parametric statistics, when testing \\(H_{0}:\\theta = 0\\) the most powerful test of \\(H_{0}: \\theta = \\theta_{0}\\) vs. \\(H_{A}:\\theta = \\theta_{A}\\) is based on rejecting \\(H_{0}\\) whenever the likelihood ratio is large enough: \\[\\begin{equation} \\textrm{Reject } H_{0} \\textrm{ if: } \\quad \\frac{p_{\\theta_{A}}(\\mathbf{z})}{p_{\\theta_{0}}(\\mathbf{z})} \\geq c_{\\alpha, n} \\tag{3.19} \\end{equation}\\] This is the Neyman-Pearson Lemma. The same property is true if we are considering tests based on ranks. The most powerful test for testing \\(H_{0}: \\theta = \\theta_{0}\\) vs. \\(H_{A}:\\theta = \\theta_{A}\\) is based on \\[\\begin{equation} \\textrm{Reject } H_{0} \\textrm{ if: } \\quad \\frac{P_{\\theta_{A}}\\Big( R_{1}(\\mathbf{Z}), \\ldots, R_{N}(\\mathbf{Z}) \\Big)}{ P_{\\theta_{0}}\\Big( R_{1}(\\mathbf{Z}), \\ldots, R_{N}(\\mathbf{Z}) \\Big) } \\geq c_{\\alpha, n} \\tag{3.20} \\end{equation}\\] The main difference between (3.19) and (3.19) is that the distribution \\(P_{\\theta_{A}}\\Big( R_{1}(\\mathbf{Z}), \\ldots, R_{N}(\\mathbf{Z}) \\Big)\\) is unknown unless we are willing to make certain distributional assumptions. Nevertheless, we can approximate this probability if \\(\\theta_{A}\\) is a location parameter “close” to \\(\\theta_{0}\\) \\[\\begin{equation} P_{\\theta_{A}}\\Big( R_{1}(\\mathbf{Z}), \\ldots, R_{N}(\\mathbf{Z}) \\Big) \\approx P_{\\theta_{0}}\\Big( R_{1}(\\mathbf{Z}), \\ldots, R_{N}(\\mathbf{Z}) \\Big) + \\frac{\\theta_{A}}{N!}\\sum_{i=1}^{N} c_{iN} E\\Bigg\\{ \\frac{\\partial \\log f(Z_{(i)})}{ \\partial Z} \\Bigg\\} \\nonumber \\end{equation}\\] where \\(Z_{(i)}\\) denotes the \\(i^{th}\\) order statistic. See, for example, Chapter 13 of Van der Vaart (2000) for more details on the derivation of this approximation. So, large values of the linear rank statistic \\(T_{N} = \\sum_{i=1}^{N} c_{iN} a_{N}(i)\\) will approximately correspond to large values of \\(P_{\\theta_{A}}\\Big( R_{1}(\\mathbf{Z}), \\ldots, R_{N}(\\mathbf{Z}) \\Big)\\) if we choose the scores to be \\[\\begin{equation} a_{N}(i) = E\\Bigg\\{ \\frac{\\partial \\log f(Z_{(i)})}{ \\partial Z} \\Bigg\\} \\nonumber \\end{equation}\\] Linear rank statistics with scores generated this way are usually called locally most powerful rank tests. The best choice of the scores will depend on what we assume about the density \\(f\\). For example, if we assume that \\(f(z)\\) is \\(\\textrm{Normal}(0,1)\\), then \\[\\begin{equation} \\frac{\\partial \\log f(z)}{\\partial z} = -z \\nonumber \\end{equation}\\] The approximate expectation of the order statistics from a Normal\\((0,1)\\) distribution are \\[\\begin{equation} E\\{ Z_{(i)} \\} \\approx \\Phi^{-1}\\Bigg( \\frac{i}{N+1} \\Bigg) \\nonumber \\end{equation}\\] This implies that the van der Waerden’s scores are approximately optimal if we assume the distribution of the \\(Z_{i}\\) is Normal. This can also be worked out for other choices of \\(f(z)\\). If \\(f(z)\\) is a Logistic distribution, the optimal scores correspond to the Wilcoxon rank sum test statistic. If \\(f(z)\\) is Laplace (meaning that \\(f(z) = \\frac{1}{2}e^{-|z|}\\)), then the optimal scores correspond to the median test. 3.6 Additional Reading Additional reading which covers the material discussed in this chapter includes: Chapters 3-4 from Hollander, Wolfe, and Chicken (2013) 3.7 Exercises Exercise 3.1: Suppose \\(X_{1}, X_{2}, X_{3}\\) are i.i.d. observations from a continuous distribution function \\(F_{X}\\). Compute the covariance matrix of the vector of ranks \\(\\big( R_{1}(\\mathbf{X}), R_{2}(\\mathbf{X}), R_{3}( \\mathbf{X} ) \\big)\\). Exercise 3.2: Again, suppose that \\(X_{1}, X_{2}, X_{3}, X_{4}\\) are i.i.d. observations from a continuous distribution function \\(F_{X}\\). Let \\(T= R_{1}( \\mathbf{X} ) + R_{2}(\\mathbf{X})\\). Compute \\(P( T = j )\\) for \\(j = 3, 4, 5, 6, 7\\). Exercise 3.3. Using the exact distribution of the WRS test statistic, what is the smallest possible one-sided p-value associated with the WRS test for a fixed value of \\(n\\) and \\(m\\) (assuming the probability of ties is zero)? Exercise 3.4. Suppose we have observations \\((-2, 1, -1/2, 3/2, 3)\\) from a single group. What is the value of the Wilcoxon signed rank statistic? Exercise 3.5. Under the assumptions of model (3.8), what is the density function of \\(|D_{i}|\\) and \\(-|D_{i}|\\)? Exercise 3.6. Under the assumptions of model (3.8) and assuming that \\(\\theta = 0\\), show that the expectation of the Wilcoxon signed-rank statistic is \\(0\\). Exercise 3.7: Derive the formula for \\(c_{\\alpha, n}\\) shown in (3.12). Exercise 3.8: Suppose that \\(X_{1}, \\ldots, X_{n} \\sim \\textrm{Exponential}(\\lambda)\\) and \\(Y_{1}, \\ldots, Y_{m} \\sim \\textrm{Gamma}(\\alpha, \\theta)\\) meaning that the p.d.f. of \\(Y_{i}\\) is \\[\\begin{equation} f_{Y_{i}}(y) = \\begin{cases} \\frac{\\theta^{\\alpha}}{ \\Gamma(\\alpha) } y^{\\alpha - 1}e^{-\\theta y} &amp; \\textrm{ if } y &gt; 0 \\nonumber \\\\ 0 &amp; \\textrm{ otherwise.} \\end{cases} \\end{equation}\\] The pooled-data vector is \\(\\mathbf{Z} = (Z_{1}, \\ldots, Z_{n + m}) = (X_{1}, \\ldots, X_{n}, Y_{1}, \\ldots, Y_{m})\\) so that \\(Z_{j} = X_{j}\\) for \\(1 \\leq j \\leq n\\) and \\(Z_{j} = Y_{j-n}\\) for \\(n + 1 \\leq j \\leq n + m\\). Compute both \\(E\\{ R_{1}(\\mathbf{Z}) \\}\\) and \\(E\\{ R_{n+1}( \\mathbf{Z} ) \\}\\). References "],
["krusk-wallis.html", "Chapter 4 Rank Tests for Multiple Groups 4.1 The Kruskal-Wallis Test 4.2 Performing the Kruskal-Wallis Test in R 4.3 Comparison of Specific Groups 4.4 An Additional Example 4.5 Additional Reading", " Chapter 4 Rank Tests for Multiple Groups We can roughly think of the tests discussed in Chapter 3 as being related to the well-known parametric tests shown in the table below. \\[\\begin{eqnarray} \\textbf{Parametric Test} &amp; &amp; \\qquad \\textbf{ Nonparametric Tests } \\nonumber \\\\ &amp; &amp; \\nonumber \\\\ \\textrm{One-Sample t-test} &amp; &amp; \\qquad \\textrm{Wilcoxon Signed Rank/Sign Test} \\nonumber \\\\ \\textrm{Two-Sample t-test} &amp; &amp; \\qquad \\textrm{Wilcoxon Rank Sum/Normal Scores/Median Test} \\nonumber \\end{eqnarray}\\] The Kruskal-Wallis test can be though of as the nonparametric analogue of one-way analysis of variance (ANOVA). For \\(K \\geq 3\\) groups, one-way ANOVA considers the analysis of observations arising from the following model \\[\\begin{equation} Y_{kj} = \\mu_{k} + \\varepsilon_{kj}, \\qquad j=1,\\ldots, n_{k}; k=1,\\ldots,K \\tag{4.1} \\end{equation}\\] where it is often assumed that \\(\\varepsilon_{kj} \\sim \\textrm{Normal}(0, \\sigma^{2})\\). Usually, the one-way ANOVA hypothesis of interest is something like \\[\\begin{equation} H_{0}: \\mu_{1} = \\mu_{2} = \\ldots = \\mu_{K} \\tag{4.2} \\end{equation}\\] which is often referred to as the homogeneity hypothesis. A test of the hypothesis (4.2) is based on decomposing the observed variation in the responses \\(Y_{kj}\\): \\[\\begin{eqnarray} \\underbrace{ \\sum_{k=1}^{K}\\sum_{j=1}^{n_{k}} (Y_{kj} - \\bar{Y}_{..})^{2}}_{SST} &amp;=&amp; \\sum_{k=1}^{K}\\sum_{j=1}^{n_{k}} (\\bar{Y}_{k.} - \\bar{Y}_{..})^{2} + \\sum_{k=1}^{K}\\sum_{j=1}^{n_{k}} (Y_{kj} - \\bar{Y}_{k.})^{2} \\nonumber \\\\ &amp;=&amp; \\underbrace{\\sum_{k=1}^{K} n_{k} (\\bar{Y}_{k.} - \\bar{Y}_{..})^{2}}_{SSA} + \\underbrace{\\sum_{k=1}^{K}\\sum_{j=1}^{n_{k}} (Y_{kj} - \\bar{Y}_{k.})^{2}}_{SSE} \\tag{4.3} \\end{eqnarray}\\] where \\(\\bar{Y}_{k.} = \\frac{1}{n_{k}}\\sum_{j=1}^{n_{k}} Y_{kj}\\) and \\(\\bar{Y}_{..} = \\frac{1}{K}\\sum_{k=1}^{K} \\bar{Y}_{k.}\\). Large values of \\(SSA = \\sum_{k=1}^{K} n_{k} (\\bar{Y}_{k.} - \\bar{Y}_{..})^{2}\\) provide evidence against the null hypothesis (4.2). The alternative hypothesis here is that there is at least one pair of means \\(\\mu_{h}, \\mu_{l}\\) such that \\(\\mu_{h} \\neq \\mu_{l}\\). 4.1 The Kruskal-Wallis Test 4.1.1 Definition Instead of assuming (4.1) for the responses \\(Y_{kj}\\), a nonparametric way of thinking about this problem is to instead only assume that \\[\\begin{equation} Y_{kj} \\sim F_{k} \\end{equation}\\] That is, \\(Y_{k1}, Y_{k2}, \\ldots, Y_{kn_{k}}\\) is an i.i.d. sample from \\(F_{k}\\) for each \\(k\\). A nonparametric version of the one-way ANOVA homogeneity hypothesis is \\[\\begin{equation} H_{0}: F_{1} = F_{2} = \\ldots = F_{K} \\tag{4.4} \\end{equation}\\] The “shift alternative” in this case can be stated as \\[\\begin{equation} H_{A}: F_{k}(t) = F(t - \\Delta_{k}), \\quad \\textrm{ for } k = 1, \\ldots K \\quad \\textrm{ and not all $\\Delta_{k}$ equal} \\end{equation}\\] The Kruskal-Wallis test statistic is similar to the SSA term (defined in (4.3)) in the one-way ANOVA setting. Rather than comparing the group-specific means \\(\\bar{Y}_{k.}\\) with the overall mean \\(\\bar{Y}_{..}\\), the Kruskal-Wallis test statistic compares the group-specific rank means \\(\\bar{R}_{k.}\\) with their overall expectation under the null hypothesis. The Kruskal-Wallis test statistic \\(KW_{N}\\) is defined as \\[\\begin{equation} KW_{N} = \\frac{12}{N(N-1)}\\sum_{k=1}^{K} n_{k}\\Big( \\bar{R}_{k.} - \\frac{N + 1}{2} \\Big)^{2}, \\quad \\textrm{ where } N = \\sum_{k=1}^{K} n_{k} \\tag{4.5} \\end{equation}\\] In (4.5), \\(\\bar{R}_{k.}\\) is the average rank of those in the \\(k^{th}\\) group \\[\\begin{equation} \\bar{R}_{k.} = \\frac{1}{n_{k}} \\sum_{j=1}^{n_{k}} R_{kj}(\\mathbf{Z}), \\end{equation}\\] where \\(\\mathbf{Z}\\) denotes the pooled-data vector and \\(R_{kj}(\\mathbf{Z})\\) denotes the rank of \\(Y_{kj}\\) in the “pooled-data ranking”. What is the expectation of \\(\\bar{R}_{k.}\\) under the null hypothesis (4.4)? Again, if the null hypothesis is true, we can treat all of our responses \\(Y_{kj}\\) as just an i.i.d. sample of size \\(N\\) from a common distribution function \\(F\\). Hence, as we showed in (3.2) from Chapter 3, \\(E\\{ R_{kj}(\\mathbf{Z}) \\} = (N+1)/2\\) under the assumption that the data are an i.i.d. sample from a common distribution function. So, the intuition behind the definition of \\(KW_{N}\\) is that the differences \\(\\bar{R}_{k.} - \\frac{N + 1}{2}\\) should be small whenever the homogeneity hypothesis (4.4) is true. When \\(K=2\\), the following relationship between the Kruskal-Wallis statistic \\(KW_{N}\\) and the Wilcoxon rank sum test statistic \\(W\\) from Chapter 3 holds. \\[\\begin{equation} KW_{N} = \\frac{12}{mn(N+1)}\\Big( W - \\frac{n(N+1)}{2} \\Big)^{2}. \\tag{4.6} \\end{equation}\\] Hence, the p-value from a Kruskal-Wallis test and a (two-sided) WRS test should be the same when \\(K = 2\\). However, you cannot directly perform a one-sided test using the Kruskal-Wallis test. Exercise 4.1 If \\(K=2\\), show that equation (4.6) holds. An Example Group Y Rank Group 1 1.00 8 Group 1 -1.20 2 Group 1 -1.50 1 Group 2 0.00 5 Group 2 -0.10 4 Group 2 1.10 9 Group 3 0.90 7 Group 3 -0.40 3 Group 3 0.60 6 Consider the data shown in the table. In this case, \\(N = 9\\), \\(\\bar{R}_{1.} = 11/3\\), \\(\\bar{R}_{2.} = 6\\), and \\(\\bar{R}_{3.} = 16/3\\). The Kruskall-Wallis statistic is \\[\\begin{equation} KW_{N} = \\frac{1}{2}\\Big\\{ 3(11/3 - 5)^{2} + 3(6 - 5)^{2} + 3(16/3 - 5)^{2} \\Big\\} = 13/9 \\nonumber \\end{equation}\\] 4.1.2 Asymptotic Distribution and Connection to One-Way ANOVA The Kruskal-Wallis statistic \\(KW_{N}\\) has an asymptotic chi-square distribution with \\(K-1\\) degrees of freedom under the null hypothesis (4.4). This follows from the fact that \\((\\bar{R}_{k.} - (N+1)/2)\\) is approximately normally distributed for large \\(n_{k}\\). R uses the large-sample approximation when computing the p-value for the Kruskal-Wallis test. The Kruskal-Wallis test can also be thought of as the test you would obtain if you applied the one-way ANOVA setup to the ranks of \\(Y_{kj}\\). The one-way ANOVA test is based on the value of SSA where, as in (4.3), SSA is defined as \\[\\begin{equation} SSA = \\sum_{k=1}^{K} n_{k} (\\bar{Y}_{k.} - \\bar{Y}_{..})^{2} \\nonumber \\end{equation}\\] You then reject \\(H_{0}\\), when \\(SSA/SSE = SSA/(SST - SSA)\\) is sufficiently large. Notice that if we computed SSA using the ranks \\(R_{kj}( \\mathbf{Z} )\\) rather than the observations \\(Y_{kj}\\), we would get: \\[\\begin{eqnarray} SSA_{r} &amp;=&amp; \\sum_{k=1}^{K} n_{k} (\\bar{R}_{k.} - \\bar{R}_{..})^{2} \\nonumber \\\\ &amp;=&amp; \\sum_{k=1}^{K} n_{k} \\Big( \\bar{R}_{k.} - \\frac{N+1}{2} \\Big)^{2} \\nonumber \\\\ &amp;=&amp; \\frac{N(N-1)}{12} KW_{N} \\tag{4.7} \\end{eqnarray}\\] If you were applying ANOVA to the ranks of \\(Y_{kj}\\), \\(SST_{r}\\) would be the following fixed constant: \\[\\begin{equation} \\textrm{SST}_{r} = \\frac{N(N + 1)(N-1)}{12} \\nonumber \\end{equation}\\] So, any test of the homogeneity hypothesis would be based on just the value of \\(SSA_{r}\\) which, as we showed in (4.7), is just a constant times the Kruskal-Wallis statistic. 4.2 Performing the Kruskal-Wallis Test in R We will look at performing Kruskal-Wallis tests in R by using the “InsectSprays” dataset. head(InsectSprays) ## count spray ## 1 10 A ## 2 7 A ## 3 20 A ## 4 14 A ## 5 14 A ## 6 12 A This dataset has 72 observations. The variable count is the number of insects measured in some agricultural unit. The variable spray was the type of spray used on that unit. You could certainly argue that a standard ANOVA is not appropriate in this situation because the responses are counts, and for count data, the variance is usually a function of the mean. A generalized linear model with a log link function might be more appropriate. Applying a square-root transformation to count data is also a commonly suggested approach. (The square-root transformation is the “variance-stabilizing transformation” for Poisson-distributed data). boxplot(sqrt(count) ~ spray, data=InsectSprays,las=1, ylab=&quot;square root of insect counts&quot;) Let us perform a test of homogeneity using both the one-way ANOVA approach and a Kruskal-Wallis test anova(lm(sqrt(count) ~ spray, data=InsectSprays)) ## Analysis of Variance Table ## ## Response: sqrt(count) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## spray 5 88.438 17.6876 44.799 &lt; 2.2e-16 *** ## Residuals 66 26.058 0.3948 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 a &lt;- kruskal.test(sqrt(count) ~ spray, data=InsectSprays) a$p.value ## [1] 1.510844e-10 Notice that applying the square root transformation does not affect the value of the Kruskal-Wallis statistic or the Kruskal-Wallis p-value. kruskal.test(count ~ spray, data=InsectSprays) ## ## Kruskal-Wallis rank sum test ## ## data: count by spray ## Kruskal-Wallis chi-squared = 54.691, df = 5, p-value = 1.511e-10 This invariance to data transformation is not true for the standard one-way ANOVA. anova(lm(count ~ spray, data=InsectSprays)) ## Analysis of Variance Table ## ## Response: count ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## spray 5 2668.8 533.77 34.702 &lt; 2.2e-16 *** ## Residuals 66 1015.2 15.38 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 4.3 Comparison of Specific Groups A Kruskal-Wallis test performs a test of the overall homogeneity hypothesis \\[\\begin{equation} H_{0}: F_{1} = F_{2} = \\ldots = F_{K} \\end{equation}\\] However, a rejection of the homogeneity hypothesis does not indicate which group differences are primarily the source of this rejection nor does it provide any measure of the “magnitude” of the differences between each of the groups. Dunn’s test is the suggested way to compute pairwise tests of stochatic dominance. Performing a series of pairwise Wilcoxon rank sum test can lead to violations of transitivity. For example, group A is “better” than B which is better than C, but group C is better than A. In R, Dunn’s test can be performed using the dunn.test package. In traditional one-way ANOVA one often reports pairwise differences in the means and their associated confidence intervals. In the context of a Kruskal-Wallis test, one could report pairwise differences in the Hodges-Lehmann estimate though other comparisons may also be of interest. One nice approach is to use the proportional odds model interpretation of the Kruskal-Wallis test and then report the difference in the estimated proportional odds coefficients. See Section 7.6 of http://hbiostat.org/doc/bbr.pdf for more details on the proportional odds model. 4.4 An Additional Example We will use the “cane” dataset from the boot package. library(boot) data(cane) head(cane) ## n r x var block ## 1 87 76 19 1 A ## 2 119 8 14 2 A ## 3 94 74 9 3 A ## 4 95 11 12 4 A ## 5 134 0 12 5 A ## 6 92 0 3 6 A These data come from a study trying to determine the susceptibility of different types of sugar cane to a particular type of disease. The variable n contains the total number of shoots in each plot. The variable r containes the total number of diseased shoots. We can create a new variable prop that measures the proportion of shoots that are diseased. cane$prop &lt;- cane$r/cane$n You could certainly argue that a logistic regression model is a better approach here, but we will analyze the transformed proportions using the arcsine square root transformation. cane$prop.trans &lt;- asin(sqrt(cane$prop)) boxplot(prop.trans ~ block, data=cane, las=1, ylab=&quot;number of shoots&quot;) kruskal.test(prop ~ block, data=cane) ## ## Kruskal-Wallis rank sum test ## ## data: prop by block ## Kruskal-Wallis chi-squared = 1.1355, df = 3, p-value = 0.7685 4.5 Additional Reading Additional reading which covers the material discussed in this chapter includes: Chapters 6 from Hollander, Wolfe, and Chicken (2013) References "],
["permutation.html", "Chapter 5 Permutation Tests 5.1 Notation 5.2 Permutation Tests for the Two-Sample Problem 5.3 The Permutation Test as a Conditional Test 5.4 A Permutation Test for Correlation 5.5 A Permutation Test for Variable Importance in Regression and Machine Learning", " Chapter 5 Permutation Tests Permutation tests are a useful tool that allows you to avoid depending on specific parametric assumptions. Permutation tests are also useful in more complex modern applications where it can be difficult to work out the theoretical null distribution of the desired test statistic. 5.1 Notation A permutation \\(\\pi\\) of a set \\(S\\) is a function \\(\\pi: S \\longrightarrow S\\) is a function that is both one-to-one and onto. We will usually think of \\(S\\) as the set of observation indices in which case \\(S = \\{1, \\ldots, N\\}\\) for sample size \\(N\\). Each permutation \\(\\pi\\) of \\(S = \\{1, \\ldots, N\\}\\) defines a particular ordering of the elements of \\(S\\). For this reason, a permutation is often expressed as the following ordered list \\[\\begin{equation} \\pi = \\big( \\pi(1), \\pi(2), \\ldots, \\pi(N) \\big) \\nonumber \\end{equation}\\] In other words, we can think of a permutation of \\(S\\) as a particular ordering of the elements of \\(S\\). For example, if \\(S = \\{1,2,3\\}\\), and \\(\\pi_{1}\\) is a permutation of \\(S\\) defined as \\(\\pi_{1}(1) = 3\\), \\(\\pi_{1}(2) = 1\\), \\(\\pi_{1}(3) = 2\\), then this permutation expressed as an ordered list would be \\[\\begin{equation} \\pi_{1} = (3, 1, 2) \\nonumber \\end{equation}\\] There are \\(5\\) other possible permutations of \\(S\\): \\[\\begin{eqnarray} \\pi_{2} &amp;=&amp; (1,2,3) \\nonumber \\\\ \\pi_{3} &amp;=&amp; (2,1,3) \\nonumber \\\\ \\pi_{4} &amp;=&amp; (1,3,2) \\nonumber \\\\ \\pi_{5} &amp;=&amp; (3,2,1) \\nonumber \\\\ \\pi_{6} &amp;=&amp; (2, 3, 1) \\nonumber \\end{eqnarray}\\] If \\(S\\) has \\(N\\) distinct elements, there are \\(N!\\) possible permutations of \\(S\\). We will let \\(\\mathcal{S}_{N}\\) denote the set of all permutations of the set \\(\\{1, \\ldots, N\\}\\). 5.2 Permutation Tests for the Two-Sample Problem Permutation tests for two-sample problems are motivated by the following reasoning: If there is no real difference between the two groups, there is nothing “special” about the difference in means between the two groups. The observed difference in the mean between the two groups should not be notably different than mean differences from randomly formed groups. Forming “random” groups can be done by using many permutations of the original data. 5.2.1 Example 1 Suppose we have observations from two groups \\(X_{1}, \\ldots, X_{n} \\sim F_{X}\\) and \\(Y_{1}, \\ldots, Y_{m} \\sim F_{Y}\\). Let \\(\\mathbf{Z} = (Z_{1}, \\ldots, Z_{N})\\) denote the pooled data \\[\\begin{equation} (Z_{1}, \\ldots, Z_{N}) = (X_{1}, \\ldots, X_{n}, Y_{1}, \\ldots, Y_{m}) \\end{equation}\\] For a permutation \\(\\pi\\) of \\(\\{1, \\ldots, N\\}\\), we will let \\(\\mathbf{Z}_{\\pi}\\) denote the corresponding permuted dataset \\[\\begin{equation} \\mathbf{Z}_{\\pi} = (Z_{\\pi(1)}, Z_{\\pi(2)}, \\ldots, Z_{\\pi(N)}) \\end{equation}\\] Example of Permuting a Vector of Responses. This example assumes n=m=5. OriginalData Perm1 Perm2 Perm3 Perm4 Perm5 z1 0.60 -0.60 0.60 -0.90 0.70 0.60 z2 -0.80 -1.40 -0.60 0.70 -0.40 -0.60 z3 -0.60 0.70 0.20 0.60 -1.40 -0.80 z4 -0.90 0.20 -0.40 0.20 0.20 0.30 z5 0.30 -0.40 -1.30 -0.40 -0.90 -0.40 z6 -1.30 -1.30 -1.40 -0.60 -0.80 0.70 z7 0.20 0.30 0.70 -1.40 0.30 -0.90 z8 0.70 0.60 0.30 -1.30 0.60 0.20 z9 -1.40 -0.90 -0.80 0.30 -0.60 -1.40 z10 -0.40 -0.80 -0.90 -0.80 -1.30 -1.30 mean difference 0.16 0.12 0.12 0.80 0.00 0.36 For example, the columns in Table 5.1 are just permutations of the original data \\(\\mathbf{Z}\\). Suppose we want to base a test on the difference in the means between the two groups \\[\\begin{equation} T_{N}(\\mathbf{Z}) = \\bar{X} - \\bar{Y} = \\frac{1}{n}\\sum_{i=1}^{n} Z_{i} - \\frac{1}{m}\\sum_{i=n+1}^{N} Z_{i} \\end{equation}\\] We will let \\(t_{obs}\\) denote the observed value of the mean difference. That is, \\(t_{obs} = T_{N}(\\mathbf{Z}_{obs})\\), where \\(\\mathbf{Z}_{obs}\\) is the vector of the observed data. Under the null hypothesis that \\(F_{X} = F_{Y}\\), the observed mean difference should not be “abnormal” when compared with the mean differences from many other permutations of the data. z &lt;- c(0.6, -0.8, -0.6, -0.9, 0.3, -1.3, 0.2, 0.7, -1.4, -0.4) ## data observed.diff &lt;- mean(z[1:5]) - mean(z[6:10]) ## observed mean difference nperms &lt;- 1000 mean.diff &lt;- rep(NA, nperms) for(k in 1:nperms) { ss &lt;- sample(1:10, size=10) ## draw a random permutation z.perm &lt;- z[ss] ## form the permuted dataset mean.diff[k] &lt;- mean(z.perm[1:5]) - mean(z.perm[6:10]) ## compute mean difference ## for permuted dataset } hist(mean.diff, las=1, col=&quot;grey&quot;, main=&quot;Permutation Distribution of Mean Difference&quot;) abline(v=observed.diff, lwd=3) 5.2.2 Permutation Test p-values The one-sided p-value for the permutation test is \\[\\begin{eqnarray} \\textrm{p-value} &amp;=&amp; \\frac{\\textrm{number of permutations such that } T_{N} \\geq t_{obs}}{ N! } \\nonumber \\\\ &amp;=&amp; \\frac{1}{N!} \\sum_{\\pi \\in \\mathcal{S}_{N}} I\\Big( T_{N}(\\mathbf{Z}_{\\pi}) \\geq t_{obs} \\Big) \\nonumber \\end{eqnarray}\\] The two-sided p-value for the two-sample problem would be \\[\\begin{equation} \\textrm{p-value} = \\frac{1}{N!} \\sum_{\\pi \\in \\mathcal{S}_{N}} I\\Big( \\Big| T_{N}(\\mathbf{Z}_{\\pi}) \\Big| \\geq |t_{obs}| \\Big) \\nonumber \\end{equation}\\] As we did when producing the above histogram, the permutation-test p-value is often computed by using a large number of random permutations rather than computing the test statistic for every possible permutation. The Monte Carlo permutation-test p-value is defined as \\[\\begin{equation} \\textrm{p-value}_{mc} = \\frac{1}{S+1}\\Bigg[ 1 + \\sum_{s = 1}^{S} I\\Big( T_{N}(\\mathbf{Z}_{\\pi_{s}}) \\geq t_{obs} \\Big) \\Bigg] \\end{equation}\\] where \\(\\pi_{1}, \\ldots, \\pi_{S}\\) are randomly drawn permutations The two-sided (Monte Carlo) p-value for the example shown in Table 5.1 is pval.mc &lt;- (1 + sum(abs(mean.diff) &gt;= abs(observed.diff)))/(nperms + 1) round(pval.mc, 2) ## [1] 0.75 5.2.3 Example 2: Ratios of Means With permutation tests, you are not limited to difference in means. You can choose the statistic \\(T_{N}(\\mathbf{Z})\\) to measure other contrasts of interest. For example, with nonnegative data you might be interested in the ratio of means between the two groups \\[\\begin{equation} T_{N}( \\mathbf{Z} ) = \\max\\Big\\{ \\bar{X}/\\bar{Y} , \\bar{Y}/\\bar{X} \\Big\\} \\end{equation}\\] Let us see how this works for a simulated example with \\(n = m = 20\\) where we assume that \\(X_{1}, \\ldots, X_{n} \\sim \\textrm{Exponential}(1)\\) and \\(Y_{1}, \\ldots, Y_{m} \\sim \\textrm{Exponential}(1/2)\\). set.seed(5127) xx &lt;- rexp(20, rate=1) yy &lt;- rexp(20, rate=0.5) zz &lt;- c(xx, yy) ## this is the original data t.obs &lt;- max(mean(zz[1:20])/mean(zz[21:40]), mean(zz[21:40])/mean(zz[1:20])) nperms &lt;- 500 mean.ratio &lt;- rep(0, nperms) for(k in 1:nperms) { ss &lt;- sample(1:40, size=40) zz.perm &lt;- zz[ss] mean.ratio[k] &lt;- max(mean(zz.perm[1:20])/mean(zz.perm[21:40]), mean(zz.perm[21:40])/mean(zz.perm[1:20])) } hist(mean.ratio, las=1, col=&quot;grey&quot;, main=&quot;Permutation Distribution of Maximum Mean Ratio&quot;, xlab=&quot;maximum mean ratio&quot;) abline(v=t.obs, lwd=3) The two-side (Monte Carlo) permutation test p-value is: pval.mc &lt;- (1 + sum(mean.ratio &gt;= t.obs))/(nperms + 1) round(pval.mc, 2) ## [1] 0.04 5.2.4 Example 3: Differences in Quantiles Permutation tests are especially useful in problems where working out the null distribution is difficult, or when certain approximations of the null distributions are hard to justify. An example of this occurrs if you want to compare medians, or more generally, compare quantiles between two groups. The difference-in-quantiles statistic would be defined as \\[\\begin{equation} T_{N}( \\mathbf{Z} ) = Q_{p}(Z_{1}, \\ldots, Z_{n}) - Q_{p}(Z_{n+1}, \\ldots, Z_{N}) \\nonumber \\end{equation}\\] where \\(Q_{p}(X_{1}, \\ldots, X_{H})\\) denotes the \\(p^{th}\\) quantile from the data \\(X_{1}, \\ldots, X_{H}\\). The difference in quantiles could be computed with the following R code: z &lt;- rnorm(10) quantile(z[1:5], probs=.3) - quantile(z[6:10], probs=.3) ## 30% ## 0.2671133 Note that setting probs=.5 in the quantile function will return the median. Exercise 5.1 Suppose we have the following data from two groups \\((X_{1}, X_{2}, X_{3}) = (-1, 0, 1)\\) and \\((Y_{1}, Y_{2}, Y_{3}) = (4, -2, 2)\\). Compute the (two-sided) permutation p-value for the following two statistics: \\(T_{N}( \\mathbf{Z} ) = \\textrm{median}(X_{1}, X_{2}, X_{3}) - \\textrm{median}(Y_{1}, Y_{2}, Y_{3})\\). \\(T_{N}( \\mathbf{Z} ) = \\bar{X} - \\bar{Y}\\). Exercise 5.2. Suppose we have data from two groups such that \\(X_{1}, \\ldots, X_{n} \\sim \\textrm{Normal}(0, 1)\\) and \\(Y_{1}, \\ldots, Y_{m} \\sim \\textrm{Normal}(1, 1)\\). Using \\(n=m=50\\) and 500 simulation replications, compute \\(500\\) significance thresholds from the one-sided permutation test which uses the statistic \\(T_{N}( \\mathbf{Z} ) = \\bar{X} - \\bar{Y}\\). How, does this compare with the t-statistic threshold of approximately \\(1.64*\\sqrt{2/50}\\)? Exercise 5.3. Suppose we have data from two groups such that \\(X_{1}, \\ldots, X_{n} \\sim \\textrm{Normal}(0, 1)\\) and \\(Y_{1}, \\ldots, Y_{m} \\sim \\textrm{Normal}(1, 1)\\). Using \\(n=m=50\\) and 500 simulation replications, compute the power of the permutation test which uses the statistic \\(T_{N}( \\mathbf{Z} ) = \\bar{X} - \\bar{Y}\\) to detect this true alternative. How, does the power compare with the (two-sided) two-sample t-statistic and the (two-sided) Wilcoxon rank sum test? 5.3 The Permutation Test as a Conditional Test A permutation test is an example of a conditional test. Typically, the p-value is defined as \\[\\begin{equation} \\textrm{p-value} = P(T \\geq t_{obs}|H_{0}) \\end{equation}\\] for some test statistic \\(T\\). In other words, the p-value is the probability that a random variable which follows the null distribution exceeds \\(t_{obs}\\). In many problems however, the null hypothesis \\(H_{0}\\) is not determined by a single parameter but contains many parameters. For example, the null hypothesis in a t-test is really \\(H_{0}: \\mu_{x} = \\mu_{y}\\) and \\(\\sigma &gt; 0\\). That is, the null hypothesis is true for many different values of \\(\\sigma\\). When \\(H_{0}\\) contains many parameter values, one approach for computing a p-value is to choose the test statistic \\(T\\) so that its distribution is the same for every point in \\(H_{0}\\). A more general approach is to instead compute a conditional p-value A conditional p-value is defined as \\[\\begin{equation} \\textrm{p-value} = P(T \\geq t_{obs}| S=s, H_{0}) \\nonumber \\end{equation}\\] where \\(S\\) is a sufficient statistic for the unknown terms in \\(H_{0}\\). A classic example of this is Fisher’s exact test. A permutation test computes a conditional p-value where the sufficient statistic is the vector of order statistics \\((Z_{(1)}, Z_{(2)}, \\ldots,Z_{(N)})\\). Recall that the order statistics are defined as \\[\\begin{eqnarray} Z_{(1)} &amp;=&amp; \\textrm{ smallest observation } \\nonumber \\\\ Z_{(2)} &amp;=&amp; \\textrm{ second smallest observation} \\nonumber \\\\ &amp; \\vdots &amp; \\nonumber \\\\ Z_{(N)} &amp;=&amp; \\textrm{ largest observation} \\nonumber \\end{eqnarray}\\] What is the conditional distribution of the observed data conditional on the observed order statistics? It is: \\[\\begin{eqnarray} f_{Z_{1}, \\ldots, Z_{N}|Z_{(1)}, \\ldots, Z_{(N)}}( z_{\\pi(1)}, \\ldots, z_{\\pi(N)} | z_{1}, \\ldots, z_{N}) &amp;=&amp; \\frac{f_{Z_{1}, \\ldots, Z_{N}}( z_{\\pi(1)}, \\ldots, z_{\\pi(N)} ) }{ f_{Z_{(1)},\\ldots,Z_{(N)}}(z_{1}, \\ldots, z_{N}) } \\nonumber \\\\ &amp;=&amp; \\frac{f_{Z_{i}}(z_{\\pi(1)}) \\cdots f_{Z_{i}}(z_{\\pi(N)})}{ N!f_{Z_{i}}(z_{1}) \\cdots f_{Z_{i}}(z_{N}) } \\nonumber \\\\ &amp;=&amp; \\frac{1}{N!} \\nonumber \\end{eqnarray}\\] (See Chapter 5 of Casella and Berger (2002) for a detailed description of the distribution of order statistics) In other words, if we know the value of: \\(Z_{(1)}=z_{1}, \\ldots, Z_{(N)} = z_{N}\\), then any event of the form \\(\\{ Z_{1} = z_{\\pi(1)}, \\ldots, Z_{N} = z_{\\pi(N)} \\}\\) has an equal probability of occurring for any permutation chosen. This equal probability of \\(1/N!\\) is only true under \\(H_{0}\\) where we can regard the data as coming from a common distribution. If we are conditioning on \\(Z_{(1)}=z_{1}, \\ldots, Z_{(N)} = z_{N}\\), then the probability that \\(T_{N}(Z_{1}, \\ldots, Z_{N}) \\geq t\\) is just the number of permutations of \\((z_{1}, \\ldots, z_{N})\\) where the test statistic is greater than \\(t\\) divided by \\(N!\\). In other words \\[\\begin{eqnarray} &amp; &amp; P\\Big\\{ T_{N}(Z_{1}, \\ldots, Z_{N}) \\geq t| Z_{(1)} = z_{1}, \\ldots, Z_{(N)} = z_{N} \\Big\\} \\\\ &amp;=&amp; \\frac{1}{N!} \\sum_{\\pi \\in \\mathcal{S}_{N}} I\\Big( T_{N}(z_{\\pi(1)}, \\ldots, z_{\\pi(N)}) \\geq t \\Big) \\end{eqnarray}\\] Let us now consider a concrete example. Suppose we have a two-sample problem with four observations. The first two observations come from the first group while the last two observations come from the second group. The order statistics that we will condition on are: \\[\\begin{eqnarray} Z_{(1)} &amp;=&amp; z_{1} = -3 \\\\ Z_{(2)} &amp;=&amp; z_{2} = -1 \\\\ Z_{(3)} &amp;=&amp; z_{3} = 2 \\\\ Z_{(4)} &amp;=&amp; z_{4} = 5 \\end{eqnarray}\\] If \\(T_{4}\\) is the mean difference \\[\\begin{equation} T_{4}(Z_{1}, Z_{2},Z_{3},Z_{4}) = \\frac{Z_{1} + Z_{2} - Z_{3} - Z_{4}}{2} \\end{equation}\\] what is the probability \\[\\begin{equation} P\\Big\\{ T_{4}(Z_{1}, Z_{2}, Z_{3}, Z_{4}) \\geq 2.5 | Z_{(1)}=z_{1}, Z_{(2)}=z_{2}, Z_{(3)}=z_{3}, Z_{(4)} = z_{4} \\Big \\} \\end{equation}\\] From the below table, we see that the number of times \\(T_{4} \\geq 2.5\\) occurs is \\(8\\). Hence, \\[\\begin{eqnarray} &amp; &amp; P\\Big\\{ T_{4}(Z_{1}, Z_{2}, Z_{3}, Z_{4}) \\geq 2.5 | Z_{(1)}=z_{1}, Z_{(2)}=z_{2}, Z_{(3)}=z_{3}, Z_{(4)} = z_{4} \\Big \\} \\nonumber \\\\ &amp;=&amp; 8/24 = 1/3. \\nonumber \\end{eqnarray}\\] a1 a2 a3 a4 P(Z1 = a1, Z2=a2, Z3=a3, Z4=a4|order stat) T(a1, a2, a3, a4) T(a1, a2, a3, a4) &gt;= 2.5 -3 -1 2 5 1/24 -5.50 0 -3 -1 5 2 1/24 -5.50 0 -3 2 -1 5 1/24 -2.50 0 -3 2 5 -1 1/24 -2.50 0 -3 5 -1 2 1/24 0.50 0 -3 5 2 -1 1/24 0.50 0 -1 -3 2 5 1/24 -5.50 0 -1 -3 5 2 1/24 -5.50 0 -1 2 -3 5 1/24 -0.50 0 -1 2 5 -3 1/24 -0.50 0 -1 5 -3 2 1/24 2.50 1 -1 5 2 -3 1/24 2.50 1 2 -3 -1 5 1/24 -2.50 0 2 -3 5 -1 1/24 -2.50 0 2 -1 -3 5 1/24 -0.50 0 2 -1 5 -3 1/24 -0.50 0 2 5 -3 -1 1/24 5.50 1 2 5 -1 -3 1/24 5.50 1 5 -3 -1 2 1/24 0.50 0 5 -3 2 -1 1/24 0.50 0 5 -1 -3 2 1/24 2.50 1 5 -1 2 -3 1/24 2.50 1 5 2 -3 -1 1/24 5.50 1 5 2 -1 -3 1/24 5.50 1 5.4 A Permutation Test for Correlation Suppose we have \\(N\\) pairs of observations \\((U_{1}, V_{1}), \\ldots, (U_{N}, V_{N})\\) The correlation between these pairs is defined as \\[\\begin{equation} \\rho_{UV} = \\frac{\\textrm{Cov}(U_{i}, V_{i})}{\\sigma_{U}\\sigma_{V}} \\end{equation}\\] The test statistic of interest here is the sample correlation \\[\\begin{equation} T_{N}(\\mathbf{U}, \\mathbf{V}) = \\frac{\\sum_{i=1}^{N}(U_{i} - \\bar{U})(V_{i} - \\bar{V})}{\\sqrt{ \\sum_{i=1}^{N}(U_{i} - \\bar{U})^{2}\\sum_{i=1}^{N}(V_{i} - \\bar{V})^{2}} } \\end{equation}\\] To find the the permutation distribution, we only need to look at \\(T_{N}(\\mathbf{U}_{\\pi}, \\mathbf{V})\\) for different permutations \\(\\pi\\). In other words, we are computing correlation among pairs of the form \\((U_{\\pi(1)}, V_{1}), \\ldots, (U_{\\pi(N)}, V_{N})\\). We only need to look at \\(\\mathbf{U}_{\\pi}\\) because this achieves the objective of randomly “switching observation pairs”. The two-sided p-value for the permutation test of \\(H_{0}: \\rho_{UV} = 0\\) vs. \\(H_{A}: \\rho_{UV} \\neq 0\\) is \\[\\begin{equation} \\textrm{p-value} = \\frac{1}{N!} \\sum_{\\pi \\in \\mathcal{S}_{N}} I\\Big( \\Big| T_{N}(\\mathbf{U}_{\\pi}, \\mathbf{V}) \\Big| \\geq |t_{obs}| \\Big) \\nonumber \\end{equation}\\] library(rattle.data) ## Computing the permutation distribution for correlation ## between flavanoids and phenols n.obs &lt;- nrow(wine) ## number of observations t.obs.pf &lt;- cor(wine$Phenols, wine$Flavanoids) ## observed correlation nperms &lt;- 2000 cor.perm.pf &lt;- rep(0, nperms) for(k in 1:nperms) { ss &lt;- sample(1:n.obs, size=n.obs) uu.perm &lt;- wine$Phenols[ss] cor.perm.pf[k] &lt;- cor(uu.perm, wine$Flavanoids) } hist(cor.perm.pf, xlim=c(-1, 1), las=1, col=&quot;grey&quot;, main=&quot;Permutation Distribution of Correlation between Phenols and Flavanoids&quot;, xlab=&quot;correlation&quot;) abline(v=t.obs.pf, lwd=3) Now let us compute the p-values for both the Phenols/Flavanoids and Phenols/Color association tests. # p-value for correlation between Phenols/Flavanoids pval.mc &lt;- (1 + sum(abs(cor.perm.pf) &gt;= abs(t.obs.pf)))/(nperms + 1) round(pval.mc, 4) ## [1] 5e-04 # p-value for correlation between Phenols/Color pval.mc &lt;- (1 + sum(abs(cor.perm.pc) &gt;= abs(t.obs.pc)))/(nperms + 1) round(pval.mc, 4) ## [1] 0.4648 5.5 A Permutation Test for Variable Importance in Regression and Machine Learning The idea of permutation testing can also be applied in the context of regression. In regression, we have a series of responses \\(y_{1}, \\ldots, y_{N}\\), and we have a series of associated covariates vectors \\(\\mathbf{x}_{i}\\). For regression, we are going to perform permutations on the vector of responses \\(\\mathbf{y} = (y_{1}, \\ldots, y_{N})\\) and compute some measure for each permutation. For example, we might compute some measure of variable importance for each permutation. The idea here is that when permuting \\(\\mathbf{y}\\), the association between \\(\\mathbf{y}\\) and any “important covariates” should be lost. We want to see what the typical values of our variable importance measures will be when we break any association between \\(\\mathbf{y}\\) and a covariate. The approach of permuting the response vector can be useful in the context of difficult-to-interpret variable importance measures or variable importance measures which are known to have certain biases. This idea has been suggested as an alternative way of measuring variable importance for random forests (see e.g., Altmann et al. (2010) or Nembrini (2019)) With these approaches, we permute the response vector \\(\\mathbf{y}\\) many times. A permutation p-value for the importance of a particular variable will be the proportion of permutations where that variable’s importance score exceeded the importance score from the original data. (In this case, a smaller p-value would mean the variable was more important). Specifically, the permutation p-value for the importance of variable \\(h\\) would be given by \\[\\begin{equation} \\textrm{p-value}_{h} = \\frac{1}{N!}\\sum_{\\pi \\in \\mathcal{S}_{N}} I\\Big( s_{h}(\\mathbf{y}_{\\pi}, \\mathbf{X}) \\geq s_{h}(\\mathbf{y}, \\mathbf{X}) \\Big) \\tag{5.1} \\end{equation}\\] where \\(\\mathbf{y}\\) denotes the vector of responses and \\(\\mathbf{X}\\) denotes the design matrix. Here, \\(s_{h}(\\mathbf{y}, \\mathbf{X})\\) denotes the variable importance score for variable \\(h\\) when using reponse vector \\(\\mathbf{y}\\) and design matrix \\(\\mathbf{X}\\). Note that the formula (5.1) could be applied in the context of any method that generates a variable importance score from \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\). Let us see an example of that if we look at a random forest model for predicting wine type from the wine data. First, we will load the data and fit a random forest model. library(rattle.data) library(randomForest) ## randomForest 4.6-14 ## Type rfNews() to see new features/changes/bug fixes. wine2 &lt;- subset(wine, Type==1 | Type==2) wine2$Type &lt;- factor(wine2$Type) X &lt;- model.matrix(Type ~ . -1, data=wine2) yy &lt;- wine2$Type n &lt;- length(yy) nvars &lt;- ncol(X) ## Variable importance scores using original data originalRF &lt;- randomForest(X, y=yy) var.imp &lt;- originalRF$importance var.imp ## MeanDecreaseGini ## Alcohol 16.4477773 ## Malic 1.6547628 ## Ash 0.9512346 ## Alcalinity 1.8280983 ## Magnesium 4.2799222 ## Phenols 2.4436762 ## Flavanoids 6.2584880 ## Nonflavanoids 0.5184224 ## Proanthocyanins 0.6201685 ## Color 10.0968269 ## Hue 0.5552606 ## Dilution 1.0398252 ## Proline 17.3324599 Now, let us compare these original variable importance scores with the importance scores obtained across 10,000 permuted datasets. nperm &lt;- 10000 VarImpMat &lt;- matrix(0, nrow=nperm, ncol=ncol(X)) for(k in 1:nperm) { ytmp &lt;- yy[sample(1:n,size=n)] rf.fit &lt;- randomForest(X, y=ytmp) VarImpMat[k,] &lt;- rf.fit$importance ## VarImpMat[k,h] contains the importance score of ## variable h in permutation k } perm.pval &lt;- rep(0, nvars) for(h in 1:nvars) { perm.pval[h] &lt;- (1 + sum(VarImpMat[,h] &gt;= var.imp[h]))/(nperm + 1) } Permutation p-val Alcohol 0.000 Malic 1.000 Ash 1.000 Alcalinity 1.000 Magnesium 0.923 Phenols 1.000 Flavanoids 0.080 Nonflavanoids 1.000 Proanthocyanins 1.000 Color 0.000 Hue 1.000 Dilution 1.000 Proline 0.000 References "],
["ustat.html", "Chapter 6 U-Statistics 6.1 Definition 6.2 Examples 6.3 Inference using U-statistics 6.4 U-statistics for Two-Sample Problems 6.5 Measures of Association", " Chapter 6 U-Statistics 6.1 Definition Suppose we have i.i.d. observations \\(X_{1}, \\ldots, X_{n}\\). U-statistics are a family of statistics used to estimate quantities that can be written as \\[\\begin{equation} \\theta = E\\Big\\{ h(X_{1}, \\ldots, X_{r}) \\Big\\} \\tag{6.1} \\end{equation}\\] The U-statistic \\(U\\) which estimates (6.1) is given by the following formula: \\[\\begin{equation} U = \\frac{1}{{n \\choose r}} \\sum_{c \\in \\mathcal{C}_{n,r}} h(X_{c_{1}}, \\ldots, X_{c_{r}}) \\tag{6.2} \\end{equation}\\] The function \\(h\\) is usually called the kernel of the U-statistic. We will assume the kernel is symmetric. The integer \\(r\\) is called the order of the U-statistic. Typically, \\(r=1\\), \\(r = 2\\), or \\(r = 3\\) at most. In (6.2), \\(\\mathcal{C}_{n,r}\\) denotes the set of all \\({n \\choose r}\\) combinations of size \\(r\\) from the set \\(\\{1, \\ldots, n\\}\\). For example, if \\(n = 3\\) and \\(r = 2\\) then \\[\\begin{equation} \\mathcal{C}_{n, r} = \\{ (1,2), (1,3), (2, 3) \\} \\nonumber \\end{equation}\\] For many common U-statistics \\(r=2\\) in which case (6.2) can be written as \\[\\begin{equation} U = \\frac{2}{n(n-1)} \\sum_{i=1}^{n}\\sum_{j=i+1}^{n} h(X_{i}, X_{j}) \\end{equation}\\] 6.2 Examples A wide range of well-known statistics can be represented as U-statistics. 6.2.1 Example 1: The Sample Mean The sample mean is actually an example of a U-statistic with \\(r = 1\\). Choosing \\(h(x) = x\\) means that the corresponding U-statistic is \\[\\begin{equation} U_{m} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i} \\nonumber \\end{equation}\\] Taking the expectation of \\(h(X_{1})\\) gives the parameter that \\(U_{m}\\) is estimating \\[\\begin{equation} E\\{ h(X_{1}) \\} = E\\{ X_{1} \\} = \\mu \\nonumber \\end{equation}\\] 6.2.2 Example 2: The Sample Variance The sample variance is actually another example of a U-statistic. In this case, \\(r = 2\\). To show why this is the case, choose the kernel \\(h(x_{1}, x_{2})\\) to be \\[\\begin{equation} h(x_{1}, x_{2}) = \\frac{1}{2}(x_{1} - x_{2})^{2} \\nonumber \\end{equation}\\] The expectation of this kernel is \\(\\sigma^{2} = E\\{ h(X_{1}, X_{2}) \\}\\) because \\[\\begin{eqnarray} E\\{ h(X_{1}, X_{2}) \\} &amp;=&amp; \\frac{1}{2}\\Big[ E(X_{1}^{2}) - 2E(X_{1})E(X_{2}) + E(X_{2}^{2}) \\Big] \\nonumber \\\\ &amp;=&amp; \\frac{1}{2}\\Big[ \\sigma^{2} + \\mu^{2} - 2\\mu^{2} + \\sigma^{2} + \\mu^{2} \\Big] \\nonumber \\\\ &amp;=&amp; \\sigma^{2} \\tag{6.3} \\end{eqnarray}\\] Also, by using formula (6.2), this choice of kernel generates the sample variance at its U-statistic: \\[\\begin{eqnarray} U_{var} &amp;=&amp; \\frac{1}{{n \\choose 2}} \\sum_{c \\in \\mathcal{C}_{n,2}} h(X_{c_{1}}, X_{c_{2}}) = \\frac{2}{n(n-1)}\\sum_{i=1}^{n} \\sum_{j=i+1}^{n} \\frac{1}{2} (X_{i} - X_{j})^{2} \\nonumber \\\\ &amp;=&amp; \\frac{2}{n(n-1)}\\sum_{i=1}^{n} \\sum_{j=1}^{n} \\frac{1}{4}(X_{i} - X_{j})^{2} \\nonumber \\\\ &amp;=&amp; \\frac{1}{2n(n-1)}\\sum_{i=1}^{n} \\sum_{j=1}^{n} \\{ (X_{i} - \\bar{X})^{2} - 2(X_{i} - \\bar{X})(X_{j} - \\bar{X}) + (X_{j} - \\bar{X})^{2} \\} \\nonumber \\\\ &amp;=&amp; \\frac{1}{2n(n-1)}\\sum_{i=1}^{n} n(X_{i} - \\bar{X})^{2} + \\frac{1}{2n(n-1)}\\sum_{j=1}^{n} n(X_{j} - \\bar{X})^{2} \\nonumber \\\\ &amp;=&amp; \\frac{1}{n-1}\\sum_{i=1}^{n} (X_{i} - \\bar{X})^{2} \\nonumber \\end{eqnarray}\\] Typically, the variance has the interpretation of \\(\\sigma^{2} = E\\{ (X_{i} - \\mu)^{2} \\}\\). That is, \\(\\sigma^{2}\\) measures the expected squared deviation of \\(X_{i}\\) from its mean. Given the form of the U-statistic (6.3), we can also interpret the variance in the following way: if we select two observations \\(X_{i}\\) and \\(X_{j}\\) at random, the expected squared distance between \\(X_{i}\\) and \\(X_{j}\\) will be \\(2\\sigma^{2}\\). You can see this through the following computer experiment. n &lt;- 50000 xx &lt;- rlogis(n, location=2, scale=0.75) diff.sq &lt;- rep(0, 5000) for(k in 1:5000) { idx &lt;- sample(1:n, size=2) diff.sq[k] &lt;- (xx[idx[1]] - xx[idx[2]])^2 } round(var(xx), 3) ## [1] 1.839 round(mean(diff.sq)/2, 3) ## [1] 1.88 6.2.3 Example 3: Gini’s Mean Difference Gini’s mean difference statistic is defined as \\[\\begin{equation} U_{G} = \\frac{1}{{n \\choose 2}} \\sum_{i=1}^{n}\\sum_{j=i+1}^{n} | X_{i} - X_{j} | \\nonumber \\end{equation}\\] This is a U-statistic with \\(r=2\\) and kernel \\[\\begin{equation} h(X_{1},X_{2}) = | X_{1} - X_{2} | \\nonumber \\end{equation}\\] The parameter that we are estimating with Gini’s mean difference statistic is: \\[\\begin{equation} \\theta_{G} = E\\Big\\{ \\Big| X_{1} - X_{2} \\Big| \\Big\\} \\nonumber \\end{equation}\\] Gini’s mean difference parameter \\(\\theta_{G}\\) can be interpreted in the following way: If we draw two observations at random from our population, \\(\\theta_{G}\\) represents the expected absolute difference between these two observations. The Gini coefficient \\(\\theta_{Gc}\\) is a popular measure of inequality. It is related to the mean difference parameter via \\[\\begin{equation} \\theta_{Gc} = \\frac{ \\theta_{G}}{ 2\\mu }, \\nonumber \\end{equation}\\] where \\(\\mu = E( X_{i} )\\). Exercise 6.1. Compute the Gini coefficient \\(\\theta_{Gc}\\) when it is assumed that \\(X_{i} \\sim \\textrm{Normal}( \\mu, \\sigma^{2})\\), for \\(\\mu &gt; 0\\). \\(X_{i} \\sim \\textrm{Exponential}(\\lambda)\\), (Hint: The difference between two independent Exponential random variables has a Laplace distribution). 6.2.4 Example 4: Wilcoxon Signed Rank Statistic The Wilcoxon signed rank test statistic is related to the following U statistic \\[\\begin{equation} U_{WS} = \\frac{2}{n(n-1)}\\sum_{i=1}^{n}\\sum_{j=i+1}^{n} I\\Big( X_{i} + X_{j} \\geq 0 \\Big) \\nonumber \\end{equation}\\] \\(U_{WS}\\) is a U-statistic of order \\(2\\) with kernel \\[\\begin{equation} h(x, y) = I\\Big( x + y \\geq 0\\Big) \\nonumber \\end{equation}\\] Hence, \\(U_{WS}\\) can be interpreted as an estimate of the following parameter \\[\\begin{equation} \\theta_{WS} = P\\Big( X_{i} + X_{j} \\geq 0 \\Big) = P\\Big( X_{i} \\geq -X_{j} \\Big) \\nonumber \\end{equation}\\] If the distribution of \\(X_{i}\\) is symmetric around \\(0\\), \\(\\theta_{WS}\\) will be equal to \\(1/2\\). Recall that the Wilcoxon signed rank test is designed to detect distributions which are not symmetric around \\(0\\). The Wilcoxon signed rank statistic \\(T_{n}\\) that we defined in Chapter 3 had the following formula \\[\\begin{equation} T_{n} = \\sum_{i=1}^{n} \\textrm{sign}( X_{i}) R_{i}( |\\mathbf{X}| ) \\nonumber \\end{equation}\\] Additional algebra can show that \\[\\begin{eqnarray} T_{n} &amp;=&amp; n(n-1) U_{WS} + 2\\sum_{i=1}^{n} I(X_{i} &gt; 0) - \\frac{n(n+1)}{2} \\nonumber \\\\ &amp;=&amp; n(n-1) U_{WS} + 2 S_{n} - \\frac{n(n+1)}{2} \\tag{6.4} \\end{eqnarray}\\] where \\(S_{n}\\) is the sign test statistic defined in Chapter 3. For large \\(n\\), \\(T_{n}\\) is largely determined by \\(U_{WS} - 1/2\\). Hence, a “large” value of \\(U_{WS} - 1/2\\) will lead to rejection of the one-sample null hypothesis discussed in Section 3.3. If you want to derive (6.4) (though you don’t need to know how), I think it is helpful to note the following \\[\\begin{eqnarray} I( X_{(i)} &gt; 0)R_{(i)}(|\\mathbf{X}|) &amp;=&amp; \\sum_{j=1}^{n} I( X_{(i)} &gt; 0)I(|X_{(i)}| \\geq |X_{j}|) = \\sum_{j=1}^{n} I(X_{(i)} \\geq |X_{j}|) \\nonumber \\\\ &amp;=&amp; \\sum_{j=1}^{n} I(X_{(i)} \\geq |X_{(j)}|) = \\sum_{j=1}^{i} I(X_{(i)} \\geq -X_{(j)}) \\nonumber \\\\ &amp;=&amp; \\sum_{j=1}^{i} I(X_{(i)} + X_{(j)} \\geq 0) \\nonumber \\end{eqnarray}\\] 6.3 Inference using U-statistics By using a large-sample approximation, you can construct a confidence interval for your U-statistic parameter of interest \\(\\theta\\) where \\[\\begin{equation} \\theta = E\\Big\\{ h(X_{1}, \\ldots, X_{r}) \\Big\\} \\end{equation}\\] While the U-statistic is a sum of random variables that are not necessarily independent, there is a type of Central Limit Theorem for \\(U\\)-statistics. Specifically, under appropriate regularity conditions: \\[\\begin{equation} \\sqrt{n}(U - \\theta) \\longrightarrow \\textrm{Normal}\\Big( 0, r^{2} \\varphi \\Big) \\nonumber \\end{equation}\\] The formula for \\(\\varphi\\) is \\[\\begin{equation} \\varphi = \\textrm{Cov}\\Big( h(X_{1}, X_{2}, \\ldots, X_{r}) , h(X_{1}, X_{2}&#39;, \\ldots, X_{r}&#39;) \\Big), \\nonumber \\end{equation}\\] where \\(X_{1}&#39;, X_{2}&#39;, \\ldots, X_{r}&#39;\\) are thought of as another i.i.d. sample from the same distribution as \\(X_{1}, \\ldots, X_{r}\\). 6.4 U-statistics for Two-Sample Problems In two-sample problems, we have data from two groups which we label \\(X_{1}, \\ldots, X_{n}\\) and \\(Y_{1}, \\ldots, Y_{m}\\) A U-statistic with order \\((r,s)\\) for a two-sample problem is \\[\\begin{equation} U = \\frac{1}{{n \\choose r}}\\frac{1}{{m \\choose s}} \\sum_{c \\in \\mathcal{C}_{n,r}} \\sum_{q \\in \\mathcal{C}_{m,s}} h(X_{c_{1}}, \\ldots, X_{c_{r}}, Y_{q_{1}}, \\ldots, Y_{q_{s}}) \\end{equation}\\] 6.4.1 The Mann-Whitney Statistic Consider the following U-statistic \\[\\begin{equation} U_{MW} = \\frac{1}{mn}\\sum_{i=1}^{n}\\sum_{j=1}^{m} I( X_{i} \\geq Y_{j}) \\end{equation}\\] This is a U-statistic of order \\((1,1)\\) with kernel \\(h(x, y) = I(x \\geq y)\\). Hence, the U-statistic \\(U_{MW}\\) can be thought of as an estimate of the following parameter \\[\\begin{equation} \\theta_{MW} = P\\Big( X_{i} \\geq Y_{j} \\Big) \\tag{6.5} \\end{equation}\\] If both \\(X_{i}\\) and \\(Y_{j}\\) have the same distribution, then \\(\\theta_{MW}\\) should equal \\(1/2\\). The statistic \\(mn U_{MW}\\) is known as the Mann-Whitney statistic. The Mann-Whitney statistic has a close relation to the Wilcoxon rank sum statistic \\(W\\) that we defined in Section 3.2: \\[\\begin{eqnarray} mn U_{MW} &amp;=&amp; \\sum_{i=1}^{n}\\sum_{j=1}^{m} I( X_{i} \\geq Y_{j}) \\nonumber \\\\ &amp;=&amp; \\sum_{i=1}^{n}\\Big[ \\sum_{j=1}^{m} I( X_{i} \\geq Y_{j}) + \\sum_{k=1}^{n} I( X_{i} \\geq X_{k}) \\Big] - \\sum_{i=1}^{n}\\sum_{k=1}^{n} I( X_{i} \\geq X_{k}) \\nonumber \\\\ &amp;=&amp; \\sum_{i=1}^{n} R_{i}(\\mathbf{Z}) - \\sum_{i=1}^{n} R_{i}( \\mathbf{X} ) \\tag{6.6} \\\\ &amp;=&amp; W - \\frac{n(n+1)}{2} \\nonumber \\end{eqnarray}\\] In other words, the Mann-Whitney statistic is equal to the WRS statistic minus a constant term. In (6.6), we are defining \\(\\mathbf{Z}\\) as the pooled-data vector \\(\\mathbf{Z} = (X_{1}, \\ldots, X_{n}, Y_{1}, \\ldots, Y_{m})\\). Also, the above derivation assumes no ties so that \\(\\sum_{i=1}^{n} R_{i}( \\mathbf{X} ) = n(n+1)/2\\). Because \\(W = n(n+1)/2 + mn U_{MW}\\) is a linear function of \\(U_{MW}\\), inference from the Wilcoxon rank sum test (when using large-sample p-values) should match inferences made from using \\(U_{MW}\\) to test the hypothesis \\(H_{0}: \\theta_{MW} = 1/2\\). In other words, the two-sided Wilcoxon rank sum test can be thought of as a test of \\(H_{0}: \\theta_{MW} = 1/2\\) vs. \\(H_{A}:\\theta_{MW} \\neq 1/2\\), where \\(\\theta_{MW}\\) is the parameter defined in (6.5). 6.5 Measures of Association Many important measures of association are also examples of U-statistics. For measures of association, we have observations on \\(n\\) pairs of variables \\[\\begin{equation} (X_{1}, Y_{1}), \\ldots, (X_{n}, Y_{n}), \\nonumber \\end{equation}\\] and our goal is to report some measure which quantifies the relationship between these two variables. In this context, we will think about U-statistics which have the form \\[\\begin{equation} U = \\frac{1}{{n \\choose r}} \\sum_{c \\in \\mathcal{C}_{n,r} } h\\Bigg( \\begin{bmatrix} X_{c_{1}} \\\\ Y_{c_{1}} \\end{bmatrix}, \\ldots, \\begin{bmatrix} X_{c_{r}} \\\\ Y_{c_{r}} \\end{bmatrix} \\Bigg) \\end{equation}\\] 6.5.1 Spearman’s Rank Correlation Spearman’s sample rank correlation is defined as \\[\\begin{eqnarray} \\hat{\\rho}_{R} &amp;=&amp; \\frac{\\sum_{i=1}^{n} \\{R_{i}(\\mathbf{X}) - \\bar{R}(\\mathbf{X}) \\}\\{ R_{i}(\\mathbf{Y}) - \\bar{R}(\\mathbf{Y}) \\}}{ \\big[ \\sum_{i=1}^{n} \\{R_{i}(\\mathbf{X}) - \\bar{R}(\\mathbf{X}) \\}^{2} \\sum_{i=1}^{n}\\{ R_{i}(\\mathbf{Y}) - \\bar{R}(\\mathbf{Y}) \\}^{2} \\big]^{1/2} } \\nonumber \\\\ &amp;=&amp; \\frac{12}{n(n-1)(n+1)}\\sum_{i=1}^{n} R_{i}( \\mathbf{X} )R_{i}(\\mathbf{Y}) - \\frac{3(n+1)}{n-1}, \\tag{6.7} \\end{eqnarray}\\] where \\(\\bar{R}(X) = \\frac{1}{n}\\sum_{i=1}^{n} R_{i}( \\mathbf{X} )\\) and \\(\\bar{R}( \\mathbf{Y} ) = \\frac{1}{n} \\sum_{i=1}^{n} R_{i}( \\mathbf{Y} )\\). Remember that \\(R_{i}(\\mathbf{X})\\) denotes the rank of \\(X_{i}\\) when only using the vector \\(\\mathbf{X} = (X_{1}, \\ldots, X_{n})\\) to compute the rankings. Likewise, \\(R_{i}(\\mathbf{Y})\\) denotes the rank of \\(Y_{i}\\) when only using the vector \\(\\mathbf{Y} = (Y_{1}, \\ldots, Y_{n})\\) to compute the rankings. Notice that \\(\\hat{\\rho}_{R}\\) comes from applying the usual Pearson’s estimate of correlation to the ranks \\((R_{i}( \\mathbf{X} )\\), \\(R_{i}(\\mathbf{Y}) )\\) rather than the original data \\((X_{i}, Y_{i})\\). As with the usual estimate of correlation, \\(\\hat{\\rho}_{R}\\) is large (i.e., closer to 1) whenever large values of \\(X_{i}\\) tend to be associated with large values of \\(Y_{i}\\). Similarly, \\(\\hat{\\rho}_{R}\\) is small wheneve large values of \\(X_{i}\\) tend to be associated with small values of \\(Y_{i}\\). Values of \\(\\hat{\\rho}_{R}\\) near zero indicate that there is little association between these two variables. Due to its use of ranks, \\(\\hat{\\rho}_{R}\\) is less sensitive to outliers than Pearson’s correlation. Another important feature of \\(\\hat{\\rho}_{R}\\) is that it is invariant to monotone transformations of the data. While Pearson’s correlation is very effective for detecting linear associations between two variables, the rank correlation is very effective at detecting any monotone associations between two variables. \\(\\hat{\\rho}_{R}\\) will equal 1 if \\(Y_{i}\\) is a monotone increasing function of \\(X_{i}\\), and \\(\\hat{\\rho}_{R}\\) will equal -1 if \\(Y_{i}\\) is a monotone decreasing function \\(X_{i}\\). xx &lt;- pmax(rnorm(100, mean=10), 0.01) yy &lt;- pmax(xx + rnorm(100, sd=.5), 0.01) ## Compare the usual Pearson&#39;s correlation between ## (xx, yy) and (xx, yy^2) round( c( cor(xx, yy), cor(xx, yy^2)), 3) ## [1] 0.885 0.884 ## Now do the same for Spearman&#39;s rank correlation round(c( cor(xx, yy, method=&quot;spearman&quot;), cor(xx, yy^2, method=&quot;spearman&quot;)), 3) ## [1] 0.872 0.872 \\(\\hat{\\rho}_{R}\\) can be thought of as an estimate of the following population quantity: \\[\\begin{equation} \\theta_{R} = 12 P\\Big( X_{1} \\geq X_{2}, Y_{1} \\geq Y_{3} \\Big) - 3 \\end{equation}\\] To justify this, first notice that \\[\\begin{eqnarray} V_{R} &amp;=&amp; \\frac{1}{n^{3}}\\sum_{i=1}^{n} R_{i}(\\mathbf{X})R_{i}(\\mathbf{Y}) = \\frac{1}{n^{3}}\\sum_{i=1}^{n} \\sum_{j=1}^{n} I(X_{i} \\geq X_{j}) \\sum_{k=1}^{n} I(Y_{i} \\geq Y_{k}) \\nonumber \\\\ &amp;=&amp; \\frac{1}{n^{3}}\\sum_{i=1}^{n} \\sum_{j=1}^{n} \\sum_{k=1}^{n} I(X_{i} \\geq X_{j}) I(Y_{i} \\geq Y_{k}) \\nonumber \\end{eqnarray}\\] While \\(V_{R}\\) is not exactly a U-statistic, it can be thought of as roughly a “U-statistic” with non-symmetric kernel function \\[\\begin{equation} h\\Bigg( \\begin{bmatrix} X_{1} \\\\ Y_{1} \\end{bmatrix}, \\begin{bmatrix} X_{1} \\\\ Y_{1} \\end{bmatrix}, \\begin{bmatrix} X_{3} \\\\ Y_{3} \\end{bmatrix} \\Bigg) = I(X_{1} \\geq X_{2}) I(Y_{1} \\geq Y_{3}) \\nonumber \\end{equation}\\] So, we should expect \\(V_{R}\\) to converge to \\(P\\{X_{1} \\geq X_{2}, Y_{1} \\geq Y_{3}\\}\\) as \\(n\\) gets larger. Using (6.7) and our formula for \\(V_{R}\\), we can write \\(\\hat{\\rho}_{R}\\) as \\[\\begin{eqnarray} \\hat{\\rho}_{R} &amp;=&amp; \\frac{12}{n(n-1)(n+1)}\\sum_{i=1}^{n} R_{i}( \\mathbf{X} )R_{i}(\\mathbf{Y}) - \\frac{3(n+1)}{n-1} \\nonumber \\\\ &amp;=&amp; 12 V_{R} \\Big( \\frac{n^{3}}{n(n-1)(n+1)} \\Big) - \\frac{3(n+1)}{n-1}. \\nonumber \\end{eqnarray}\\] Exercise 6.2. Why does \\(\\theta_{R}\\) equal zero when \\(X_{i}\\) and \\(Y_{i}\\) are independent? Why is \\(-1\\leq \\theta_{R} \\leq 1\\)? 6.5.2 Kendall’s tau Ignoring the possibility of ties, Kendall’s \\(\\tau\\)-statistic \\(U_{\\tau}\\) is given by \\[\\begin{eqnarray} U_{\\tau} &amp;=&amp; \\frac{2}{n(n-1)}\\sum_{i=1}^{n}\\sum_{j=i+1}^{n} \\Bigg[ 2 \\times I\\Big\\{ (X_{j} - X_{i})(Y_{j} - Y_{i}) &gt; 0 \\Big\\} - 1 \\Bigg] \\nonumber \\end{eqnarray}\\] Note that \\(U_{\\tau}\\) is a U-statistic of order \\(2\\) with kernel \\[\\begin{equation} h\\Bigg( \\begin{bmatrix} X_{1} \\\\ Y_{1} \\end{bmatrix}, \\begin{bmatrix} X_{2} \\\\ Y_{2} \\end{bmatrix} \\Bigg) = 2 \\times I\\Big\\{ (X_{2} - X_{1})(Y_{2} - Y_{1}) &gt; 0 \\Big\\} - 1 \\end{equation}\\] Assuming the probability of ties is zero, Kendall’s \\(\\tau\\) can be thought of as an estimate of the following quantity \\[\\begin{equation} \\theta_{\\tau} = 2 P\\Big\\{ (X_{j} - X_{i})(Y_{j} - Y_{i}) &gt; 0 \\Big\\} - 1 \\end{equation}\\] Kendall’s \\(\\tau\\) must be in between \\(-1\\) and \\(1\\). If \\(X_{i}\\) and \\(Y_{i}\\) are idependent, Kendall’s \\(\\tau\\) will be equal to zero (why?). In the context of computing \\(U_{\\tau}\\), pairs of observations \\((X_{i}, Y_{i})\\) and \\((X_{j}, Y_{j})\\) are said to be concordant if the sign of \\(X_{j} - X_{i}\\) agrees with the sign of \\(Y_{j} - Y_{i}\\). If the sign of \\(X_{j} - X_{i}\\) and \\(Y_{j} - Y_{i}\\) do not agree, then the pairs \\((X_{i}, Y_{i})\\) and \\((X_{j}, Y_{j})\\) are said to be discordant. If either \\(X_{j}=X_{i}\\) or \\(Y_{j}=Y_{i}\\), then the pairs \\((X_{i}, Y_{i})\\) and \\((X_{j}, Y_{j})\\) are neither concordant or discordant. Let us define the following \\[\\begin{eqnarray} n_{c} &amp;=&amp; \\textrm{ the number of concordant pairs} \\nonumber \\\\ n_{d} &amp;=&amp; \\textrm{ the number of discordant pairs} \\nonumber \\\\ n_{n} &amp;=&amp; \\textrm{ number of pairs which are neither} \\nonumber \\end{eqnarray}\\] Here, we are counting \\(n_{c}\\) and \\(n_{d}\\) from the number of unique possible pairings. There are \\(n(n-1)/2\\) unique pairings, and hence \\[\\begin{equation} n_{c} + n_{d} + n_{n} = {n \\choose 2} = \\frac{n(n-1)}{2} \\end{equation}\\] Notice that \\(n_{c}\\) can be expressed in terms of indicator functions as \\[\\begin{equation} n_{c} = \\sum_{i=1}^{n}\\sum_{j=i+1}^{n} I\\Big\\{ (X_{j} - X_{i})(Y_{j} - Y_{i}) &gt; 0 \\Big\\} \\nonumber \\end{equation}\\] If we assume that there are no ties (i.e., \\(n_{n} = 0\\)), then \\(U_{\\tau}\\) can be written as \\[\\begin{equation} U_{\\tau} = \\frac{4n_{c}}{n(n-1)} - 1 = \\frac{2n_{c} + 2n_{c} - n(n - 1)}{n(n-1)} = \\frac{2n_{c} - 2n_{d} }{n(n-1)} = \\frac{2(n_{c} - n_{d})}{n(n-1)} \\nonumber \\end{equation}\\] Under independence, the number of concordant and discordant pairs should be roughly equal. We just need ordinal data to use Kendall’s \\(\\tau\\). Kendall’s \\(\\tau\\) can be computed as long you can tell if one observation is “larger” than another. Kendall’s \\(\\tau\\) is often used in the context of assessing the agreement between different ratings. In this context, we might have \\(K\\) different judges which are rating \\(J\\) different objects. If \\(r_{jk}\\) denotes the object-j rating given by judge \\(k\\), Kendall’s \\(\\tau\\) from the \\(J\\) pairs \\((r_{11}, r_{12}), \\ldots, (r_{J1}, r_{J2})\\) would give a measure of the agreement between judges 1 and 2. 6.5.3 Distance Covariance and Correlation A correlation equal to zero does not imply that two random variables are independent. For example, if \\(X \\sim \\textrm{Normal}(0, 1)\\), then \\[\\begin{equation} \\textrm{Corr}(X, X^{2}) = \\textrm{Cov}(X, X^{2}) = E( X^{3} ) = 0 \\nonumber \\end{equation}\\] This is also true for Spearman’s rank correlation and Kendall’s \\(\\tau\\). You can have situations where \\(\\theta_{R} = 0\\) but \\(X\\) and \\(Y\\) are not independent. Similarly, you can have situations where \\(\\theta_{\\tau} = 0\\) while \\(X\\) and \\(Y\\) are not independent. Note that the association between the two variables in the figures below is non-monotone. 6.5.3.1 Definition Distance covariance and distance correlation are two measures of dependence that have been developed much more recently (see Székely et al. (2007)). The interesting thing about these two measures is that: if they equal zero then it implies that the two random variables are independent. Moreover, the measures have a relatively straightforward formula, and they have easily computable estimates. For i.i.d. bivariate random variables \\((X_{1}, Y_{1}), \\ldots, (X_{n}, Y_{n})\\), the squared distance covariance parameter is defined as \\[\\begin{eqnarray} \\theta_{dCov,XY}^{2} &amp;=&amp; E\\Big\\{ |X_{1} - X_{2}| |Y_{1} - Y_{2}| \\Big\\} + E\\Big\\{ |X_{1} - X_{2}| \\Big\\}E\\Big\\{ |Y_{1} - Y_{2}| \\Big\\} \\nonumber \\\\ &amp;-&amp; 2E\\Big\\{ |X_{1} - X_{2}||Y_{1} - Y_{3}| \\Big\\} \\nonumber \\end{eqnarray}\\] The distance correlation bettween \\(X_{i}\\) and \\(Y_{i}\\) is then defined as \\[\\begin{equation} \\rho_{d, XY} = \\frac{ \\theta_{dCov,XY} }{\\theta_{dCov,XX} \\theta_{dCov, YY} } \\nonumber \\end{equation}\\] Notice that we must have \\(\\theta_{dCov, XY} \\geq 0\\) and \\(\\rho_{d, XY} \\geq 0\\). There is no notion of a negative correlation when using distance correlation. The interpretation of \\(\\theta_{dCov,XY}^{2}\\) is perhaps not as clear as the usual correlation parameter. Nevertheless, \\(\\theta_{dCov,XY} = 0\\) implies independence, and larger values of \\(\\theta_{dCov,XY}\\) imply that \\(X_{i}\\) and \\(Y_{i}\\) have some form of ``greater&quot; association. Example Let us consider the example we had before where we compared \\(X\\) and \\(X^{2}\\). Specifically, suppose we have observed pairs \\((X_{1}, Y_{1}), \\ldots, (X_{n},Y_{n})\\) where \\(X_{i} \\sim \\textrm{Normal}(0, 1)\\) and \\(Y_{i} = X_{i}^{2}\\). In this case, the distance covariance turns out to be \\[\\begin{eqnarray} &amp;&amp;\\theta_{dCov,XY}^{2} = E\\Big\\{ |X_{1} - X_{2}| |Y_{1} - Y_{2}| \\Big\\} + E\\Big\\{ |X_{1} - X_{2}| \\Big\\}E\\Big\\{ |Y_{1} - Y_{2}| \\Big\\} - 2E\\Big\\{ |X_{1} - X_{2}||Y_{1} - Y_{3}| \\Big\\} \\nonumber \\\\ &amp;&amp;= E\\Big\\{ |X_{1} - X_{2}| |X_{1}^{2} - X_{2}^{2}| \\Big\\} + E\\Big\\{ |X_{1} - X_{2}| \\Big\\}E\\Big\\{ |X_{1}^{2} - X_{2}^{2}| \\Big\\} - 2E\\Big\\{ |X_{1} - X_{2}||X_{1}^{2} - X_{3}^{2}| \\Big\\} \\nonumber \\end{eqnarray}\\] It could be a lot work to compute the above expectation exactly. However, we can estimate it pretty closely using simulation: set.seed(4157) nreps &lt;- 500000 ## number of simulation replications term1 &lt;- term2 &lt;- term3 &lt;- term4 &lt;- rep(0, nreps) for(k in 1:nreps) { xx &lt;- rnorm(3) term1[k] &lt;- abs(xx[1] - xx[2])*abs(xx[1]^2 - xx[2]^2) term2[k] &lt;- abs(xx[1] - xx[2]) term3[k] &lt;- abs(xx[1]^2 - xx[2]^2) term4[k] &lt;- abs(xx[1] - xx[2])*abs(xx[1]^2 - xx[3]^2) } dcov.sq.est &lt;- mean(term1) + mean(term2)*mean(term3) - 2*mean(term4) dcov.sq.est ## [1] 0.137895 The squared distance covariance for this example seems to be about \\(0.14\\). Thus, the distance covariance is positive for this example where the two variables are dependent while the usual covariance between these two variables is zero. Exercise 6.2. For the example where we have observed pairs \\((X_{1}, Y_{1}), \\ldots, (X_{n},Y_{n})\\) with \\(X_{i} \\sim \\textrm{Normal}(0, 1)\\) and \\(Y_{i} = X_{i}^{2}\\), compute Kendall’s \\(\\tau\\) parameter \\(\\theta_{\\tau}\\). 6.5.3.2 Estimation of Distance Covariance and Distance Correlation The distance covariance and correlation are estimated by using a bunch of pairwise distances between our observations. The pairwise distances \\(a_{ij}\\) and \\(b_{ij}\\) for the \\(X_{i}\\) and \\(Y_{i}\\) are defined as \\[\\begin{eqnarray} a_{ij} &amp;=&amp; | X_{i} - X_{j}| \\nonumber \\\\ b_{ij} &amp;=&amp; | Y_{i} - Y_{j}| \\nonumber \\end{eqnarray}\\] We then construct the \\(n \\times n\\) matrix \\(\\mathbf{A}\\) (with elements \\(A_{ij}\\)) and the \\(n \\times n\\) matrix \\(\\mathbf{B}\\) (with elements \\(B_{ij}\\)) in the following way \\[\\begin{equation} A_{ij} = \\begin{cases} a_{ij} - \\frac{1}{n-2} a_{i.} - \\frac{1}{n-2} a_{.j} + \\frac{1}{(n-1)(n-2)}a_{..} &amp; \\textrm{ if } i \\neq j \\nonumber \\\\ 0 &amp; \\textrm{ if } i = j \\nonumber \\end{cases} \\end{equation}\\] \\[\\begin{equation} B_{ij} = \\begin{cases} b_{ij} - \\frac{1}{n-2} b_{i.} - \\frac{1}{n-2} b_{.j} + \\frac{1}{(n-1)(n-2)}b_{..} &amp; \\textrm{ if } i \\neq j \\nonumber \\\\ 0 &amp; \\textrm{ if } i = j \\nonumber \\end{cases} \\end{equation}\\] where \\(a_{i.} = \\sum_{k=1}^{n} a_{ik}\\), \\(a_{.j} = \\sum_{k=1}^{n} a_{kj}\\), and \\(a_{..} = \\sum_{k=1}^{n}\\sum_{l=1}^{n} a_{ij}\\). In other words, \\(\\mathbf{A}\\) is a matrix containing “centered” pairwise distances. The estimate of the squared distance covariance parameter is then given by \\[\\begin{equation} \\hat{\\theta}_{dCov,XY}^{2} = \\frac{1}{n(n-3)}\\sum_{i=1}^{n}\\sum_{j=1}^{n} A_{ij}B_{ij} \\end{equation}\\] The estimate of the distance correlation is \\[\\begin{equation} \\hat{\\rho}_{d, XY} = \\frac{ \\hat{\\theta}_{dCov,XY} }{\\hat{\\theta}_{dCov,XX} \\hat{\\theta}_{dCov, YY} } \\end{equation}\\] It turns out that \\(\\hat{\\theta}_{dCov, XY}^{2}\\) is a U-statistic of order \\(4\\) (see Huo and Székely (2016) for a justification of this). It has kernel function \\[\\begin{equation} h\\Bigg( \\begin{bmatrix} X_{1} \\\\ Y_{1} \\end{bmatrix}, \\begin{bmatrix} X_{1} \\\\ Y_{1} \\end{bmatrix}, \\begin{bmatrix} X_{3} \\\\ Y_{3} \\end{bmatrix}, \\begin{bmatrix} X_{4} \\\\ Y_{4}\\end{bmatrix} \\Bigg) = \\frac{1}{4}\\sum_{i=1}^{4}\\sum_{j=1}^{4} A_{ij}B_{ij} \\end{equation}\\] You can compute distance covariances and distance correlations using the energy package in R. library(energy) n &lt;- 5000 ## generate &quot;parabola&quot; data xx1 &lt;- rnorm(n, sd=0.5) yy1 &lt;- xx1^2 + rnorm(n, sd=0.05) ## generate circle data xx2 &lt;- runif(n, min=-1, max=1) yy2 &lt;- sample(c(-1,1), size=n, replace=TRUE)*sqrt(1 - xx2^2) + rnorm(n, sd=.05) d.cor1 &lt;- dcor(xx1, yy1) d.cor2 &lt;- dcor(xx2, yy2) par(mfrow=c(1,2)) plot(xx1, yy1, xlab=&quot;x&quot;, ylab=&quot;y&quot;, main=paste(&quot;Sample Distance Corr. = &quot;, round(d.cor1 ,4)), las=1) plot(xx2, yy2, xlab=&quot;x&quot;, ylab=&quot;y&quot;, main=paste(&quot;Sample Distance Corr. = &quot;, round(d.cor2, 4)), las=1) ## Let&#39;s just compare the values of distance correlation and Pearson&#39;s ## for both examples p.cor1 &lt;- cor(xx1, yy1) p.cor2 &lt;- cor(xx2, yy2) kend.cor1 &lt;- cor(xx1, yy1, method=&quot;kendall&quot;) kend.cor2 &lt;- cor(xx2, yy2, method=&quot;kendall&quot;) spear.cor1 &lt;- cor(xx1, yy1, method=&quot;spearman&quot;) spear.cor2 &lt;- cor(xx2, yy2, method=&quot;spearman&quot;) # Pearson, Kendall&#39;s-tau, Rank, Distance Correlation round(c(p.cor1, kend.cor1, spear.cor1, d.cor1), 4) ## parabola ## [1] -0.0124 0.0077 0.0109 0.5268 round(c(p.cor2, kend.cor2, spear.cor2, d.cor2), 4) ## circle ## [1] 0.0134 0.0041 0.0138 0.1508 References "],
["edf.html", "Chapter 7 The Empirical Distribution Function 7.1 Definition and Basic Properties 7.2 Confidence intervals for F(t) 7.3 The Empirical Distribution Function in R 7.4 The Kolmogorov-Smirnov Test 7.5 The empirical distribution function and statistical functionals 7.6 Additional Reading", " Chapter 7 The Empirical Distribution Function 7.1 Definition and Basic Properties Every random variable has a cumulative distribution function (cdf). The cdf of a random variable \\(X\\) is defined as \\[\\begin{equation} F(t) = P( X \\leq t) \\end{equation}\\] The empirical distribution function or empirical cumulative distribution function (ecdf) estimates \\(F(t)\\) by computing the proportion of observations which are less than or equal to \\(t\\). For i.i.d. random variables \\(X_{1}, \\ldots, X_{n}\\) with cdf \\(F\\), the empirical distribution function is defined as \\[\\begin{equation} \\hat{F}_{n}(t) = \\frac{1}{n}\\sum_{i=1}^{n} I( X_{i} \\leq t) \\nonumber \\end{equation}\\] Note that the empirical distribution function can be computed for any type of data without making any assumptions about the distribution from which the data arose. The only assumption we are making is that \\(X_{1}, \\ldots, X_{n}\\) constitute an i.i.d. sample from some common distribution function \\(F\\). For example, if we observed \\(X_{1} = 0.7\\), \\(X_{2} = 0.2\\), and \\(X_{3} = 1.3\\), the corresponding empirical distribution function would be \\[\\begin{equation} \\hat{F}_{3}(t) = \\begin{cases} 0 &amp; \\textrm{ for } t &lt; 0.2 \\\\ 1/3 &amp; \\textrm{ for } 0.2 \\leq t &lt; 0.7 \\\\ 2/3 &amp; \\textrm{ for } 0.7 \\leq t &lt; 1.3 \\\\ 1 &amp; \\textrm{ for } t \\geq 1.3 \\end{cases} \\end{equation}\\] 7.2 Confidence intervals for F(t) For a fixed value of \\(t\\), the distribution of \\(n\\hat{F}_{n}(t)\\) is \\[\\begin{equation} n \\hat{F}_{n}(t) \\sim \\textrm{Binomial}\\big( n, F(t) \\big) \\end{equation}\\] This is because, for a fixed \\(t\\), \\(n\\hat{F}_{n}(t)\\) is the sum of \\(n\\) independent Bernoulli random variables \\(W_{1}^{t}, \\ldots, W_{n}^{t}\\) \\[\\begin{equation} n \\hat{F}_{n}(t) = \\sum_{i=1}^{n} W_{i}^{t} = \\sum_{i=1}^{n} I( X_{i} \\leq t) \\end{equation}\\] The probability that \\(W_{i}^{t} = 1\\) is \\[\\begin{equation} P( W_{i}^{t} = 1) = P(X_{i} \\leq t) = F(t) \\nonumber \\end{equation}\\] Pointwise Confidence Intervals Because \\(\\hat{F}_{n}(t)\\) is a mean of independent random variables, we can say that \\[\\begin{equation} \\frac{ \\sqrt{n}\\Big( \\hat{F}_{n}(t) - F(t) \\Big) }{\\sqrt{ \\hat{F}_{n}(t)(1 - \\hat{F}_{n}(t))}} \\longrightarrow \\textrm{Normal}\\Big(0, 1 \\Big) \\nonumber \\end{equation}\\] The above asymptotic statement is the basis for constructing pointwise confidence intervals for \\(F(t)\\). For a fixed \\(t\\), a \\(100 \\times (1-\\alpha)\\%\\) confidence interval for \\(F(t)\\) is the following \\[\\begin{eqnarray} CI_{\\alpha}^{pw}(t) &amp;=&amp; [L_{\\alpha}^{pw}(t), U_{\\alpha}^{pw}(t)] \\nonumber \\\\ L_{\\alpha}^{pw}(t) &amp;=&amp; \\max\\Bigg\\{\\hat{F}_{n}(t) - z_{1 - \\alpha/2} \\sqrt{ \\frac{\\hat{F}_{n}(t)(1 - \\hat{F}_{n}(t)) }{n} }, 0 \\Bigg\\} \\nonumber \\\\ U_{\\alpha}^{pw}(t) &amp;=&amp; \\min\\Bigg\\{ \\hat{F}_{n}(t) + z_{1 - \\alpha/2} \\sqrt{ \\frac{\\hat{F}_{n}(t)(1 - \\hat{F}_{n}(t)) }{n} }, 1 \\Bigg\\} \\tag{7.1} \\end{eqnarray}\\] Plotting \\(CI_{\\alpha}^{pw}(t)\\) for different values of \\(t\\), would give pointwise confidence intervals for the distribution function. Plotting pointwise confidence intervals for \\(F(t)\\) or for survival functions \\(S(t) = 1 - F(t)\\) is fairly common in practice. However, these pointwise confidence intervals only hold for each point separately. Simultaneous Confidence Bands Simultaneous confidence bands can be thought of as two functions \\(L_{\\alpha}^{band}(t)\\) and \\(U_{\\alpha}^{band}(t)\\) such that we are “\\(100 \\times (1 - \\alpha)\\)% confident” that all of \\(F(t)\\) is contained within the bands \\(L_{\\alpha}^{band}(t)\\) and \\(U_{\\alpha}^{band}(t)\\). Specifically, we want the statement \\[\\begin{equation} L_{\\alpha}^{band}(t) \\leq F(t) \\leq U_{\\alpha}^{band}(t) \\quad \\textrm{ for all } t \\end{equation}\\] to hold with at least \\(1 - \\alpha\\) probability. In other words, we want less than \\(\\alpha\\) probability for any part of the path of \\(F(t)\\) going outside of the bands. One choice of \\(L_{\\alpha}^{band}(t)\\) and \\(U_{\\alpha}^{band}(t)\\) which has this property is the following \\[\\begin{equation} L_{\\alpha}^{band}(t) = \\max\\{\\hat{F}_{n}(t) - \\delta_{\\alpha,n}, 0 \\} \\qquad U_{\\alpha}^{band}(t) = \\min\\{\\hat{F}_{n}(t) + \\delta_{\\alpha,n}, 1 \\}, \\tag{7.2} \\end{equation}\\] where \\(\\delta_{\\alpha,n}\\) is given by \\[\\begin{equation} \\delta_{\\alpha, n} = \\sqrt{\\frac{1}{2n} \\ln\\Big(\\frac{2}{\\alpha} \\Big)} \\nonumber \\end{equation}\\] The reason this choice of confidence band works is the Dvoretzky-Kiefer-Wolfowitz (DKW) inequality. The DKW inequality states that \\[\\begin{equation} P\\Bigg( \\sup_{t} |F(t) - \\hat{F}_{n}(t) | &gt; \\varepsilon \\Bigg) \\leq 2 e^{-2n \\varepsilon^{2}} \\nonumber \\end{equation}\\] Our choice of confidence bands (7.2) then works because \\[\\begin{equation} \\sup_{t} | F(t) - \\hat{F}_{n}(t)| \\leq \\delta_{\\alpha, n} \\nonumber \\end{equation}\\] is equivalent to \\[\\begin{equation} L_{\\alpha}^{band}(t) \\leq F(t) \\leq U_{\\alpha}^{band}(t) \\qquad \\textrm{for all } t \\nonumber \\end{equation}\\] Then, from the DKW inequality we have \\[\\begin{eqnarray} P\\Bigg( L_{\\alpha}^{band}(t) \\leq F(t) \\leq U_{\\alpha}^{band}(t) \\quad \\textrm{for all } t \\Bigg) &amp;=&amp; P\\Bigg( \\sup_{t} | F(t) - \\hat{F}_{n}(t)| \\leq \\delta_{n, \\alpha} \\Bigg) \\nonumber \\\\ &amp;\\geq&amp; 1 - 2 e^{-2n \\delta_{\\alpha,n}^{2}} \\nonumber \\\\ &amp;=&amp; 1 - \\alpha. \\nonumber \\end{eqnarray}\\] Confidence bands will almost always be wider than the pointwise confidence intervals. This extra width is due to the fact that we are requiring the coverage probability to hold for the entire path of \\(F(t)\\) rather than at just a single point. 7.3 The Empirical Distribution Function in R We will see how to work with empirical distribution functions in R by using data from a study on kidney function. This dataset has \\(157\\) observations which has the age of each study participant and a measure of overall kidney function. The data can be obtained at https://web.stanford.edu/~hastie/CASI_files/DATA/kidney.txt We will only look at the tot variable in this chapter. kidney &lt;- read.table(&quot;https://web.stanford.edu/~hastie/CASI_files/DATA/kidney.txt&quot;, header=TRUE) head(kidney) ## age tot ## 1 18 2.44 ## 2 19 3.86 ## 3 19 -1.22 ## 4 20 2.30 ## 5 21 0.98 ## 6 21 -0.50 The ecdf function is the main function which computes the empirical distribution function in R The ecdf function will create an ecdf object. To create an ecdf object for the kidney totals, use the following code: kidney.Fhat &lt;- ecdf(kidney$tot) You can plot the ecdf for the kidney totals by just calling plot(ecdf) plot(kidney.Fhat, main = &quot;Kidney Data: Default plot for ecdf&quot;, las=1) If you don’t like the look of the points in the ecdf plot, you can use add the argument do.points = FALSE when calling plot. Also, you can add the argument verticals =TRUE if you want the plot to draw vertical lines whenever there is a jump in the empirical distribution function. plot(kidney.Fhat, do.points=FALSE, verticals=TRUE, main = &quot;Kidney Data: ecdf with vertical lines and without points&quot;, las=1, lwd=2) A nice feature of of the ecdf function is that ecdf object can be treated as a function which computes the empirical distribution function. For example, kidney.Fhat &lt;- ecdf(kidney$tot) kidney.Fhat(0) ## [1] 0.5095541 kidney.Fhat( c(-1,1,4) ) ## [1] 0.3057325 0.6560510 0.9745223 R does not plot confidence intervals when plotting the empirical distribution function. We can do this ourselves, by using the pointwise confidence interval formula shown in (7.1) ## 1. First, we will compute the standard errors at each of the ## observed time points tt &lt;- sort(unique(kidney$tot)) std.err &lt;- sqrt(kidney.Fhat(tt)*(1 - kidney.Fhat(tt))/ length(kidney$tot)) ## 2. Now, compute the confidence intervals at each time point ci.low &lt;- pmax(kidney.Fhat(tt) - qnorm(.975)*std.err, 0) ci.upper &lt;- pmin(kidney.Fhat(tt) + qnorm(.975)*std.err, 1) ## 3. Now, plot the results. Note that type=&quot;s&quot; in the lines function produces ## &quot;step functions&quot; which pass through the provided points. plot(kidney.Fhat, do.points=FALSE, verticals=TRUE, main = &quot;Kidney Data: 95% pointwise confidence intervals&quot;, las=1, lwd=2) lines(tt, ci.low, type=&quot;s&quot;, lty=2, lwd=2) lines(tt, ci.upper, type=&quot;s&quot;, lty=2, lwd=2) We could plot the confidence bands as well. n &lt;- length(kidney$tot) ## Compute the confidence bands at each time point ci.band.low &lt;- pmax(kidney.Fhat(tt) - sqrt(log(2/0.05)/(2*n)), 0) ci.band.upper &lt;- pmin(kidney.Fhat(tt) + sqrt(log(2/0.05)/(2*n)), 1) plot(kidney.Fhat, do.points=FALSE, verticals=TRUE, main = &quot;Kidney Data: 95% Confidence Bands&quot;, las=1, lwd=2) lines(tt, ci.band.low, type=&quot;s&quot;, lty=2, lwd=2) lines(tt, ci.band.upper, type=&quot;s&quot;, lty=2, lwd=2) Comparing the pointwise confidence intervals and the simultaneous confidence bands in the same plot shows how much wider the confidence bands are: 7.4 The Kolmogorov-Smirnov Test The one-sample Kolmogorov-Smirnov (KS) test is a type of goodness-of-fit test that is based on the empirical distribution function. The KS test will test whether or not our data \\(X_{1}, \\ldots, X_{n}\\) comes from a specific distribution of interest \\(F_{0}\\). Supposing our data \\(X_{1}, \\ldots, X_{n}\\) are an i.i.d. sample with distribution function \\(F\\), the hypothesis test of interest can be stated as \\[\\begin{equation} H_{0}: F = F_{0} \\quad \\textrm{ vs. } \\quad H_{A}: F \\neq F_{0} \\end{equation}\\] The one-sample KS test could be used, for example, as a test of normality. The one-sample Kolmogorov-Smirnov test statistic \\(KS_{n}^{(1)}\\) looks at the maximum distance between the empirical distribution function and \\(F_{0}\\) \\[\\begin{equation} KS_{n}^{(1)} = \\sup_{t} \\big| \\hat{F}_{n}(t) - F_{0}(t) \\big| \\nonumber \\end{equation}\\] Large values of \\(KS_{n}^{(1)}\\) provide evidence againts the null hypothesis. Under \\(H_{0}\\), \\(\\sqrt{n}KS_{n}^{(1)}\\) converges in distribution to a Kolmogorov distribution as \\(n\\) goes to infinity. The Kolmogorov distribution has the following cumulative distribution function: \\[\\begin{equation} F_{Kolmo}(t) = 1 - 2\\sum_{j=1}^{\\infty} (-1)^{(j+1)} e^{-2j^{2}t^{2}} \\nonumber \\end{equation}\\] The one-sample KS test can be performed in R using the ks.test function. For one-sample tests, you have to provide the “name” of the distribution function that you are choosing for \\(F_{0}\\). xx &lt;- rt(100, df=2) ## generate 100 observations from a t-dist with 2 d.f. ks.test(xx, y=&quot;pnorm&quot;) ## test that these data follow Normal(0, 1) ## ## One-sample Kolmogorov-Smirnov test ## ## data: xx ## D = 0.12565, p-value = 0.08504 ## alternative hypothesis: two-sided You can even test that the data follow some other \\(\\textrm{Normal}(\\mu, \\sigma^{2})\\) by just providing mean and sd arguments. ks.test(xx, y=&quot;pnorm&quot;, mean=1, sd=2) ## ## One-sample Kolmogorov-Smirnov test ## ## data: xx ## D = 0.29394, p-value = 6.254e-08 ## alternative hypothesis: two-sided Suppose we have data from two groups: \\(X_{1}, \\ldots, X_{n} \\sim F_{X}\\) and \\(Y_{1}, \\ldots, Y_{m} \\sim F_{Y}\\). The two-sample KS test performs a test of the following hypothesis \\[\\begin{equation} H_{0}: F_{X} = F_{Y} \\quad \\textrm{vs.} \\quad H_{A}: F_{X} \\neq F_{Y}. \\nonumber \\end{equation}\\] In this case, we are only testing whether the distributions of the two groups are different. We are not testing whether observations from group 1 tend to be larger (or smaller) than those from group 2. The two-sample KS test compares the empirical distribution functions from these two groups. The two-sample KS test statistic is defined as the maximum distance between the two empirical distribution functions: \\[\\begin{equation} KS_{n,m}^{(2)} = \\sup_{t} \\big| \\hat{F}_{n,X}(t) - \\hat{F}_{m,Y}(t) \\big| \\nonumber \\end{equation}\\] Here, \\(\\hat{F}_{n,X}(t) = \\frac{1}{n}\\sum_{i=1}^{n} I(X_{i} \\leq t)\\) and \\(\\hat{F}_{m,Y}(t) = \\frac{1}{m}\\sum_{j=1}^{m} I(Y_{j} \\leq t)\\) denote the empirical distribution functions from the X and Y samples. The two-sample KS test statistic also converges to the same limit as the one-sample KS test statistic. In particular, under \\(H_{0}\\): \\[\\begin{equation} \\sqrt{ \\frac{nm}{n + m } }KS_{n,m}^{(2)} \\longrightarrow \\textrm{Kolmogorov} \\qquad \\textrm{ as } n,m \\longrightarrow \\infty \\nonumber \\end{equation}\\] The ks.test function in R also performs two-sample KS tests. xx &lt;- rnorm(100) yy &lt;- rlogis(100) ks.test(xx, yy) ## ## Two-sample Kolmogorov-Smirnov test ## ## data: xx and yy ## D = 0.18, p-value = 0.07832 ## alternative hypothesis: two-sided We can compute the KS statistic ourselves and check that this matches the value of the KS statistic returned by the ks.test function: zz &lt;- c(xx, yy) zz.order &lt;- sort(zz) F.x &lt;- ecdf(xx) F.y &lt;- ecdf(yy) KS.stat &lt;- max( abs( F.x(zz.order) - F.y(zz.order) ) ) KS.stat ## [1] 0.18 Exercise 7.1. Why does \\[\\begin{equation} KS_{n,m}^{(2)} = \\max_{1 \\leq i \\leq n+m} \\big| \\hat{F}_{n,X}(Z_{(i)}) - \\hat{F}_{n,Y}(Z_{(i)}) \\big|, \\nonumber \\end{equation}\\] where \\(\\mathbf{Z} = (Z_{1}, \\ldots, Z_{n+m})\\) denotes the pooled sample and \\(Z_{(1)}, \\ldots, Z_{(n+m)}\\) denote the order statistics from \\(\\mathbf{Z}\\)? 7.5 The empirical distribution function and statistical functionals In many areas of mathematics, it is common to refer to a function which is a “functions of functions” as a functional. For example, \\(T(f)\\) which is defined as \\[\\begin{equation} T(f) = \\int_{0}^{1} f^{2}(x) dx \\end{equation}\\] is a functional because \\(T(f)\\) takes arguments which are functions and outputs real numbers. Many common parameters that we encounter in statistics can be thought of as functionals where the input of the functional is usually a distribution function. For example, the mean is an example of a functional \\[\\begin{equation} \\mu(F) = \\int x dF(x) = \\int x f(x) dx \\nonumber \\end{equation}\\] As indicated by the notation \\(\\mu(F)\\), the value of the mean depends on the underlying distribution function \\(F\\). Also, the variance is an example of a functional \\[\\begin{equation} \\sigma^{2}(F) = \\int (x - \\mu(F))^{2} dF(x) = \\int x^{2} dF(x) - \\mu^{2}(F) \\nonumber \\end{equation}\\] The median is an example of a functional \\[\\begin{equation} \\textrm{med}(F) = F^{-1}(1/2) \\nonumber \\end{equation}\\] The tail probability \\(P(X_{i} &gt; c)\\) is also a functional \\[\\begin{equation} \\theta_{c}(F) = \\int I(x &gt; c) dF(x) \\nonumber \\end{equation}\\] Many common estimators can be thought of as coming from “plugging in” the empirical cdf into the appropriate statistical functional. For example, plugging in the empirical cdf into the mean functional gives: \\[\\begin{equation} \\mu( \\hat{F}_{n} ) = \\int x d\\hat{F}_{n}(x) = \\frac{1}{n}\\sum_{i=1}^{n} X_{i} = \\bar{X} \\nonumber \\end{equation}\\] Plugging in the empirical cdf into the tail probability functional gives \\[\\begin{equation} \\theta_{c}( \\hat{F}_{n} ) = \\int I(x &gt; c) d\\hat{F}_{n}(x) = \\frac{1}{n}\\sum_{i=1}^{n}I(X_{i} &gt; c) = 1 - \\hat{F}_{n}(c) \\nonumber \\end{equation}\\] The sample variance is not quite a plug-in estimate for \\(\\sigma^{2}(F)\\) but it is very close \\[\\begin{eqnarray} \\sigma^{2}(\\hat{F}_{n}) &amp;=&amp; \\int x^{2} d\\hat{F}_{n}(x) - \\mu^{2}(\\hat{F}_{n}) = \\frac{1}{n} \\sum_{i=1}^{n} X_{i}^{2} - \\bar{X}^{2} \\nonumber \\\\ &amp;=&amp; \\frac{1}{n} \\sum_{i=1}^{n} (X_{i} - \\bar{X})^{2} = \\frac{n-1}{n} \\hat{\\sigma}^{2} \\nonumber \\end{eqnarray}\\] This notation for statistical functionals will be useful when we discuss the bootstrap later in the course. The notation for statistical functionals is also very useful in the context of influence functions and robust statistics, but we will not discuss these topics in this course. 7.6 Additional Reading Additional reading which covers the material discussed in this chapter includes: Chapter 2 from Wasserman (2006) References "],
["density-estimation.html", "Chapter 8 Density Estimation 8.1 Introduction 8.2 Histograms 8.3 A Box-type Density Estimate 8.4 Kernel Density Estimation 8.5 Cross-Validation for Bandwidth Selection 8.6 Density Estimation in R 8.7 Additional Reading", " Chapter 8 Density Estimation 8.1 Introduction In this section, we focus on methods for estimating a probability density function (pdf) \\(f(x)\\). For a continuous random variable \\(X\\), areas under the probability density function are probabilities \\[\\begin{equation} P(a &lt; X &lt; b) = \\int_{a}^{b} f(x) dx \\nonumber \\end{equation}\\] and \\(f(x)\\) is related to the cumulative distribution function via \\(f(x) = F&#39;(x)\\). With parametric approaches to density estimation, you only need to estimate several parameters as these parameters completely determine the form of \\(f(x)\\). For example, with a Gaussian distribution you only need to find \\(\\mu\\) and \\(\\sigma^{2}\\) to determine the form of \\(f(x)\\). In a nonparametric approach to estimating a pdf, we will assume that our observations \\(X_{1}, \\ldots, X_{n}\\) are an independent identically distribution sample from a distribution with pdf \\(f(x)\\), but otherwise we will make few assumptions about the particular form of \\(f(x)\\). 8.2 Histograms Figure 8.1: Histogram of ages from kidney function data. Data retrieved from: https://web.stanford.edu/~hastie/CASI_files/DATA/kidney.txt 8.2.1 Definition While histograms are often thought of as mainly a visualization tool, a histogram can also be thought of as an estimate of the density \\(f(x)\\). To construct a histogram, you first need to define a series of “bins”: \\(B_{1}, \\ldots, B_{D_{n}}\\). Each bin is a left-closed interval. That is, the bins have the form \\(B_{k} = [x_{0} + (k-1)h_{n}, x_{0} + kh_{n})\\): \\[\\begin{eqnarray} B_{1} &amp;=&amp; [x_{0}, x_{0} + h_{n}) \\nonumber \\\\ B_{2} &amp;=&amp; [x_{0} + h_{n}, x_{0} + 2h_{n}) \\nonumber \\\\ &amp;\\vdots&amp; \\nonumber \\\\ B_{D_{n}} &amp;=&amp; [x_{0} + (D_{n}-1)h_{n}, x_{0} + D_{n}h_{n}) \\nonumber \\end{eqnarray}\\] \\(x_{0}\\) - the origin \\(h_{n}\\) - bin width \\(D_{n}\\) - number of bins Histograms are based on the counts \\(n_{k}\\) of observations that fall into each bin: \\[\\begin{eqnarray} n_{k} &amp;=&amp; \\# \\text{ of observations falling into the $k^{th}$ bin } \\nonumber \\\\ &amp;=&amp; \\sum_{i=1}^{n} I( x_{0} + (k-1)h_{n} \\leq X_{i} &lt; x_{0} + kh_{n} ) \\nonumber \\end{eqnarray}\\] From the counts \\(n_{k}\\), the histogram estimate of the density at a point \\(x\\) in the \\(k^{th}\\) bin (that is if \\(x_{0} + (k-1)h_{n} \\leq x &lt; x_{0} + kh_{n}\\)), is defined as \\[\\begin{equation} \\hat{f}_{h_{n}}^{H}(x) = \\frac{n_{k}}{nh_{n}} \\nonumber \\tag{8.1} \\end{equation}\\] Note: Histogram plots often show the actual bin counts \\(n_{k}\\) rather than the values of \\(\\hat{f}_{h_{n}}^{H}(x)\\). To see the motivation for the histogram estimate, notice that if we choose a relatively small value of \\(h_{n} &gt; 0\\) \\[\\begin{equation} P(a &lt; X_{i} &lt; a + h_{n}) = \\int_{a}^{a + h_{n}} f(t) dt \\approx h_{n}f(c), \\nonumber \\end{equation}\\] for any point \\(a \\leq c \\leq a + h_{n}\\). So, for a point \\(x \\in B_{k}\\), the expected value of \\(\\hat{f}_{h_{n}}^{H}(x)\\) is \\[\\begin{eqnarray} E\\{ \\hat{f}_{h_{n}}^{H}(x) \\} &amp;=&amp; \\frac{1}{n h_{n}} E\\{ n_{k} \\} \\nonumber \\\\ &amp;=&amp; \\frac{1}{n h_{n}} \\sum_{i=1}^{n} P( x_{0} + (k-1)h_{n} \\leq X_{i} &lt; x_{0} + kh_{n} ) \\nonumber \\\\ &amp;=&amp; \\frac{1}{h_{n}} P( x_{0} + (k-1)h_{n} \\leq X_{i} &lt; x_{0} + kh_{n} ) \\nonumber \\\\ &amp;\\approx&amp; f(x) \\nonumber \\end{eqnarray}\\] 8.2.2 Histograms in R In R, histograms are computed using the hist function hist(x, breaks, probability, plot, ...) The breaks argument Default is “Sturges”. This is a method for finding the bin width. Can be a name giving the name of an algorithm for computing bin width (e.g., “Scott” and “FD”). Can also be a single number. This gives the number of bins used. Could be a vector giving the breakpoints between bins. Could also be a function which computes the number of bins. The probability argument. If this is set to FALSE, then the bin counts are shown in the histogram. If set to TRUE, then the bin counts divided by \\(nh_{n}\\) are shown in the histogram. The plot argument. If TRUE, a histogram is plotted whenever hist is called. If FALSE, a histogram is not plotted when hist is called. Note: The default for R, is to use right-closed intervals \\((a, b]\\). This can be changed using the right argument of the hist function. Let’s use the kidney function data again to demonstrate the use of histograms in R. This time we will focus on the age variable. kidney &lt;- read.table(&quot;https://web.stanford.edu/~hastie/CASI_files/DATA/kidney.txt&quot;, header=TRUE) You can plot a histogram of age just by calling the hist function. kidney.hist &lt;- hist(kidney$age, main=&quot;&quot;, xlab=&quot;Age from Kidney Data&quot;) Use the probability = TRUE argument to plot the density-estimate version of the histogram. This histogram should integrate to 1. kidney.hist2 &lt;- hist(kidney$age, main=&quot;Histogram of Age on Probability Scale&quot;, xlab=&quot;Age from Kidney Data&quot;, probability=TRUE) In addition to generating a histogram plot, the histogram function also returns useful stuff. names(kidney.hist) ## [1] &quot;breaks&quot; &quot;counts&quot; &quot;density&quot; &quot;mids&quot; &quot;xname&quot; &quot;equidist&quot; breaks the boundaries for the histogram bins. The bins are of the form ( breaks[k], breaks[k+1] ] counts the number of observations falling into each bin density the value of the estimated density within each of the bins mids the midpoint of each of the bins kidney.hist$breaks ## [1] 10 20 30 40 50 60 70 80 90 kidney.hist$counts ## [1] 4 74 35 11 13 12 6 2 ## The following sum should match the first element of kidney.hist$counts[1] sum(kidney.hist$breaks[1] &lt; kidney$age &amp; kidney$age &lt;= kidney.hist$breaks[2]) ## [1] 4 Let’s check that the density values returned by hist match our definition of the histogram density estimate in (8.1). binwidth &lt;- kidney.hist$breaks[2] - kidney.hist$breaks[1] kidney.hist$density ## [1] 0.002547771 0.047133758 0.022292994 0.007006369 0.008280255 0.007643312 ## [7] 0.003821656 0.001273885 kidney.hist$counts/(length(kidney$age)*binwidth) ## [1] 0.002547771 0.047133758 0.022292994 0.007006369 0.008280255 0.007643312 ## [7] 0.003821656 0.001273885 8.2.3 Performance of the Histogram Estimate and Bin Width Selection 8.2.3.1 Bias/Variance Decomposition It is common to evaluate the performance of a density estimator through its mean-squared error (MSE). In general, MSE is a function of bias and variance \\[\\begin{equation} \\textrm{MSE} = \\textrm{Bias}^2 + \\textrm{Variance} \\nonumber \\end{equation}\\] We will first look at the mean-squared error of \\(\\hat{f}_{h_{n}}^{H}( x )\\) at a single point \\(x\\) \\[\\begin{eqnarray} \\textrm{MSE}\\{ \\hat{f}_{h_{n}}^{H}(x) \\} &amp;=&amp; E\\Big( \\{ \\hat{f}_{h_{n}}^{H}(x) - f(x) \\}^{2} \\Big) \\nonumber \\\\ &amp;=&amp; E\\Big( \\Big[ \\hat{f}_{h_{n}}^{H}(x) - E\\{ \\hat{f}_{n}^{H}(x) \\} + E\\{ \\hat{f}_{h_{n}}^{H}(x) \\} - f(x) \\Big]^{2} \\Big) \\nonumber \\\\ &amp;=&amp; E\\Big( \\Big[ \\hat{f}_{h_{n}}^{H}(x) - E\\{ \\hat{f}_{n}^{H}(x) \\} \\Big]^{2} \\Big) + E\\Big( \\Big[ E\\{ \\hat{f}_{h_{n}}^{H}(x) \\} - f(x) \\Big]^{2} \\Big) \\nonumber \\\\ &amp;+&amp; 2E\\Big( \\Big[ \\hat{f}_{h_{n}}^{H}(x) - E\\{ \\hat{f}_{n}^{H}(x) \\}\\Big]\\Big[ E\\{ \\hat{f}_{h_{n}}^{H}(x) \\} - f(x) \\Big] \\Big) \\nonumber \\\\ &amp;=&amp; \\underbrace{\\textrm{Var}\\{ \\hat{f}_{h_{n}}^{H}(x) \\}}_{\\textrm{Variance}} + \\underbrace{\\Big( E\\{ \\hat{f}_{h_{n}}^{H}(x) \\} - f(x) \\Big)^{2} }_{\\textrm{Bias Squared}} \\nonumber \\end{eqnarray}\\] In general, as the bin width \\(h_{n}\\) increases, the histogram estimate will have less variation but will become more biased. 8.2.3.2 Bias and Variance of the Histogram Estimate Recall that, for a histogram estimate, we have \\(D_{n}\\) bins where the \\(k^{th}\\) bin takes the form \\[\\begin{equation} B_{k} = [x_{0} + (k-1)h_{n}, x_{0} + kh_{n}) \\nonumber \\end{equation}\\] For a point \\(x \\in B_{k}\\), that “belongs” to the \\(k^{th}\\) bin, the histogram density estimate is \\[\\begin{equation} \\hat{f}_{n}^{H}(x) = \\frac{n_{k}}{nh_{n}}, \\quad \\textrm{ where } n_{k} = \\textrm{ number of observations falling into bin } B_{k} \\nonumber \\end{equation}\\] To better examine what happens as \\(n\\) changes, we will define the function \\(A_{h_{n}, x_{0}}(x)\\) as the function which returns the index of the interval to which \\(x\\) belongs. For example, if \\(x_{0} = 0\\), \\(h_{n} = 1/3\\), and \\(x = 1/2\\), then \\(A_{h_{n}, x_{0}}( x ) = 2\\). So, we can also write the histogram density estimate at the value \\(x\\) as \\[\\begin{equation} \\hat{f}_{h_{n}}^{H}(x) = \\frac{n_{A_{h_{n}, x_{0}}(x)}}{ nh_{n} } \\nonumber \\end{equation}\\] Exercise 8.1. Suppose \\(x_{0} = -2\\) and \\(h_{n} = 1/2\\). What are the values of \\(A_{h_{n}, x_{0}}( -1 )\\), \\(A_{h_{n}, x_{0}}( 1.3 )\\), and \\(A_{h_{n}, x_{0}}( 0.75 )\\)? Note that we can express \\(n_{A_{h_{n}, x_{0}}(x)}\\) as \\[\\begin{equation} n_{A_{h_{n}, x_{0}}(x)} = \\sum_{i = 1}^{n} I\\Big( X_{i} \\in B_{A_{h_{n}, x_{0}}(x)} \\Big) \\nonumber \\end{equation}\\] Hence, is a binomial random variable with \\(n\\) trials and success probability \\(p_{h_{n}, x_{0}}(x)\\) (why?) \\[\\begin{equation} n_{A_{h_{n}}(x)} \\sim \\textrm{Binomial}\\{ n, p_{h_{n}, x_{0}}(x) \\} \\nonumber \\end{equation}\\] The success probability \\(p_{h_{n}, x_{0}}(x)\\) is defined as \\[\\begin{eqnarray} p_{h_{n}, x_{0}}(x) &amp;=&amp; P\\Big\\{ X_{i} \\textrm{ falls into bin } B_{A_{h_{n}, x_{0}}}(x) \\Big\\} \\nonumber \\\\ &amp;=&amp; \\int_{x_{0} + (A_{h_{n}, x_{0}}(x) - 1)h_{n}}^{x_{0} + A_{h_{n}, x_{0}}(x)h_{n} } f(t) dt. \\nonumber \\end{eqnarray}\\] Because \\(n_{A_{h_{n}, x_{0}}(x)}\\) follows a binomial distribution, we know that \\[\\begin{eqnarray} E( n_{A_{h_{n}, x_{0}}(x)} ) &amp;=&amp; np_{h_{n}, x_{0}}(x) \\nonumber \\\\ \\textrm{Var}( n_{A_{h_{n}, x_{0}}(x)} ) &amp;=&amp; np_{h_{n}, x_{0}}(x)\\{1 - p_{h_{n},x_{0}}(x) \\} \\nonumber \\end{eqnarray}\\] So, we can express the bias of the histogram density estimate \\(\\hat{f}_{h_{n}}^{H}(x) = n_{A_{h_{n}, x_{0}}(x)}/(nh_{n})\\) as \\[\\begin{eqnarray} \\textrm{Bias}\\{ \\hat{f}_{h_{n}}^{H}(x) \\} &amp;=&amp; E\\{ \\hat{f}_{h_{n}}^{H}(x) \\} - f(x) \\nonumber \\\\ &amp;=&amp; \\frac{1}{nh_{n}}E( n_{A_{h_{n}, x_{0}}(x)} ) - f(x) \\nonumber \\\\ &amp;=&amp; \\frac{ p_{h_{n}, x_{0}}(x) }{ h_{n} } - f(x), \\nonumber \\end{eqnarray}\\] and we can express the variance as: \\[\\begin{eqnarray} \\textrm{Var}\\{ \\hat{f}_{h_{n}}^{H}(x) \\} &amp;=&amp; \\frac{1}{n^{2}h_{n}^{2}}\\textrm{Var}( n_{A_{h_{n}, x_{0}}(x)} ) \\nonumber \\\\ &amp;=&amp; \\frac{ p_{h_{n}, x_{0}}(x)\\{1 - p_{h_{n}, x_{0}}(x) \\} }{ nh_{n}^{2} } \\nonumber \\end{eqnarray}\\] Using the approximation \\(f(t) \\approx f(x) + f&#39;(x)(t - x)\\) for \\(t\\) close to \\(x\\), we have that \\[\\begin{eqnarray} \\frac{ p_{h_{n}, x_{0}}(x) }{ h_{n} } &amp;=&amp; \\frac{1}{h_{n}}\\int_{x_{0} + (A_{h_{n}, x_{0}}(x) - 1)h_{n}}^{x_{0} + A_{h_{n}, x_{0}}(x)h_{n} } f(t) dt \\nonumber \\\\ &amp;\\approx&amp; \\frac{1}{h_{n}}\\int_{x_{0} + (A_{h_{n}, x_{0}}(x) - 1)h_{n}}^{x_{0} + A_{h_{n}, x_{0}}(x)h_{n} } f(x) dt + \\frac{f&#39;(x)}{h_{n}}\\int_{x_{0} + (A_{h_{n}}(x) - 1)h_{n}}^{x_{0} + A_{h_{n}, x_{0}}(x)h_{n} } (t - x) dt \\nonumber \\\\ &amp;=&amp; f(x) + \\frac{f&#39;(x)}{2h_{n}}\\Big[ (t - x)^{2}\\Big|_{x_{0} + (A_{h_{n}, x_{0}}(x) - 1)h_{n}}^{x_{0} + A_{h_{n}, x_{0}}(x)h_{n} } \\Big] \\nonumber \\\\ &amp;=&amp; f(x) + \\frac{f&#39;(x)}{2h_{n}}\\Big[ (x_{0} + A_{h_{n}, x_{0}}(x)h_{n})^{2} - (x_{0} + (A_{h_{n}, x_{0}}(x)-1)h_{n} )^{2} - 2xh_{n} \\Big] \\nonumber \\\\ &amp;=&amp; f(x) + \\frac{f&#39;(x)}{2h_{n}}\\Big[ 2x_{0}A_{h_{n}, x_{0}}(x)h_{n} + A_{h_{n}, x_{0}}^{2}(x)h_{n}^{2} - 2x_{0}(A_{h_{n}, x_{0}}(x)-1)h_{n} \\nonumber \\\\ &amp; &amp; - (A_{h_{n}, x_{0}}(x)-1)^{2}h_{n}^{2} - 2xh_{n} \\Big] \\nonumber \\\\ &amp;=&amp; f(x) + \\frac{f&#39;(x)}{2h_{n}}\\Big[ 2x_{0}h_{n} + 2A_{h_{n}, x_{0}}(x)h_{n}^{2} - h_{n}^{2} - 2xh_{n} \\Big] \\nonumber \\\\ &amp;=&amp; f(x) + f&#39;(x)\\Big[ h_{n}/2 - [ x - x_{0} - \\{ A_{h_{n}, x_{0}}(x) - 1 \\}h_{n} ] \\Big] \\nonumber \\end{eqnarray}\\] So, the bias of the histogram density estimate \\(\\hat{f}_{h_{n}}^{H}(x)\\) is \\[\\begin{eqnarray} \\textrm{Bias}\\{ \\hat{f}_{h_{n}}^{H}(x) \\} &amp;=&amp; \\frac{ p_{h_{n}, x_{0}}(x) }{ h_{n} } - f(x) \\nonumber \\\\ &amp;\\approx&amp; f&#39;(x)\\Big[ h_{n}/2 - [ x - x_{0} - \\{ A_{h_{n}, x_{0}}(x) - 1 \\}h_{n} ] \\Big] \\tag{8.2} \\end{eqnarray}\\] Choosing a very small bin width \\(h_{n}\\) will result in a small bias because the left endpoint of the bin \\(x_{0} + (A_{h_{n}}(x) - 1)h_{n}\\) will always be very close to \\(x\\). Now, let us turn to the variance of the histogram estimate at \\(x\\): \\[\\begin{eqnarray} \\textrm{Var}\\{ \\hat{f}_{h_{n}}^{H}(x) \\} &amp;=&amp; \\frac{p_{h_{n}, x_{0}}(x) }{nh_{n}^{2}}\\{1 - p_{h_{n}, x_{0}}(x) \\} \\nonumber \\\\ &amp;\\approx&amp; \\frac{f(x) + f&#39;(x)\\{ \\tfrac{h_{n}}{2} - [ x - x_{0} - (A_{h_{n}, x_{0}}(x) - 1)h_{n} ] \\}}{nh_{n}}\\{1 - p_{h_{n}, x_{0}}(x)\\} \\nonumber \\\\ &amp;\\approx&amp; \\frac{f(x)}{n h_{n} } \\tag{8.3} \\end{eqnarray}\\] For a more detailed description of the above approximation see Scott (1979) or Chapter 6 of Wasserman (2006). Note that large bin widths will reduce the variance of \\(\\hat{f}_{h_{n}}^{H}(x)\\). 8.2.3.3 Pointwise Approximate Mean Squared Error Using (8.3) and (8.2), the approximate mean-squared error (AMSE) of the histogram density estimate at a particular point \\(x\\) is given by \\[\\begin{eqnarray} \\textrm{MSE}\\{ \\hat{f}_{h_{n}}^{H}(x) \\} &amp;=&amp; E\\Big( \\{ \\hat{f}_{h_{n}}^{H}(x) - f(x) \\}^{2} \\Big) \\nonumber \\\\ &amp;=&amp; \\Big( \\textrm{Bias}\\{ \\hat{f}_{h_{n}}^{H}(x) \\} \\Big)^{2} + \\textrm{Var}\\{ \\hat{f}_{h_{n}}^{H}(x) \\} \\nonumber \\\\ &amp;\\approx&amp; \\frac{h_{n}^{2}[f&#39;(x)]^{2} }{4} - h_{n}f&#39;(x)[ x - x_{0} - \\{ A_{h_{n}, x_{0}}(x) - 1 \\}h_{n} ] \\nonumber \\\\ &amp;+&amp; [f&#39;(x)]^{2}\\Big( x - x_{0} - \\{ A_{h_{n}, x_{0}}(x) - 1 \\}h_{n} \\Big)^{2} + \\frac{f(x)}{n h_{n} } \\nonumber \\\\ &amp;=&amp; \\textrm{AMSE}\\{ \\hat{f}_{h_{n}}^{H}(x) \\} \\tag{8.4} \\end{eqnarray}\\] For any approach to bin width selection, we should have \\(h_{n} \\longrightarrow 0\\) and \\(nh_{n} \\longrightarrow \\infty\\). This MSE approximation depends on a particular choice of \\(x\\). Difficult to use (8.4) as a criterion for selecting the bandwidth because the best choice of \\(h_{n}\\) will usually depend on your choice of \\(x\\). 8.2.3.4 Integrated Mean Squared Error and Optimal Histogram Bin Width Using the approximate mean integrated squared error (AMISE) allows us to find an optimal bin width that does not depend on a particular choice of \\(x\\). The AMISE is defined as \\[\\begin{eqnarray} \\textrm{AMISE}\\{ \\hat{f}_{h_{n}}^{H} \\} &amp;=&amp; E\\Big\\{ \\int_{-\\infty}^{\\infty} \\{ \\hat{f}_{h_{n}}^{H}(x) - f(x) \\}^{2}dx \\Big\\} \\nonumber \\\\ &amp;=&amp; \\int_{-\\infty}^{\\infty} \\textrm{MSE}\\{ \\hat{f}_{h_{n}}^{H}(x) \\} dx \\nonumber \\end{eqnarray}\\] Using the previously derived approximation (8.4) for the AMSE, it can be shown that \\[\\begin{eqnarray} \\textrm{MISE}\\{ \\hat{f}_{h_{n}}^{H} \\} \\approx \\frac{1}{nh_{n}} + \\frac{h_{n}^{2}}{12}\\int_{-\\infty}^{\\infty} [f&#39;(x)]^{2} dx \\tag{8.5} \\end{eqnarray}\\] To select the optimal bin width, we minimize the MISE as a function of \\(h_{n}\\). Minimizing (8.5), as a function of \\(h_{n}\\) yields the following formula for the optimal bin width \\[\\begin{equation} h_{n}^{opt} = \\Big( \\frac{6}{n \\int_{-\\infty}^{\\infty} [f&#39;(x)]^{2} dx} \\Big)^{1/3} = C n^{-1/3} \\tag{8.6} \\end{equation}\\] Notice that \\(h_{n}^{opt} \\longrightarrow 0\\) and \\(nh_{n}^{opt} \\longrightarrow \\infty\\) as \\(n \\longrightarrow \\infty\\). Notice also that the optimal bin width depends on the unknown quantity \\(\\int_{-\\infty}^{\\infty} [f&#39;(x)]^{2} dx\\). 8.2.4 Choosing the Histogram Bin Width We will mention three rules for selecting the bin width of a histogram. Scott rule: (based on the optimal bin width formula (8.6)) Friedman and Diaconis (FD) rule (also based on the optimal bin width formula (8.6)) Sturges rule: (based on wanting the histogram to look Gaussian when the data are in fact Gaussian-distributed) Both the Scott and the FD rule are based on the optimal bin width formula (8.6). The main problem with using the formula (8.6) is the presence of \\(\\int_{-\\infty}^{\\infty} [f&#39;(x)]^{2} dx\\). Solution: See what this quantity looks like if we assume that \\(f(x)\\) corresponds to a \\(\\textrm{Normal}(\\mu, \\sigma^{2})\\) density. With the assumption that \\(f(x) = \\textrm{Normal}(\\mu, \\sigma^{2})\\): \\[\\begin{equation} h_{n}^{opt} = 3.5 \\sigma n^{-1/3} \\nonumber \\end{equation}\\] Scott rule: Use \\(h_{n}^{*} = 3.5 \\hat{\\sigma} n^{-1/3}\\), where \\(\\hat{\\sigma}\\) denotes the sample standard deviation. FD rule: Use \\(h_{n}^{*} = 2 \\times IQR \\times n^{-1/3}\\). This is a somewhat more robust choice of \\(h_{n}\\) as it is not as sensitive to outliers. Sturges rule: The bin width is chosen so that we have \\(1 + log_{2}(n)\\) bins. This choice tends to give wide intervals. 8.3 A Box-type Density Estimate A related estimator \\(\\hat{f}_{h_{n}}^{B}\\) of the density \\(f(x)\\) uses a “sliding bin” at each point \\(x\\) to calculate the estimate of \\(f(x)\\). Specifically, the “box estimate” \\(\\hat{f}_{h_{n}}^{B}(x)\\) at the point \\(x\\) is defined as \\[\\begin{eqnarray} \\hat{f}_{h_{n}}^{B}(x) &amp;=&amp; \\frac{1}{2nh_{n}} \\Big[ \\# \\text{ of observations falling in the interval } (x - h_{n}, x + h_{n}) \\Big] \\nonumber \\\\ &amp;=&amp; \\frac{1}{2nh_{n}} \\sum_{i=1}^{n} I(x - h_{n} &lt; X_{i} &lt; x + h_{n} ) \\nonumber \\end{eqnarray}\\] In other words, for each \\(x\\) we are forming a bin of width \\(2h_{n}\\) around \\(x\\), and we are counting the number of observations that fall in this bin. You can think of each point \\(x\\) as being the center of its own bin. The expectation of the box estimator at the point \\(x\\) is \\[\\begin{equation} E \\{ \\hat{f}_{h_{n}}^{B}(x) \\} = \\frac{1}{2h_{n}} P(x - h_{n} &lt; X_{i} &lt; x + h_{n}) \\approx f(x) \\nonumber \\end{equation}\\] Unlike the histogram, the box estimate does not require the density estimate to be constant within each bin. Also, histograms can have dramatic changes near the bin edges while the box estimate suffers less from this problem. However, plots of the box estimate will still largely be non-smooth and have a “jagged” appearance. We can also express \\(\\hat{f}_{h_{n}}^{B}(x)\\) in the following way: \\[\\begin{equation} \\hat{f}_{h_{n}}^{B}(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{h_{n}} w\\Big( \\frac{X_{i} - x}{h_{n}} \\Big), \\tag{8.7} \\end{equation}\\] where \\(w(t)\\) is defined as the following “box function” \\[\\begin{equation} w(t) = \\begin{cases} \\frac{1}{2} &amp; \\textrm{ if } |t| &lt; 1 \\nonumber \\\\ 0 &amp; \\textrm{ otherwise } \\nonumber \\end{cases} \\end{equation}\\] While the estimator \\(\\hat{f}_{h_{n}}^{B}\\) does seem reasonable, it always results in density estimates which are not “smooth.” Most kernel density estimates are formed by replacing the box function \\(w(t)\\) with a smoother function. Exercise 8.2. Write an R function which computes the \\(\\hat{f}_{h_{n}}^{B}(x)\\) at a collection of specified points. Exercise 8.3. Suppose we have observations \\((X_{1}, X_{2}, X_{3}, X_{4}) = (-1, 0, 1/2, 1)\\). Assuming \\(h_{n} = 1/2\\), plot \\(w(\\tfrac{X_{i} - x}{h_{n}})/nh_{n}\\) for \\(i = 1, \\ldots, 4\\) and plot the box density estimate \\(\\hat{f}_{h_{n}}^{B}(x)\\). Exercise 8.4. Suppose we have i.i.d. observations \\(X_{1}, \\ldots, X_{n} \\sim F\\) where the cdf \\(F\\) is assumed to be continuous. What is \\(\\textrm{Var}\\{ \\hat{f}_{h_{n}}^{B}(x) \\}\\)? 8.4 Kernel Density Estimation 8.4.1 Definition Kernel density estimates are a generalization of the box-type density estimate \\(\\hat{f}_{h_{n}}^{B}(x)\\). With kernel density estimation, we replace the “box function” in (8.7) with a function \\(K(\\cdot)\\) which is much smoother. The function \\(K(\\cdot)\\) will also give higher weight to observations which are closer to \\(x\\) than those that are further away from \\(x\\). A kernel density estimator \\(\\hat{f}_{h_{n}}(x)\\) of the density \\(f(x)\\) is defined as \\[\\begin{equation} \\hat{f}_{h_{n}}(x) = \\frac{1}{nh_{n}} \\sum_{i=1}^{n} K\\Big( \\frac{x - X_{i}}{h_{n}} \\Big) \\tag{8.8} \\end{equation}\\] The function \\(K( \\cdot )\\) is referred to as the kernel function. The scalar term \\(h_{n} &gt; 0\\) is called the bandwidth. The value of the bandwidth \\(h_{n}\\) largely determines how “bumpy” the density estimate will appear. The appearance and the statistical performance of \\(\\hat{f}_{h_{n}}(x)\\) depend much more on the value of \\(h_{n}\\) than on the choice of kernel function. Kernel functions are usually chosen so that \\[\\begin{equation} K(t) \\geq 0 \\quad \\textrm{ for all } t \\nonumber \\end{equation}\\] and that they also satisfy the following properties: \\[\\begin{eqnarray} K(t) = K(-t) \\qquad \\int_{-\\infty}^{\\infty} K(t) dt = 1 \\nonumber \\qquad \\int_{-\\infty}^{\\infty} K^{2}(t) dt &lt; \\infty \\nonumber \\end{eqnarray}\\] You can think of \\(K(u)\\) as a probability density function which is symmetric around \\(0\\). Some of the most common choices of kernel functions include \\[\\begin{eqnarray} \\textrm{Gaussian} :&amp;&amp; \\quad K(u) = \\exp(-u^{2}/2)/\\sqrt{2\\pi} \\nonumber \\\\ \\textrm{Epanechnikov} :&amp;&amp; \\quad K(u) = \\tfrac{3}{4\\sqrt{5}}(1 - \\tfrac{1}{5} u^{2}) I(|u| &lt; \\sqrt{5}) \\nonumber \\\\ \\textrm{biweight} :&amp;&amp; \\quad K(u) = \\tfrac{15}{16\\sqrt{7}}(1 - \\tfrac{1}{7} u^{2})^{2} I(|u| &lt; \\sqrt{7}) \\nonumber \\end{eqnarray}\\] When plotting \\(\\frac{1}{n h_{n}}K\\big( \\tfrac{x - X_{i}}{h_{n}} \\big)\\) as a function of \\(x\\), it should look like a “small hill” centered around \\(X_{i}\\). As \\(h_{n}\\) decreases, \\(\\frac{1}{n h_{n}}K\\big( \\tfrac{x - X_{i}}{h_{n}} \\big)\\) becomes more strongly concentrated around \\(X_{i}\\) and has a higher peak. The kernel density estimate \\(\\hat{f}_{h_{n}}(x)\\) is a sum of all these “small hills”. The nice thing about (8.8) is that it guarantees that \\(\\hat{f}_{h_{n}}(x)\\) is a probability density function \\[\\begin{eqnarray} \\int_{-\\infty}^{\\infty} \\hat{f}_{h_{n}}(x) dx &amp;=&amp; \\int_{-\\infty}^{\\infty} \\frac{1}{nh_{n}} \\sum_{i=1}^{n} K\\Big( \\frac{x - X_{i}}{h_{n}} \\Big) dx \\nonumber \\\\ &amp;=&amp; \\frac{1}{n} \\sum_{i=1}^{n} \\int_{-\\infty}^{\\infty} \\frac{1}{h_{n}} K\\Big( \\frac{x - X_{i}}{h_{n}} \\Big) dx \\nonumber \\\\ &amp;=&amp; 1 \\nonumber \\end{eqnarray}\\] Also, formula (8.8) guarantees that \\(\\hat{f}_{h_{n}}(x)\\) inherits the smoothness properties of \\(K(u)\\) \\[\\begin{equation} \\hat{f}_{h_{n}}&#39;(x) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{h_{n}^{2}} K&#39;\\Big( \\frac{x - X_{i}}{h_{n}} \\Big) \\nonumber \\end{equation}\\] 8.4.2 Bias, Variance, and AMISE of Kernel Density Estimates As with the bin width in histogram estimation, the bias/variance tradeoff drives the best choice of the bandwidth \\(h_{n}\\) in kernel density estimation. Approximate Bias The exact expectation of a kernel density estimate \\(\\hat{f}_{h_{n}}(x)\\) is \\[\\begin{eqnarray} E\\{ \\hat{f}_{h_{n}}(x) \\} &amp;=&amp; \\frac{1}{nh_{n}} \\sum_{i=1}^{n} E\\Big\\{ K\\Big( \\frac{x - X_{i}}{h_{n}} \\Big) \\Big\\} \\nonumber \\\\ &amp;=&amp; \\frac{1}{h_{n}} E\\Big\\{ K\\Big( \\frac{x - X_{1}}{h_{n}} \\Big) \\Big\\} \\nonumber \\\\ &amp;=&amp; \\frac{1}{h_{n}} \\int_{-\\infty}^{\\infty} K\\Big( \\frac{x - t}{h_{n}} \\Big) f(t) dt \\nonumber \\\\ &amp;=&amp; \\int_{-\\infty}^{\\infty} K( u ) f(x - uh_{n}) du \\nonumber \\end{eqnarray}\\] Above, we used the substitution \\(u = (x - t)/h_{n}\\). Thus, the exact bias of \\(\\hat{f}_{h_{n}}(x)\\) is \\[\\begin{eqnarray} \\textrm{Bias}\\{ \\hat{f}_{h_{n}}(x) \\} &amp;=&amp; \\int_{-\\infty}^{\\infty} K( u ) f(x - uh_{n}) du - f(x) \\nonumber \\\\ &amp;=&amp; \\int_{-\\infty}^{\\infty} K( u ) \\{ f(x - uh_{n}) - f(x) \\} du \\tag{8.9} \\end{eqnarray}\\] The expression for bias shown in (8.9) will only have a closed form for special choices of \\(K(\\cdot)\\) and \\(f( \\cdot )\\). Nevertheless, we can get a reasonable approximation of this bias for small \\(h_{n}\\). To approximate the bias for small \\(h_{n}\\), we can use the following Taylor series approximation \\[\\begin{equation} f(x - uh_{n}) - f(x) \\approx -uh_{n}f&#39;(x) + \\frac{u^{2}h_{n}^{2}}{2} f&#39;&#39;(x) \\nonumber \\end{equation}\\] Plugging this approximation into our expression for the bias in (8.9) gives: \\[\\begin{eqnarray} \\textrm{Bias}\\{ \\hat{f}_{h_{n}}(x) \\} &amp;\\approx&amp; -h_{n}f&#39;(x) \\int_{-\\infty}^{\\infty} u K( u ) du + \\frac{h_{n}^{2}f&#39;&#39;(x)}{2} \\int_{-\\infty}^{\\infty} u^{2} K( u ) du \\nonumber \\\\ &amp;=&amp; \\frac{h_{n}^{2}f&#39;&#39;(x)}{2} \\int_{-\\infty}^{\\infty} u^{2} K( u ) du \\nonumber \\\\ &amp;=&amp; \\frac{h_{n}^{2}f&#39;&#39;(x)\\mu_{2}(K)}{2} \\qquad \\textrm{ where } \\mu_{2}(K) = \\int_{-\\infty}^{\\infty} u^{2} K( u ) du \\nonumber \\\\ &amp;=&amp; \\textrm{ABias}\\{ \\hat{f}_{h_{n}}(x) \\} \\tag{8.10} \\end{eqnarray}\\] Here, \\(\\textrm{ABias}\\{ \\hat{f}_{h_{n}}(x) \\}\\) stands for the approximate bias. Formula (8.10) shows the direct dependence of the magnitude of bias on the value of the bandwidth. Formula (8.10) also shows the effect of the curvature \\(f&#39;&#39;(x)\\) of the density on the magnitude of bias. Approximate Variance The exact variance of a kernel density estimate \\(\\hat{f}_{h_{n}}(x)\\) is \\[\\begin{eqnarray} \\textrm{Var}\\{ \\hat{f}_{h_{n}}(x) \\} &amp;=&amp; \\frac{1}{n^{2}h_{n}^{2}} \\sum_{i=1}^{n} \\textrm{Var}\\Big\\{ K\\Big( \\frac{x - X_{i}}{ h_{n} } \\Big) \\Big\\} \\nonumber \\\\ &amp;=&amp; \\frac{1}{nh_{n}^{2}} \\textrm{Var}\\Big\\{ K\\Big( \\frac{x - X_{1}}{ h_{n} } \\Big) \\Big\\} \\nonumber \\\\ &amp;=&amp; \\frac{1}{n h_{n}^{2} }\\int_{-\\infty}^{\\infty} K^{2}\\Big( \\frac{x - t}{h_{n}} \\Big) f(t) dt - \\frac{1}{n}\\Big[ \\frac{1}{h_{n}}E\\Big\\{ K\\Big( \\frac{x - X_{1}}{ h_{n} } \\Big) \\Big\\} \\Big]^{2} \\nonumber \\\\ &amp;=&amp; \\frac{1}{n h_{n} }\\int_{-\\infty}^{\\infty} K^{2}(u) f(x - uh_{n}) du - \\frac{1}{n}\\Big[ \\textrm{Bias}\\{ \\hat{f}_{h_{n}}(x) \\} + f(x) \\Big]^{2} \\nonumber \\end{eqnarray}\\] We will ignore the last term because it is of order \\(1/n\\). Then, if we use the Taylor series approximation \\(f(x - uh_{n}) = f(x) - uh_{n}f&#39;(x) + ...\\), we have: \\[\\begin{eqnarray} \\textrm{Var}\\{ \\hat{f}_{h_{n}}(x) \\} &amp;\\approx&amp; \\frac{f(x)}{n h_{n} }\\int_{-\\infty}^{\\infty} K^{2}(u) du - \\frac{f&#39;(x)}{n}\\int_{-\\infty}^{\\infty} u K^{2}(u) du + ... \\nonumber \\\\ &amp;\\approx&amp; \\frac{f(x)\\kappa_{2}(K) }{n h_{n} } \\quad \\textrm{ where } \\kappa_{2}(K) = \\int_{-\\infty}^{\\infty} K^{2}( u ) du \\nonumber \\\\ &amp;=&amp; \\textrm{AVar}\\{ \\hat{f}_{h_{n}}(x) \\} \\nonumber \\end{eqnarray}\\] Approximate Mean Integrated Squared Error (AMISE) Using our approximations for the bias and variance, we can get an approximate expression for the mean-squared error of \\(\\hat{f}_{h_{n}}(x)\\) at the point \\(x\\) \\[\\begin{eqnarray} \\textrm{MSE}\\{ \\hat{f}_{h_{n}}(x) \\} &amp;\\approx&amp; \\textrm{AVar}\\{ \\hat{f}_{h_{n}}(x) \\} + \\Big( \\textrm{ABias}\\{ \\hat{f}_{h_{n}}(x) \\} \\Big)^{2} \\nonumber \\\\ &amp;=&amp; \\frac{f(x)\\kappa_{2}(K) }{n h_{n} } + \\frac{h_{n}^{4}[f&#39;&#39;(x)]^{2}\\mu_{2}^{2}(K)}{4} \\tag{8.11} \\end{eqnarray}\\] Notice that if we want the MSE to go to \\(0\\) as \\(n \\longrightarrow \\infty\\), we need the following two things to happen: \\(h_{n} \\longrightarrow 0\\) as \\(n \\longrightarrow \\infty\\) \\(n h_{n} \\longrightarrow \\infty\\) as \\(n \\longrightarrow \\infty\\) The approximate mean integrated squared error (AMISE) of the kernel density estimator is obtained by integrating the approximation (8.11) for MSE across \\(x\\) \\[\\begin{eqnarray} \\textrm{AMISE}\\{ \\hat{f}_{h_{n}} \\} &amp;=&amp; \\frac{\\kappa_{2}(K) }{n h_{n} }\\int_{-\\infty}^{\\infty} f(x) dx + \\frac{h_{n}^{4}\\mu_{2}^{2}(K)}{4} \\int_{-\\infty}^{\\infty} [f&#39;&#39;(x)]^{2} dx \\nonumber \\\\ &amp;=&amp; \\frac{\\kappa_{2}(K) }{n h_{n} } + \\frac{h_{n}^{4}\\mu_{2}^{2}(K)}{4} \\int_{-\\infty}^{\\infty} [f&#39;&#39;(x)]^{2} dx \\tag{8.12} \\end{eqnarray}\\] 8.4.3 Bandwidth Selection with the Normal Reference Rule and Silverman’s “Rule of Thumb” If we differentiate the formula for AMISE given in (8.12) with respect to \\(h_{n}\\) and set it to zero, we get the following equation for \\(h_{n}^{opt}\\) which would be the bandwidth minimizing \\(\\textrm{AMISE}\\{ \\hat{f}_{h_{n}} \\}\\): \\[\\begin{equation} 0 = \\frac{-\\kappa_{2}(K) }{n (h_{n}^{opt})^{2} } + (h_{n}^{opt})^{3}\\mu_{2}^{2}(K) \\int_{-\\infty}^{\\infty} [f&#39;&#39;(x)]^{2} dx \\nonumber \\end{equation}\\] The solution of the above equation gives the optimal bandwidth: \\[\\begin{equation} h_{n}^{opt} = n^{-1/5} \\Big( \\int_{-\\infty}^{\\infty} [f&#39;&#39;(x)]^{2} dx \\Big)^{-1/5}\\kappa_{2}(K)^{1/5} \\mu_{2}(K)^{-2/5} \\nonumber \\end{equation}\\] For the case of a Gaussian kernel, \\[\\begin{eqnarray} \\kappa_{2}(K) &amp;=&amp; \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} e^{-u^{2}} du = \\frac{1}{2\\sqrt{\\pi}}\\int_{-\\infty}^{\\infty} \\frac{\\sqrt{2}}{\\sqrt{2\\pi}} e^{-u^{2}} du = \\frac{1}{2\\sqrt{\\pi}} \\nonumber \\\\ \\mu_{2}(K) &amp;=&amp; \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty} u^{2}e^{-u^{2}/2} du = 1 \\nonumber \\end{eqnarray}\\] so that, in the case of a Gaussian kernel, the optimal bandwidth is given by \\[\\begin{equation} h_{n}^{opt} = n^{-1/5} (2\\sqrt{\\pi})^{-1/5} \\Big( \\int_{-\\infty}^{\\infty} [f&#39;&#39;(x)]^{2} dx \\Big)^{-1/5} \\nonumber \\end{equation}\\] However, the optimal bandwidth \\(h_{n}^{opt}\\) for a kernel density estimate depends on the unknown quantity \\[\\begin{equation} \\int_{-\\infty}^{\\infty} [f&#39;&#39;(x)]^{2} dx \\nonumber \\end{equation}\\] Similar to the Scott and FD rules for choosing the bin width of histogram, one way of setting the bandwidth \\(h_{n}\\) of a kernel density estimate is to use the value of \\(\\int_{-\\infty}^{\\infty} [f&#39;&#39;(x)]^{2} dx\\) when it is assumed that \\(f(x)\\) is the density of a \\(\\textrm{Normal}(0, \\sigma^{2})\\) random variable. If \\(f(x) = \\textrm{Normal}(0, \\sigma^{2})\\), then \\[\\begin{equation} \\int_{-\\infty}^{\\infty} [f&#39;&#39;(x)]^{2} dx = \\frac{3}{8\\sqrt{\\pi}\\sigma^{5}} \\nonumber \\end{equation}\\] If we use this assumption about \\(f&#39;&#39;(x)\\) and assume a Gaussian kernel, the formula for the optimal bandwidth becomes bandwidth is \\[\\begin{equation} h_{n}^{opt} = n^{-1/5}(2\\sqrt{\\pi})^{-1/5}\\Big( \\frac{3}{8\\sqrt{\\pi}\\sigma^{5}} \\Big)^{-1/5} = \\sigma n^{-1/5} \\sigma \\Big( \\frac{4}{3} \\Big)^{1/5} \\approx 1.06 \\sigma n^{-1/5} \\nonumber \\end{equation}\\] The rule \\(h_{n}^{opt} = 1.06 \\sigma n^{-1/5}\\) works pretty well if the true density has a roughly Gaussian shape. For example, unimodal, non-heavy tails, not too strong skewness, etc. The Normal reference rule for the bandwidth is \\[\\begin{equation} h_{n}^{NR} = 1.06 \\tilde{\\sigma} n^{-1/5} \\nonumber \\end{equation}\\] Here, \\(\\tilde{\\sigma}\\) is usually either \\(\\tilde{\\sigma} = \\hat{\\sigma}\\) or \\(\\tilde{\\sigma} = s\\). \\(h_{n} = 1.06 \\hat{\\sigma} n^{-1/5}\\) is typically too large if \\(f(x)\\) is multimodal or if \\(f(x)\\) has substantial skewness. This oversmoothing effect of \\(h_{n} = 1.06 \\hat{\\sigma} n^{-1/5}\\) can be reduced somewhat by replacing \\(\\hat{\\sigma}\\) with \\[\\begin{equation} s = \\min\\Big\\{ \\hat{\\sigma}, \\frac{IQR}{1.34} \\Big\\} \\nonumber \\end{equation}\\] For multimodal distributions, we typically have \\(\\hat{\\sigma} \\leq IQR/1.34\\) while for strongly skewed distributions such as the log-normal distribution we typically have \\(\\hat{\\sigma} \\geq IQR/1.34\\). For these reasons, \\(\\tilde{\\sigma} = s\\) is often suggested when using the Normal reference rule. Silverman’s “rule-of-thumb” for the bandwidth is \\[\\begin{equation} h_{n}^{SR} = 0.9 s n^{-1/5} \\nonumber \\end{equation}\\] (see Chapter 3 of Silverman (2018)). This rule is just an adjustment for the fact that \\(1.06 s n^{-1/5}\\) is still too large for many skewed or multimodal distribution and using \\(0.9\\) instead of \\(1.06\\) reduces this problem. Exercise 8.5 Assuming that \\(f(x) = \\tfrac{1}{\\sqrt{2\\pi}\\sigma}e^{-x^{2}/2\\sigma^{2}}\\), show that \\[\\begin{equation} \\int_{-\\infty}^{\\infty} [f&#39;&#39;(x)]^{2} dx = \\frac{3}{8\\sqrt{\\pi}\\sigma^{5}} \\nonumber \\end{equation}\\] Exercise 8.6 Suppose \\(X_{1}, \\ldots, X_{n} \\sim N(0, \\sigma^{2})\\). Suppose that we have density estimate \\(\\hat{f}_{h_{n}}(x)\\) that uses the Gaussian kernel. Compute the exact values of \\(E\\{ \\hat{f}_{h_{n}}(x) \\}\\) and \\(\\textrm{Var}\\{ \\hat{f}_{h_{n}}(x) \\}\\). Compute AMISE\\(\\{ \\hat{f}_{h_{n}} \\}\\). Can you get an expression for the value of \\(h_{n}\\) which minimizes AMISE? 8.5 Cross-Validation for Bandwidth Selection 8.5.1 Squared-Error Cross-Validation The usual goal in bandwidth selection is to choose the bandwidth which minimizes the mean integrated squared error (MISE) \\[\\begin{equation} \\textrm{MISE}(h) = E\\Big[ \\int \\{ \\hat{f}_{h}(x) - f(x) \\}^{2} dx \\Big] \\end{equation}\\] or some related criterion which also measures an expected discrepancy between \\(\\hat{f}_{h}(x)\\) and \\(f(x)\\). The MISE can be rewritten as \\[\\begin{equation} \\textrm{MISE}(h) = E\\Big[ \\int \\hat{f}_{h}^{2}(x) dx \\Big] - 2 \\int E\\Big[ \\hat{f}_{h}(x) \\Big] f(x) dx + \\int f^{2}(x) dx \\tag{8.13} \\end{equation}\\] If the goal is to minimize \\(\\textrm{MISE}(h)\\), we can ignore the last term since it does not depend on \\(h\\). Our goal will be to find a bandwidth that minimizes an estimate of \\(\\textrm{MISE}(h)\\) (an estimate which ignores the last term in (8.13)), and this estimate will use a technique called leave-one-out cross-validation. Regarding the first term in (8.13), we can just estimate this expectation with \\[\\begin{equation} \\int \\hat{f}_{h}^{2}(x) dx \\nonumber \\end{equation}\\] The second term in (8.13) is trickier because it has \\(f(x)\\) in it. We can simplify this term though \\[\\begin{equation} \\int E\\Big[ \\hat{f}_{h}(x) \\Big] f(x) dx = E\\Bigg[ \\frac{1}{h} K\\Big( \\frac{X_{1} - X_{2}}{h} \\Big) \\Bigg] \\nonumber \\end{equation}\\] (why is this true?) We can construct an estimate of (8.13) by first defining the following “leave-one-out” estimate \\[\\begin{equation} \\hat{f}_{h, -i}(x) = \\frac{1}{(n-1)h}\\sum_{j \\neq i} K\\Big( \\frac{x - X_{j}}{h} \\Big) \\nonumber \\end{equation}\\] In other words, \\(\\hat{f}_{h, -i}(x)\\) is a density estimate constructed from using bandwidth \\(h\\) and all the data except for the \\(i^{th}\\) observation. Then, we are going use the following quantity to estimate \\(\\int E[ \\hat{f}_{h}(x) ] f(x) dx\\) \\[\\begin{equation} \\frac{1}{n} \\sum_{i=1}^{n} \\hat{f}_{h, -i}( X_{i} ) = \\frac{1}{n(n-1)} \\sum_{i=1}^{n} \\sum_{j \\neq i} \\frac{1}{h}K\\Big( \\frac{X_{i} - X_{j}}{ h } \\Big) \\nonumber \\end{equation}\\] The term \\(\\frac{1}{n} \\sum_{i=1}^{n} \\hat{f}_{h, -i}( X_{i} )\\) is referred to as a leave-one-out cross-validation estimate. The expectation of this quantity is \\[\\begin{eqnarray} E\\Big\\{ \\frac{1}{n} \\sum_{i=1}^{n} \\hat{f}_{h, -i}( X_{i} ) \\Big\\} &amp;=&amp; \\frac{1}{n(n-1)}\\sum_{i=1}^{n} \\sum_{j \\neq i} E\\Big\\{ \\frac{1}{h} K\\Big( \\frac{X_{i} - X_{j}}{ h } \\Big) \\Big\\} \\nonumber \\\\ &amp;=&amp; E\\Big\\{ \\frac{1}{h} K\\Big( \\frac{X_{1} - X_{2}}{ h } \\Big) \\Big\\} \\nonumber \\end{eqnarray}\\] The leave-one-out cross-validation estimate of the MISE (ignoring the irrelevant \\(\\int f^{2}(x) dx\\)) is \\[\\begin{equation} \\hat{J}_{MISE}(h) = \\int \\hat{f}_{h}^{2}(x) dx - \\frac{2}{n} \\sum_{i=1}^{n} \\hat{f}_{h, -i}( X_{i} ) \\nonumber \\end{equation}\\] The integrated squared error cross-validation choice of the bandwidth \\(h_{n,cv}\\) is the value of \\(h &gt; 0\\) which minimizes \\(\\hat{J}_{MISE}(h)\\) \\[\\begin{equation} h_{n,cv} = \\arg\\min_{h &gt; 0} \\hat{J}_{MISE}(h) \\nonumber \\end{equation}\\] 8.5.2 Computing the Cross-validation Bandwidth R does have built-in capabilities for finding the bandwidth through cross-validation, but let’s do an example ourselves to see how the process works. First, we can build a function called J_mise that has input and output: Input: a value of the bandwidth \\(h\\) and a vector of data Output: the value of \\(\\hat{J}_{MISE}(h)\\) J_mise &lt;- function(h, x) { n &lt;- length(x) loo.val &lt;- rep(0, n) for(k in 1:n) { ## compute the leave-one-out density estimate ## only focus on density on interval (18, 90) loo.fhat &lt;- density(x[-k], bw=h, from=18, to=90) loo.fhat.fn &lt;- approxfun(loo.fhat$x, loo.fhat$y) loo.val[k] &lt;- loo.fhat.fn(x[k]) } fhat &lt;- density(x, bw=h, from=18, to=90) fhat.sq.fn &lt;- approxfun(fhat$x, fhat$y^2) ans &lt;- integrate(fhat.sq.fn, lower=18, upper=90)$value - 2*mean(loo.val) return(ans) } Now, we want to use our R function to compute \\(\\hat{J}(h_{j})\\) over a grid of bandwidth values \\[\\begin{equation} h_{min} \\leq h_{1} &lt; ... &lt; h_{q} \\leq h_{max} \\nonumber \\end{equation}\\] The smallest and largest possible bandwidths \\(h_{min}\\) and \\(h_{max}\\) can be chosen by looking at plots of the density for different bandwidths. \\(h_{min}\\) should correspond to a density estimate which is very nonsmooth while \\(h_{max}\\) should correspond to a density which is oversmoothed. For the ages from the kidney data, \\(h_{min} = 1/2\\) and \\(h_{max} = 5\\) seem like reasonable choices. Let us compute \\(\\hat{J}(h)\\) for a grid of \\(100\\) bandwidths between \\(1/2\\) and \\(5\\): h.grid &lt;- seq(1/2, 5, length.out=100) CV.est &lt;- rep(0, 100) for(j in 1:100) { CV.est[j] &lt;- J_mise(h=h.grid[j], x=kidney$age) } Now, if we plot \\(\\hat{J}(h)\\) vs. \\(h\\), we can see what the best value of the bandwidth is. From the graph, it appears that a bandwidth of roughly \\(h = 1.8\\) minimizes \\(\\hat{J}(h)\\) plot(h.grid, CV.est, xlab=&quot;bandwidth&quot;, ylab=&quot;J_mise(h)&quot;, main = &quot;Cross-Validation Estimates for Age Data&quot;) The specific value of the bandwidth where \\(\\hat{J}(h_{j})\\) reaches its minimum is: h.grid[ which.min(CV.est) ] ## [1] 1.772727 8.5.3 Likelihood Cross-Validation An alternative to MISE is the Kullback-Leibler (KL) divergence \\[\\begin{eqnarray} \\textrm{KL}(h) &amp;=&amp; \\int \\log \\Big( \\frac{ f(x) }{ \\hat{f}_{h}(x) } \\Big) f(x) dx \\nonumber \\\\ &amp;=&amp; -\\int \\log \\{ \\hat{f}_{h}(x) \\} f(x) dx + \\int \\log\\{ f(x) \\}f(x) dx \\nonumber \\end{eqnarray}\\] We only need to get an estimate of \\(\\int \\log \\{ \\hat{f}_{h}(x) \\} f(x) dx\\) because \\(\\int \\log\\{ f(x) \\}f(x) dx\\) does not depend on \\(h\\). We can use the same approach as we did for integrated squared error cross-validation to estimate \\(\\int \\log \\{ \\hat{f}_{h}(x) \\} f(x) dx\\). The leave-one-out cross-validation estimate of the KL divergence (ignoring the irrelevant \\(\\int \\log\\{ f(x) \\}f(x) dx\\) term) is \\[\\begin{equation} \\hat{J}_{KL}(h) = -\\frac{1}{n} \\sum_{i=1}^{n} \\log \\{ \\hat{f}_{h, -i}( X_{i} ) \\} \\nonumber \\end{equation}\\] Choosing the bandwidth which minimizes \\(\\hat{J}_{KL}(h)\\) is often referred to as likelihood cross-validation. An R function which can compute \\(\\hat{J}_{KL}(h)\\) is given below: J_KL &lt;- function(h, x) { n &lt;- length(x) loo.val &lt;- rep(0, n) for(k in 1:n) { ## compute the leave-one-out density estimate ## only focus on density on interval (18, 90) loo.fhat &lt;- density(x[-k], bw=h, from=18, to=90) loo.fhat.fn &lt;- approxfun(loo.fhat$x, loo.fhat$y) loo.val[k] &lt;- (-1)*log(loo.fhat.fn(x[k])) } return(mean(loo.val)) } Exercise 8.7 Using the age variable from the kidney function data, find the best bandwidth using a Gaussian kernel and likelihood cross-validation. 8.6 Density Estimation in R In R, kernel density estimates are computed using the density function density(x, bw, kernel, n, ...) x - the vector containing the data bw - the value of the bandwidth. bw = nrd0 gives the default bandwidth rule. This is Silverman’s rule-of-thumb \\(h_{n} = 0.9 s n^{-1/5}\\) bw = nrd gives the bandwidth \\(h_{n} = 1.06 \\hat{\\sigma} n^{-1/5}\\) bw = ucv or bw = bcv find the bandwidth using cross-validation kernel - the choice of kernel function. The default kernel is the Gaussian kernel. Be careful, some of the non-Gaussian kernels used in the density function are scaled differently than the definitions you might often see in textbooks or on-line resources. n - the number of equally spaced points at which the density is to be estimated. The default is 512 In this section, we will use the galaxies dataset in the MASS package. The first few observations of the galaxies dataset look like: library(MASS) galaxies[1:5] ## [1] 9172 9350 9483 9558 9775 Kernel density estimates can be computed in R using the density function galax.dens &lt;- density(galaxies) You can display a density plot just by applying the plot function to our galax.dens object plot(galax.dens, main=&quot;Default Density Estimate for Galaxy Data&quot;, xlab=&quot;velocity in km/sec&quot;, ylab=&quot;Density&quot;, lwd=2) The density function returns vectors x and y as components x - the vector of points at which the density was estimated y - the value of the estimated density at each of the x points So, just plotting the (x, y) should give you a plot of the density estimate plot(galax.dens$x, galax.dens$y, main=&quot;Default Density Estimate for Galaxy Data&quot;, xlab=&quot;velocity in km/sec&quot;, ylab=&quot;Density&quot;, lwd=2) The default in R is to estimate the density at 512 points. Thus, galax.dens$x and galax.dens$y should each have length 512. The bw component returned by the density function is the bandwidth used to estimate the density. galax.dens$bw ## [1] 1001.839 The default bandwidth selection rule in R is Silverman’s rule-of-thumb \\(h_{n}^{SR} = 0.9 s n^{-1/5}\\). We can check that this is true with the following code: 0.9*min(sd(galaxies), IQR(galaxies)/1.34)/(length(galaxies)^(1/5)) ## [1] 1001.839 The R function approxfun is useful if you want to compute the (approximate) value of the density estimate at a particular point. This approximation will be very close to the true value since the density function returns the value of the density over a dense grid of points. approxfun just linearly interpolates between a set of x and y values. Note that, as a default, approxfun returns NA values if you try to evaluate the function at points which are either less than the minimum x value or greater than maximum x value. For example, suppose we want to know the value of the density estimate at the points \\(18000\\) and \\(33000\\). This could be done with the following code galaxy.fn &lt;- approxfun(galax.dens$x, galax.dens$y) galaxy.fn(18000) ## [1] 4.73857e-05 galaxy.fn(33000) ## [1] 1.004542e-05 Exercise 8.8 Plot the density estimate for the galaxy dataset using the following three rules for finding the bandwidth: (1) bw = nrd, (2) bw=nrd0, (3) bw=ucv. Based on these plots, how many distinct “clusters” or “groups” would you say the galaxy observations fall into. Exercise 8.9 Consider a density estimate of the form \\[\\begin{equation} \\tilde{f}(x) = \\sum_{k=1}^{4} \\pi_{k}\\frac{1}{\\sigma_{k}}K\\Big( \\frac{x - \\mu_{k}}{\\sigma_{k}} \\Big), \\nonumber \\end{equation}\\] where \\(\\sigma_{k} &gt; 0\\) and the \\(\\pi_{k}\\) represent probabilities, that is, \\(\\pi_{k} \\geq 0\\) with \\(\\pi_{1} + \\pi_{2} + \\pi_{3} + \\pi_{4} = 1\\). Can you guess values for \\(\\pi_{k}\\), \\(\\mu_{k}\\), and \\(\\sigma_{k}\\) that fit the galaxy data well? Try finding the best values of \\((\\pi_{k}, \\mu_{k}, \\sigma_{k})\\) by plotting \\(\\tilde{f}(x)\\) next to the kernel density estimate returned by the density function in R. 8.7 Additional Reading Additional reading which covers the material discussed in this chapter includes: Chapters 2-3 from Silverman (2018) Chapter 6 from Wasserman (2006) Chapters 2-3 from Härdle et al. (2012) Chapter 4 from Izenman (2008) Sheather (2004) References "],
["bootstrap-main.html", "Chapter 9 The Bootstrap 9.1 Introduction 9.2 Description of the Bootstrap 9.3 Why is the Bootstrap Procedure Reasonable? 9.4 Pivotal Bootstrap Confidence Intervals 9.5 The Parametric Bootstrap 9.6 Additional Reading 9.7 Exercises", " Chapter 9 The Bootstrap 9.1 Introduction The jackknife and the bootstrap are nonparametric procedures that are mainly used for finding standard errors and constructing confidence intervals. Why use the bootstrap? To find better standard errors and/or confidence intervals when the standard approximations do not work very well. To find standard errors and/or confidence intervals when you have no idea how to compute reasonable standard errors. Example: Inference for \\(e^{\\mu}\\) Suppose we have i.i.d. data \\(X_{1}, \\ldots, X_{n} \\sim \\textrm{Logistic}( \\mu, s)\\), meaning that \\(E(X_{i}) = \\mu\\) and \\(\\textrm{Var}(X_{i}) = \\sigma^{2} = s^{2}\\pi^{2}/3\\). Suppose our goal is to construct a confidence interval for the parameter \\(\\theta = e^{\\mu}\\). The traditional approach to constructing a confidence interval uses the fact that \\[\\begin{equation} \\sqrt{n}\\Big( e^{\\bar{X}} - e^{\\mu} \\Big) \\longrightarrow \\textrm{Normal}(0, \\sigma^{2}e^{2\\mu}) \\nonumber \\end{equation}\\] so that we can assume \\(e^{\\bar{X}}\\) has a roughly Normal distribution with mean \\(e^{\\mu}\\) and standard deviation \\(\\sigma e^{\\mu}/\\sqrt{n}\\). This approximation is based on a Central Limit Theorem and “delta method” argument. The estimated standard error in this case is \\[\\begin{equation} \\frac{\\hat{\\sigma}e^{\\bar{X}}}{\\sqrt{n}} \\nonumber \\end{equation}\\] Using this Normal approximation for \\(e^{\\bar{X}}\\), the \\(95\\%\\) confidence interval for \\(e^{\\mu}\\) is \\[\\begin{equation} \\Big[ e^{\\bar{X}} - 1.96 \\times \\frac{\\hat{\\sigma}e^{\\bar{X}}}{\\sqrt{n}}, e^{\\bar{X}} + 1.96 \\times \\frac{\\hat{\\sigma}e^{\\bar{X}}}{\\sqrt{n}} \\Big] \\tag{9.1} \\end{equation}\\] For specific choices of \\(n\\), how good is the Normal approximation for the distribution of \\(e^{\\bar{X}}\\)? The figure below shows a histogram for the simulated distribution of \\(e^{\\bar{X}}\\) when \\(n=50\\). The density of the Normal approximation is also shown in this figure. Figure 8.1: Histogram of simulated values of exp(sample mean) with density of the Normal approximation overlaid. This assumes n=50 and that the data are from a Logistic distribution with mu = 2 and s = 2. As we can see from the above histogram, the Normal approximation is not terrible. However, it really does not capture the skewness in the distribution of \\(e^{\\bar{X}}\\) correctly. This could effect the coverage performance of confidence intervals which use Normal approximation (9.1). The bootstrap offers an alternative approach for constructing confidence intervals which does not depend on parametric approximations such as (9.1). Example: Inference for the Correlation The sample correlation \\(\\hat{\\rho}\\) which estimates the correlation \\(\\rho = \\textrm{Corr}(X_{i}, Y_{i})\\) between \\(X_{i}\\) and \\(Y_{i}\\) is defined as \\[\\begin{equation} \\hat{\\rho} = \\frac{\\sum_{i=1}^{n}(X_{i} - \\bar{X})(Y_{i} - \\bar{Y})}{\\sqrt{\\sum_{i=1}^{n}(X_{i} - \\bar{X})^{2}}\\sqrt{\\sum_{i=1}^{n}(Y_{i} - \\bar{Y})}} \\nonumber \\end{equation}\\] Even such a relatively straightforward estimate has a pretty complicated formula for the estimated standard error if you use a multivariate delta method argument: \\[\\begin{equation} \\textrm{std.err}_{corr} = \\Bigg\\{ \\frac{\\hat{\\rho}^{2}}{4n}\\Bigg[ \\frac{\\hat{\\mu}_{40}}{\\hat{\\mu}_{20}^{2}} + \\frac{\\hat{\\mu}_{04}}{\\hat{\\mu}_{02}^{2}} + \\frac{2\\hat{\\mu}_{22}}{\\hat{\\mu}_{20}\\hat{\\mu}_{02} } + \\frac{4\\hat{\\mu}_{22}}{\\hat{\\mu}_{11}^{2}} - \\frac{4\\hat{\\mu}_{31}}{\\hat{\\mu}_{11}\\hat{\\mu}_{20} } - \\frac{4\\hat{\\mu}_{13}}{\\hat{\\mu}_{11}\\hat{\\mu}_{02} } \\Bigg] \\Bigg\\}^{1/2} \\tag{9.2} \\end{equation}\\] where \\[\\begin{equation} \\hat{\\mu}_{hk} = \\sum_{i=1}^{n}(X_{i} - \\bar{X})^{h}(Y_{i} - \\bar{Y})^{k} \\nonumber \\end{equation}\\] Another popular approach for constructing a confidence interval is to use Fisher’s “z-transformation” \\[\\begin{equation} z = \\frac{1}{2} \\ln\\Big( \\frac{1 + \\hat{\\rho}}{1 - \\hat{\\rho}} \\Big) \\tag{9.3} \\end{equation}\\] where it is argued that \\(z\\) has a roughly Normal distribution with mean \\(\\tfrac{1}{2}\\ln\\{ (1 + \\rho)/(1 - \\rho) \\}\\) and standard deviation \\(1/\\sqrt{n - 3}\\). The bootstrap allows us to totally bypass the need to derive tedious formulas for the standard error such as (9.2) or bypass the need to use clever transformations such as (9.3). For many more complicated estimates deriving formulas such as (9.2) or transformations such as (9.3) may not even be feasible. The bootstrap provides an automatic way of constructing confidence intervals. You only need to be able to compute the estimate of interest. 9.2 Description of the Bootstrap 9.2.1 Description Suppose we have a statistic \\(T_{n}\\) that is an estimate of some quantity of interest \\(\\theta\\). For example: \\(T_{n}\\) - sample mean, \\(\\theta = E(X_{1})\\). \\(T_{n}\\) - sample correlation, \\(\\theta = \\textrm{Corr}(X_{1}, Y_{1})\\). \\(T_{n}\\) - sample median, \\(\\theta = F^{-1}(1/2)\\); that is, the true median. Typically, \\(T_{n}\\) can be represented as a function of our sample \\(X_{1}, \\ldots, X_{n}\\) \\[\\begin{equation} T_{n} = h\\Big( X_{1}, \\ldots, X_{n} \\Big) \\nonumber \\end{equation}\\] Suppose we want to estimate the standard deviation of \\(T_{n}\\). The true standard deviation of \\(T_{n}\\) is referred to as the standard error. Confidence intervals are often based on subtracting or adding an estimate of the standard error, e.g. \\[\\begin{equation} CI = T_{n} \\pm z_{\\alpha/2} \\times \\widehat{\\textrm{standard error}}, \\nonumber \\end{equation}\\] where \\(z_{\\alpha/2}\\) is the \\(100 \\times (1 - \\alpha/2)\\) percentile of the \\(\\textrm{Normal}(0,1)\\) distribution. The bootstrap estimates the standard deviation of \\(T_{n}\\) by repeatedly subsampling from the original data and computing the value of the statistic \\(T_{n}\\) on each subsample. More generally, we can use the bootstrap not just to find the standard deviation of \\(T_{n}\\) but to characterize the distribution of \\(T_{n}\\). The Bootstrap Procedure In our description of the bootstrap, we will assume that we have the following ingredients: \\(\\mathbf{X} = (X_{1}, \\ldots, X_{n})\\) where \\(X_{1}, \\ldots, X_{n}\\) are i.i.d. observations. The statistic \\(T_{n}\\) of interest. \\(T_{n} = h(X_{1}, \\ldots, X_{n})\\). \\(T_{n}\\) is an estimate of \\(\\theta\\). \\(R\\) - the number of bootstrap replications. The bootstrap works in the following way: For \\(r = 1, \\ldots, R\\): Draw a sample of size \\(n\\): \\((X_{1}^{*}, \\ldots, X_{n}^{*})\\) by sampling with replacement from \\(\\mathbf{X}\\). Compute \\(T_{n,r}^{*} = h(X_{1}^{*}, \\ldots, X_{n}^{*})\\). Each sample is \\((X_{1}^{*}, \\ldots, X_{n}^{*})\\) is drawn through simple random sampling with replacement. That is, \\(X_{1}^{*}, \\ldots, X_{n}^{*}\\) are independent with \\[\\begin{equation} P(X_{i}^{*} = X_{j}) = \\frac{1}{n} \\quad \\textrm{ for } j=1,\\ldots,n \\nonumber \\end{equation}\\] We will refer to each sample \\((X_{1}^{*}, \\ldots, X_{n}^{*})\\) as a bootstrap sample. We will refer to \\(T_{n,r}^{*}\\) as a bootstrap replication. The bootstrap estimate for the standard error of \\(T_{n}\\) is \\[\\begin{equation} se_{boot} = \\Bigg[ \\frac{1}{R-1} \\sum_{r=1}^{R} \\Big( T_{n,r}^{*} - \\frac{1}{R} \\sum_{r=1}^{R} T_{n,r}^{*} \\Big)^{2} \\Bigg]^{1/2} \\nonumber \\end{equation}\\] We can even use our bootstrap replications to get an approximation \\(\\hat{G}_{n}^{*}(t)\\) for the cumulative distribution function \\(G_{n}(t) = P(T_{n} \\leq t)\\) of \\(T_{n}\\): \\[\\begin{equation} \\hat{G}_{n}^{*}(t) = \\frac{1}{R} \\sum_{r=1}^{R} I\\Big( T_{n,r}^{*} \\leq t \\Big) \\nonumber \\end{equation}\\] The normal bootstrap standard error confidence interval is defined as \\[\\begin{equation} \\Big[ T_{n} - z_{\\alpha/2} se_{boot}, T_{n} + z_{\\alpha/2}se_{boot} \\Big] \\nonumber \\end{equation}\\] The bootstrap percentile confidence interval uses the percentiles of the boostrap replications \\(T_{n,1}^{*}, \\ldots, T_{n,R}^{*}\\) to form a confidence interval. The bootstrap \\(100 \\times \\alpha/2\\) and \\(100 \\times (1 - \\alpha/2)\\) percentiles are roughly defined as \\[\\begin{eqnarray} T_{[\\alpha/2]}^{boot} &amp;=&amp; \\textrm{the point } t^{*} \\textrm{ such that } 100\\alpha/2 \\textrm{ of the bootstrap replications are less than } t^{*} \\nonumber \\\\ T_{1 - [\\alpha/2]}^{boot} &amp;=&amp; \\textrm{the point } t^{*} \\textrm{ such that } 100\\alpha/2 \\textrm{ of the bootstrap replications are less than } t^{*} \\nonumber \\end{eqnarray}\\] The level \\(100 \\times (1 - \\alpha) \\%\\) level boostrap percentile confidence interval is then \\[\\begin{equation} \\Big[ T_{[\\alpha/2]}^{boot}, T_{[1 - \\alpha/2]}^{boot} \\Big] \\nonumber \\end{equation}\\] More precisely, the bootstrap percentiles are obtained by looking at the inverse of the estimated cdf of \\(T_{n}\\) \\[\\begin{equation} T_{[\\alpha/2]}^{boot} = \\hat{G}_{n}^{*, -1}(\\alpha/2) \\qquad T_{[1 - \\alpha/2]}^{boot} = \\hat{G}_{n}^{*, -1}(1 - \\alpha/2) \\nonumber \\end{equation}\\] The bootstrap approach for computing estimated standard errors and confidence intervals is very appealing due to the fact that it is automatic. That is, we do not need to expend any effort deriving formulas for the variance of \\(T_{n}\\) and/or making asymptotic arguments for the distribution of \\(T_{n}\\). We only need to be able to compute \\(T_{n}\\) many times, and the bootstrap procedure will automatically produce a confidence interval for us. 9.2.2 Example: Confidence Intervals for the Rate Parameter of an Exponential Distribution Suppose we have i.i.d. data \\(X_{1}, \\ldots, X_{n}\\) from an Exponential distribution with rate parameter \\(1/\\lambda\\). That is, the pdf of \\(X_{i}\\) is \\[\\begin{equation} f(x) = \\begin{cases} \\frac{1}{\\lambda}e^{-x/\\lambda} &amp; \\text{ if } x &gt; 0 \\nonumber \\\\ 0 &amp; \\text{ otherwise } \\end{cases} \\end{equation}\\] This means that \\[\\begin{equation} E( X_{i} ) = \\lambda \\quad \\textrm{ and } \\quad \\textrm{Var}( X_{i} ) = \\lambda^{2} \\nonumber \\end{equation}\\] If using the usual Normal approximation for constructing a confidence interval for \\(\\lambda\\), you would rely on the following asymptotic result: \\[\\begin{equation} \\frac{\\sqrt{n}(\\bar{X} - \\lambda)}{ \\bar{X} } \\longrightarrow \\textrm{Normal}(0, 1) \\nonumber \\end{equation}\\] In other words, for large \\(n\\), \\(\\bar{X}\\) has an approximately Normal distribution with mean \\(\\lambda\\) and standard deviation \\(\\bar{X}/\\sqrt{n}\\). The estimated standard error in this case is \\(\\bar{X}/\\sqrt{n}\\), and a \\(95\\%\\) confidence interval for \\(\\lambda\\) is \\[\\begin{equation} \\Bigg[ \\bar{X} - 1.96 \\times \\frac{\\bar{X}}{\\sqrt{n}}, \\bar{X} + 1.96 \\times \\frac{\\bar{X}}{\\sqrt{n}} \\Bigg] \\nonumber \\end{equation}\\] Let’s do a small simulation to see how the Normal approximation confidence interval compares with bootstrap-based confidence intervals. We will compare the Normal-approximation confidence interval with both the normal standard error bootstrap confidence interval and the percentile bootstrap confidence interval. xx &lt;- rexp(50, rate=2) ## data, sample of 50 exponential r.v.s with mean 1/2 R &lt;- 500 ## number of bootstrap replications boot.mean &lt;- rep(0, R) for(r in 1:R) { boot.samp &lt;- sample(1:50, size=50, replace=TRUE) xx.boot &lt;- xx[boot.samp] ## this is the bootstrap sample boot.mean[r] &lt;- mean(xx.boot) ## this is the rth bootstrap replication } par.ci &lt;- c(mean(xx) - 1.96*mean(xx)/sqrt(50), mean(xx) + 1.96*mean(xx)/sqrt(50)) boot.ci.sd &lt;- c(mean(xx) - 1.96*sd(boot.mean), mean(xx) + 1.96*sd(boot.mean)) boot.ci.quant &lt;- quantile(boot.mean, probs=c(.025, .975)) The normal-approximation confidence interval is round(par.ci, 2) ## [1] 0.31 0.54 The standard error boostrap confidence interval is round(boot.ci.sd, 2) ## [1] 0.32 0.52 The percentile bootstrap confidence interval round(boot.ci.quant, 2) ## 2.5% 97.5% ## 0.32 0.52 9.2.3 Example: Confidence Intervals for the Ratio of Two Quantiles Suppose we have data from two groups \\[\\begin{eqnarray} &amp;&amp; \\textrm{Group 1: } X_{1}, \\ldots, X_{n} \\sim F_{X} \\nonumber \\\\ &amp;&amp; \\textrm{Group 2: } Y_{1}, \\ldots, Y_{n} \\sim F_{Y} \\nonumber \\end{eqnarray}\\] The pth quantile for group 1 is defined as \\(\\theta_{p1} = F_{X}^{-1}(p)\\). In other words, if \\(F_{X}\\) is continuous then \\[\\begin{equation} P(X_{i} \\leq \\theta_{p1}) = F_{X}(F_{X}^{-1}(p)) = p \\nonumber \\end{equation}\\] Likewise, the pth quantile for group 2 is defined as \\(\\theta_{p2} = F_{Y}^{-1}(p)\\) Suppose we are interested in estimating and constructing a confidence for the following parameter \\[\\begin{equation} \\eta = \\frac{ \\theta_{p1}}{ \\theta_{p2} } \\nonumber \\end{equation}\\] We will let \\(\\hat{\\theta}_{p1}\\) denote the pth sample quantile from \\((X_{1}, \\ldots, X_{n})\\) and let \\(\\hat{\\theta}_{p2}\\) denote the pth sample quantile from \\((Y_{1}, \\ldots, Y_{n})\\). We will estimate \\(\\eta\\) with the ratio of the sample quantiles \\[\\begin{equation} \\hat{\\eta} = \\frac{ \\hat{\\theta}_{p1} }{ \\hat{\\theta}_{p2} } \\nonumber \\end{equation}\\] It can be shown that \\[\\begin{equation} \\hat{\\theta}_{p1} \\textrm{ has an approximate } \\textrm{Normal}\\Bigg( \\theta_{p1}, \\frac{p(1-p)}{n f_{X}^{2}(\\theta_{p1})} \\Bigg) \\textrm{ distribution}, \\nonumber \\end{equation}\\] where \\(f_{X}(t) = F_{X}&#39;(t)\\) is the probability density function of \\(X_{i}\\). Using a multivariate delta method argument, you can show that \\[\\begin{equation} \\hat{\\eta} \\textrm{ has an approximate } \\textrm{Normal}\\Bigg( \\eta, \\frac{p(1-p)}{n f_{X}^{2}(\\theta_{p1})\\theta_{p2}^{2} } + \\frac{p(1-p)\\theta_{p1}^{2} }{n f_{Y}^{2}(\\theta_{p2})\\theta_{p2}^{4} } \\Bigg) \\textrm{ distribution} \\tag{9.4} \\end{equation}\\] Using the above large-sample approximation, the estimated standard error that can be used to construct a confidence interval for \\(\\eta\\) is \\[\\begin{equation} \\sqrt{\\frac{p(1-p)}{n \\hat{f}_{X}^{2}(\\hat{\\theta}_{p1})\\hat{\\theta}_{p2}^{2} } + \\frac{p(1-p)\\hat{\\theta}_{p1}^{2} }{n \\hat{f}_{Y}^{2}(\\hat{\\theta}_{p2})\\hat{\\theta}_{p2}^{4} } } \\nonumber \\end{equation}\\] Let’s do a small simulation study to see how the confidence interval based on the large-sample approximation (9.4) compares with bootstrap-based confidence intervals. We will simulate \\(X_{i} \\sim \\textrm{Gamma}(2, 1.5)\\) and \\(Y_{i} \\sim \\textrm{Gamma}(2, 2)\\) with \\(n = 100\\) and \\(m = 100\\). n &lt;- 100 m &lt;- 100 xx &lt;- rgamma(n, shape=2, rate=1.5) yy &lt;- rgamma(m, shape=2, rate=2) We will focus on estimating the pth quantile ratio for \\(p = 0.9\\). In this case, the true value of \\(\\eta\\) is \\(\\eta \\approx 4/3\\). The estimate \\(\\hat{\\eta}\\) and the estimated standard error using the large-sample approximation (9.4) is theta.hat1 &lt;- quantile(xx, probs=0.9) theta.hat2 &lt;- quantile(yy, probs=0.9) eta.hat &lt;- theta.hat1/theta.hat2 ## estimate of quantile ratio xdensity &lt;- density(xx) ydensity &lt;- density(yy) fx &lt;- approxfun(xdensity$x, xdensity$y)(theta.hat1) fy &lt;- approxfun(ydensity$x, ydensity$y)(theta.hat2) q1.se.sq &lt;- (.9*.1)/(n*(fx*theta.hat2)^2) q2.se.sq &lt;- (.9*.1*theta.hat1*theta.hat1)/(n*fy*fy*((theta.hat2)^4)) std.err &lt;- sqrt(q1.se.sq + q2.se.sq) The confidence interval using the large-sample approximation (9.4) is CI &lt;- c(eta.hat - 1.96*std.err, eta.hat + 1.96*std.err) round(CI, 2) ## 90% 90% ## 0.94 1.57 Now, using the same simulated data, let’s compute \\(500\\) bootstrap replications of the statistic \\(\\hat{\\eta}\\) R &lt;- 500 eta.boot &lt;- numeric(R) for(r in 1:R) { boot.xx &lt;- sample(xx, size=n, replace = TRUE) boot.yy &lt;- sample(yy, size=m, replace = TRUE) thetahat.p1 &lt;- quantile(boot.xx, probs=0.9) thetahat.p2 &lt;- quantile(boot.yy, probs=0.9) eta.boot[r] &lt;- thetahat.p1/thetahat.p2 } Because this is a two-sample setting, we draw bootstrap samples \\((X_{1}^{*}, \\ldots, X_{n}^{*})\\) and \\((Y_{1}^{*}, \\ldots, Y_{m}^{*})\\) for each group separately to generate each bootstrap replications. The standard error boostrap confidence interval is boot.ci.sd &lt;- c(eta.hat - 1.96*sd(eta.boot), eta.hat + 1.96*sd(eta.boot)) round(boot.ci.sd, 2) ## [1] 0.75 1.76 The percentile bootstrap confidence interval is boot.ci.quant &lt;- quantile(eta.boot, probs=c(.025, .975)) round(boot.ci.quant, 2) ## 2.5% 97.5% ## 0.93 1.94 A histogram of the bootstrap replications of \\(\\hat{\\eta}\\) is shown in the below figure. Note that the true value of \\(\\eta\\) is \\(\\eta = 4/3\\). Figure 9.1: Bootstrap Distribution of the 0.9-Quantile Ratio. Vertical Lines are the Upper and Lower Bounds from the Percentile Bootstrap Confidence Interval. 9.2.3.1 Comparing the Performance of the Bootstrap and Large-Sample Confidence Intervals We just saw that the bootstrap and the large-sample confidence intervals gave different answers. For this problem, what is the best approach for constructing confidence intervals? We can compare the performance of different confidence intervals by looking at their coverage probability. For a vector of data \\(\\mathbf{X} = (X_{1}, \\ldots, X_{n})\\), we can represent a confidence interval for a parameter of interest \\(\\theta\\) as \\[\\begin{equation} \\Big[ L_{\\alpha}(\\mathbf{X}), U_{\\alpha}(\\mathbf{X}) \\Big] \\end{equation}\\] \\(L_{\\alpha}(\\mathbf{X})\\) is the lower confidence bound. \\(U_{\\alpha}( \\mathbf{X} )\\) is the upper confidence bound. The coverage probability of a confidence interval \\([ L_{\\alpha}(\\mathbf{X}), U_{\\alpha}(\\mathbf{X})]\\) is \\[\\begin{equation} P\\Big( L_{\\alpha}(\\mathbf{X}) \\leq \\theta \\leq U_{\\alpha}(\\mathbf{X}) \\Big), \\nonumber \\end{equation}\\] where we usually construct \\(L_{\\alpha}(\\mathbf{X})\\) and \\(U_{\\alpha}(\\mathbf{X})\\) so that the coverage probability is exactly equal or close to \\(1 - \\alpha\\). We can estimate this probability via simulation by looking at the following coverage proportion \\[\\begin{equation} \\textrm{CoverProp}_{n_{rep}}(\\theta) = \\frac{1}{n_{rep}}\\sum_{k=1}^{n_{rep}} I\\Big( L_{\\alpha}(\\mathbf{X}^{(k)}) \\leq \\theta \\leq U_{\\alpha}(\\mathbf{X}^{(k)}) \\Big) \\nonumber \\end{equation}\\] \\(n_{rep}\\) is the number of simulation replications \\(X^{(k)}\\) is the dataset from the \\(k^{th}\\) simulation replication Each dataset \\(X^{(k)}\\) is generated under the assumption that \\(\\theta\\) is the true value of the parameter of interest. We will compare coverage proportions using the same simulation design we used before for this quantile ratio example. That is, \\(p = 0.9\\) and \\(X_{i} \\sim \\textrm{Gamma}(2, 1.5)\\) and \\(Y_{i} \\sim \\textrm{Gamma}(2, 2)\\) with \\(n = 100\\) and \\(m = 100\\). Below shows code for a simulation study which uses 1000 simulation replications. It compares the large-sample confidence interval which uses (9.4) with two bootstrap confidence intervals. n &lt;- 100 m &lt;- 100 R &lt;- 500 eta.true &lt;- 4/3 nreps &lt;- 1000 Cover.par.ci &lt;- numeric(nreps) Cover.bootsd.ci &lt;- numeric(nreps) Cover.bootquant.ci &lt;- numeric(nreps) for(k in 1:nreps) { ## Step 1: Generate the Data from Two Groups xx &lt;- rgamma(n, shape=2, rate=1.5) yy &lt;- rgamma(m, shape=2, rate=2) ## Step 2: Estimate eta from this data theta.hat1 &lt;- quantile(xx, probs=0.9) theta.hat2 &lt;- quantile(yy, probs=0.9) eta.hat &lt;- theta.hat1/theta.hat2 ## Step 3: Find confidence interval using large-sample Normal approximation xdensity &lt;- density(xx) ydensity &lt;- density(yy) fx &lt;- approxfun(xdensity$x, xdensity$y)(theta.hat1) fy &lt;- approxfun(ydensity$x, ydensity$y)(theta.hat2) q1.se.sq &lt;- (.9*.1)/(n*(fx*theta.hat2)^2) q2.se.sq &lt;- (.9*.1*theta.hat1*theta.hat1)/(n*fy*fy*((theta.hat2)^4)) std.err &lt;- sqrt(q1.se.sq + q2.se.sq) par.ci &lt;- c(eta.hat - 1.96*std.err, eta.hat + 1.96*std.err) ## Step 4: Find bootstrap confidence intervals using R bootstrap replications eta.boot &lt;- numeric(R) for(r in 1:R) { boot.xx &lt;- sample(xx, size=n, replace = TRUE) boot.yy &lt;- sample(yy, size=m, replace = TRUE) thetahat.p1 &lt;- quantile(boot.xx, probs=0.9) thetahat.p2 &lt;- quantile(boot.yy, probs=0.9) eta.boot[r] &lt;- thetahat.p1/thetahat.p2 } boot.ci.sd &lt;- c(eta.hat - 1.96*sd(eta.boot), eta.hat + 1.96*sd(eta.boot)) boot.ci.quant &lt;- quantile(eta.boot, probs=c(.025, .975)) ## Step 5: Record if the true parameter is covered or not: Cover.par.ci[k] &lt;- ifelse(par.ci[1] &lt; eta.true &amp; par.ci[2] &gt;= eta.true, 1, 0) Cover.bootsd.ci[k] &lt;- ifelse(boot.ci.sd[1] &lt; eta.true &amp; boot.ci.sd[2] &gt;= eta.true, 1, 0) Cover.bootquant.ci[k] &lt;- ifelse(boot.ci.quant[1] &lt; eta.true &amp; boot.ci.quant[2] &gt;= eta.true, 1, 0) } The coverage proportions for each of the methods are: mean(Cover.par.ci) ## [1] 0.921 mean(Cover.bootsd.ci) ## [1] 0.949 mean(Cover.bootquant.ci) ## [1] 0.959 Using these simulated outcomes, we can also construct \\(95\\%\\) confidence intervals for the coverage probabilities: Estimates and Confidence Intervals for the Coverage Probabilities of Different Methods. Lower CI Estimate Upper CI Large-Sample Approximation 0.904 0.921 0.938 Bootstrap Std. Err. 0.935 0.949 0.963 Bootstrap Percentile 0.947 0.959 0.971 9.3 Why is the Bootstrap Procedure Reasonable? As mentioned before, our original motivation for the bootstrap was to find an estimate of \\(\\textrm{Var}(T_{n})\\) where \\(T_{n}\\) is a statistic that can be thought of as an estimate of \\(\\theta\\). The statistic \\(T_{n}\\) can be thought of as a function of our sample \\[\\begin{equation} T_{n} = h(X_{1}, \\ldots, X_{n}) \\nonumber \\end{equation}\\] where \\(X_{1}, \\ldots, X_{n}\\) is an i.i.d. sample with cumulative distribution function \\(F\\). If we had a way of simulating data \\(X_{i}^{(k)}\\) from \\(F\\), we could estimate \\(\\textrm{Var}(T_{n})\\) with the following quantity \\[\\begin{eqnarray} \\widehat{\\textrm{Var}( T_{n} )} &amp;=&amp; \\frac{1}{K-1}\\sum_{k=1}^{K}\\Big( h(X_{1}^{(k)}, \\ldots, X_{n}^{(k)}) - \\frac{1}{K}\\sum_{k=1}^{K} h(X_{1}^{(k)}, \\ldots, X_{n}^{(k)}) \\Big)^{2} \\nonumber \\\\ &amp;=&amp; \\frac{1}{K-1}\\sum_{k=1}^{K}\\Big( T_{n}^{(k)} - \\frac{1}{K}\\sum_{k=1}^{K} T_{n}^{(k)} \\Big)^{2} \\tag{9.5} \\end{eqnarray}\\] \\(X_{1}^{(k)}, \\ldots, X_{n}^{(k)}\\) is an i.i.d. sample from \\(F\\). \\(T_{n}^{(k)} = h(X_{1}^{(k)}, \\ldots, X_{n}^{(k)})\\) is the value of out statistic of interest from the \\(k^{th}\\) simulated dataset. In practice, \\(F\\) is unknown, and we cannot simulate data from \\(F\\). The main idea behind the bootstrap is that the empirical distribution function \\(\\hat{F}_{n}\\) is a very good estimate of \\(F\\). Hence, if we sample \\(X_{1}^{(k)}, \\ldots, X_{n}^{(k)}\\) from \\(\\hat{F}_{n}\\) instead of \\(F\\) and use the formula (9.5), this should give us a good estimate of the variance of \\(T_{n}\\). How can we simulate data from \\(\\hat{F}_{n}\\)? Recall that \\(\\hat{F}_{n}\\) is a discrete distribution that has mass \\(1/n\\) at each of the observed data points \\(\\mathbf{X} = (X_{1}, \\ldots, X_{n})\\). So, if we say \\(X_{i}^{*} \\sim \\hat{F}_{n}\\), then \\[\\begin{equation} P( X_{i}^{*} = X_{j}) = 1/n \\qquad \\textrm{ for } j=1,\\ldots,n, \\end{equation}\\] where \\((X_{1}, \\ldots, X_{n})\\) can be thought of as fixed numbers. To simulate a random variable \\(X_{i}^{*} \\sim \\hat{F}_{n}\\), we just need to draw one of the observations \\(X_{j}\\) from \\(\\mathbf{X}\\) at random and set \\(X_{i}^{*} = X_{j}\\). Then, to simulate an i.i.d. sample \\(X_{1}^{*}, \\ldots, X_{n}^{*}\\) from \\(\\hat{F}_{n}\\), we just to sample the \\(X_{i}^{*}\\) from \\(\\mathbf{X}\\) with replacement. In other words, we just need to use the same procedure we discussed earlier for generating bootstrap samples. Each bootstrap sample \\((X_{1}^{*}, \\ldots, X_{n}^{*})\\) can be thought of as an i.i.d. sample from \\(\\hat{F}_{n}\\). The variance of \\(T_{n}\\) can be written as \\[\\begin{equation} V_{T_{n}}(F) = \\int \\cdots \\int h^{2}(x_{1}, \\ldots, x_{n}) dF(x_{1})\\cdots dF(x_{n}) - E_{T_{n}}^{2}(F), \\nonumber \\end{equation}\\] where \\[\\begin{equation} E_{T_{n}}(F) = \\int \\cdots \\int h(x_{1}, \\ldots, x_{n}) dF(x_{1})\\cdots dF(x_{n}), \\nonumber \\end{equation}\\] What we are trying to compute with the bootstrap is the following variance estimate \\[\\begin{equation} V_{T_{n}}(\\hat{F}_{n}) = \\int \\cdots \\int h^{2}(x_{1}, \\ldots, x_{n}) d\\hat{F}_{n}(x_{1})\\cdots d\\hat{F}_{n}(x_{n}) - E_{T_{n}}^{2}(\\hat{F}_{n}) \\tag{9.6} \\end{equation}\\] It is too difficult to compute (9.6) in most cases. Instead, with the bootstrap, we are approximating (9.6) via simulation by drawing many i.i.d. samples from \\(\\hat{F}_{n}\\). You can think of the bootstrap as using Monte Carlo integration to approximate (9.6). There are cases where the bootstrap does not really work well. The main requirement for the bootstrap to work is that the functional \\(E_{T_{n}}(F)\\) is “smooth” as \\(F\\) varies. That is, if \\(F_{1}\\) and \\(F_{2}\\) are “close”, then \\(E_{T_{n}}(F_{1})\\) and \\(E_{T_{n}}(F_{2})\\) should also be close. More specifically, if the functional \\(E_{T_{n}}(F)\\) is differentiable in an appropriate sense, then confidence intervals from the bootstrap estimate of the variance of \\(T_{n}\\) will be “valid” in an asymptotic sense (see, for example, Chapter 5 of Shao (2003) for a somewhat more rigorous discussion of this). 9.4 Pivotal Bootstrap Confidence Intervals Confidence intervals are often based on what is referred to as a pivot. The quantity \\(W_{n}( \\mathbf{X}, \\theta)\\) is a pivot if the distribution of \\(W_{n}(\\mathbf{X}, \\theta)\\) does not depend on \\(\\theta\\). A common example of this is for the \\(\\textrm{Normal}(\\theta, \\sigma^{2})\\) distribution. In this case, \\[\\begin{equation} W_{n}(\\mathbf{X}, \\theta) = \\bar{X} - \\theta \\end{equation}\\] is a pivot. The distribution of \\(W_{n}(\\mathbf{X}, \\theta)\\) is \\(\\textrm{Normal}(0, \\sigma^{2})\\). Using the pivot allows us to construct a confidence interval because \\[\\begin{eqnarray} 1 - \\alpha &amp;=&amp; P\\Big( -\\sigma z_{1 - \\alpha/2} \\leq W_{n}(\\mathbf{X}, \\theta) \\leq \\sigma z_{1 - \\alpha/2} \\Big) \\nonumber \\\\ &amp;=&amp; P\\Big(-\\sigma z_{1 - \\alpha/2} \\leq \\bar{X} - \\theta \\leq \\sigma z_{1 - \\alpha/2} \\Big) \\nonumber \\\\ &amp;=&amp; P\\Big(\\bar{X} - \\sigma z_{1 - \\alpha/2} \\leq \\theta \\leq \\bar{X} + \\sigma z_{1 - \\alpha/2} \\Big) \\nonumber \\end{eqnarray}\\] Another common pivot (if we did not assume \\(\\sigma^{2}\\) was known) would be \\[\\begin{equation} W_{n}(\\mathbf{X}, \\theta) = \\frac{\\sqrt{n}(\\bar{X} - \\theta)}{\\hat{\\sigma}}, \\nonumber \\end{equation}\\] which would have a \\(t\\) distribution. We can use a similar idea to construct a bootstrap confidence interval for \\(\\theta = E( T_{n} )\\). Assume that \\(W_{n}(\\mathbf{X}, \\theta) = T_{n} - \\theta\\) is a pivot and suppose that \\(H(t)\\) is the cdf of this pivot Then, if we choose \\(b &gt; a\\) such that \\(H(b) - H(a) = 1 - \\alpha\\), \\[\\begin{eqnarray} 1 - \\alpha &amp;=&amp; P\\Big( a \\leq T_{n} - \\theta \\leq b) = P\\Big( -b \\leq \\theta - T_{n} \\leq -a \\Big) \\nonumber \\\\ &amp;=&amp; P\\Big(T_{n} -b \\leq \\theta \\leq T_{n} -a \\Big) \\nonumber \\end{eqnarray}\\] For example, \\(b = H^{-1}(1 - \\alpha/2)\\) and \\(a = H^{-1}(\\alpha/2)\\) would work. This would suggest that \\([T_{n} - b, T_{n} - a]\\) should be a good confidence interval for \\(\\theta\\). The only problem is that \\(H(t)\\) is not known. So, how do we find \\(a\\) and \\(b\\) if we don’t assume normality of \\(T_{n}\\) and a known \\(\\sigma^{2}\\)? Look at the distribution of \\(T_{n,r}^{*} - T_{n}\\) as a substitute for \\(T_{n} - \\theta\\) and use the empirical distribution function of \\(T_{n,r}^{*} - T_{n}\\) to estimate \\(H(t)\\) \\[\\begin{equation} \\hat{H}_{R}(t) = \\frac{1}{R}\\sum_{r=1}^{R} I\\Big(T_{n,r}^{*} - T_{n} \\leq t \\Big) \\end{equation}\\] Using this approximation for \\(H(t)\\), we could use the following confidence interval \\[\\begin{equation} \\Big[ T_{n} - \\hat{H}_{R}^{-1}(1 - \\alpha/2), T_{n} - \\hat{H}_{R}^{-1}(\\alpha/2) \\Big] \\nonumber \\end{equation}\\] Studentized Bootstrap Confidence Intervals Now, suppose we instead use the pivot \\[\\begin{equation} \\mathbf{Z}_{n}(\\mathbf{X}, \\theta) = \\frac{T_{n} - \\theta}{se_{boot}}, \\nonumber \\end{equation}\\] where \\(se_{boot}\\) denotes the bootstrap estimate of standard error. We will let \\(K(t)\\) denote the cdf of \\(\\mathbf{Z}_{n}( \\mathbf{X}, \\theta)\\). Using the same reasoning as before, \\[\\begin{eqnarray} 1 - \\alpha &amp;=&amp; P\\Big( K^{-1}(\\alpha/2) \\leq \\frac{T_{n} - \\theta}{se_{boot}} \\leq K^{-1}(1 - \\alpha/2) \\Big) \\nonumber \\\\ &amp;=&amp; P\\Big( - se_{boot} \\times K^{-1}(1 - \\alpha/2) \\leq \\theta - T_{n} \\leq -se_{boot} \\times K^{-1}(\\alpha/2) \\Big) \\nonumber \\\\ &amp;=&amp; P\\Big(T_{n} - se_{boot} \\times K^{-1}(1 - \\alpha/2) \\leq \\theta \\leq T_{n} - se_{boot} \\times K^{-1}(\\alpha/2) \\Big) \\nonumber \\end{eqnarray}\\] and hence a confidence interval for \\(\\theta\\) would be \\[\\begin{equation} \\Big[ T_{n} - se_{boot} \\times K^{-1}(1 - \\alpha/2), T_{n} - se_{boot} \\times K^{-1}(\\alpha/2) \\Big] \\nonumber \\end{equation}\\] To estimate \\(K(t)\\), we are going to use \\(Z_{n,r}^{*} = (T_{n,r}^{*} - T_{n})/\\hat{se}_{r}\\) as a substitute for \\((T_{n} - \\theta)/se_{boot}\\). The estimate \\(\\hat{se}_{r}\\) is an estimate of the standard error of \\(T_{n,r}^{*}\\). This could be estimated via \\[\\begin{equation} \\hat{se}_{r} = \\Bigg[ \\frac{1}{J-1} \\sum_{j=1}^{J} \\Big( T_{n,r,j}^{**} - \\frac{1}{J} \\sum_{j=1}^{J} T_{n,r,j}^{**} \\Big)^{2} \\Bigg]^{1/2}, \\nonumber \\end{equation}\\] where \\(T_{n,r,j}^{**}\\) is the value of our test statistic computed from the \\(j^{th}\\) bootstrap sample of the bootstrap sample that was used to produce \\(T_{n,r}^{*}\\). So, to find \\(\\hat{se}_{r}\\), we need \\(J\\) bootstrap samples within each of the \\(R\\) bootstrap samples that were used to generated \\(T_{n,1}^{*}, \\ldots, T_{n,R}^{*}\\). For this reason, this is often referred to as the double bootstrap. Then, the estimate of \\(K(t)\\) is defined as \\[\\begin{equation} \\hat{K}_{R}(t) = \\frac{1}{R} \\sum_{r=1}^{R} I\\Big( Z_{n,r}^{*} \\leq t \\Big) \\nonumber \\end{equation}\\] The studentized bootstrap confidence interval (or the bootstrap-t confidence interval) is then defined as \\[\\begin{equation} \\Big[ T_{n} - se_{boot} \\times \\hat{K}_{R}^{-1}(1 - \\alpha/2), T_{n} - se_{boot} \\times \\hat{K}_{R}^{-1}(\\alpha/2) \\Big] \\nonumber \\end{equation}\\] 9.5 The Parametric Bootstrap With the bootstrap, we generate each bootstrap sample \\((X_{1}^{*}, \\ldots, X_{n}^{*})\\) by sampling with replacement from the empirical distribution function \\(\\hat{F}_{n}\\). For this reason, it can be referred to as the nonparametric bootstrap. With the parametric bootstrap, we sample from a parametric estimate \\(F_{\\hat{\\varphi}}\\) of the cumulative distribution function instead of sampling from \\(\\hat{F}_{n}\\). For example, suppose we have data \\((X_{1}, \\ldots, X_{n})\\) that we assume are normally distributed and we estimate \\(\\mu\\) and \\(\\sigma^{2}\\) with \\[\\begin{equation} \\hat{\\mu} = \\bar{X} \\qquad \\qquad \\hat{\\sigma}^{2} = \\frac{1}{n-1}\\sum_{i=1}^{n} (X_{i} - \\bar{X})^{2} \\end{equation}\\] If we are interested in getting a confidence interval for \\(\\theta\\) where \\(T_{n} = h(X_{1}, \\ldots, X_{n})\\) is an estimate of \\(\\theta\\), the parametric bootstrap would use the following procedure: For \\(r = 1, \\ldots, R\\): Draw an i.i.d. sample: \\(X_{1}^{*}, \\ldots, X_{n}^{*} \\sim \\textrm{Normal}(\\hat{\\mu}, \\hat{\\sigma}^{2})\\). Compute \\(T_{n,r}^{*} = h(X_{1}^{*}, \\ldots, X_{n}^{*})\\). Then, form a confidence interval for \\(\\theta\\) using the parametric bootstrap replications \\(T_{n,1}^{*}, \\ldots, T_{n,R}^{*}\\) When Might the Parametric Bootstrap be Useful? In cases with smaller sample sizes. For small sample sizes, \\(F_{\\hat{\\varphi}}\\) is often a better estimate than \\(\\hat{F}_{n}\\). When you are only interested in constructing a confidence interval for the parameter of a parametric model that you think fits the data well. For non-i.i.d. data or other complicated distributions, a parametric bootstrap can sometimes be easier to work with. For non-i.i.d. data, where our observations \\((X_{1}, \\ldots, X_{n})\\) our dependent, a parametric bootstrap can often be straightforward to implement. Suppose \\(G_{\\varphi}\\) is a parametric model that describes the joint distribution of \\((X_{1}, \\ldots, X_{n})\\) and that it is easy to simulate observations from \\(G_{\\varphi}\\). Then, if you have an estimate \\(\\hat{\\varphi}\\) of \\(\\varphi\\), you can use the following procedure to generate bootstrap replications for your statistic of interest. For \\(r = 1, \\ldots, R\\): Draw a sample \\((X_{1}^{*}, \\ldots, X_{n}^{*}) \\sim G_{\\hat{\\varphi}}\\). Compute \\(T_{n,r}^{*} = h(X_{1}^{*}, \\ldots, X_{n}^{*})\\). 9.5.1 Parametric Bootstrap for the Median Age from the Kidney Data Let us consider the ages from the kidney data. One somewhat reasonable model for the ages \\(X_{i}\\) from the kidney data is that \\[\\begin{equation} X_{i} - 17 \\sim \\textrm{Gamma}(\\alpha, \\beta) \\nonumber \\end{equation}\\] We can find the maximum likelihood estimates \\((\\hat{\\alpha}, \\hat{\\beta})\\) for this model using the following R code kidney &lt;- read.table(&quot;https://web.stanford.edu/~hastie/CASI_files/DATA/kidney.txt&quot;, header=TRUE) LogProfLik &lt;- function(alpha, x) { ans &lt;- alpha*log(alpha/mean(x)) - lgamma(alpha) + (alpha - 1)*mean(log(x)) - alpha return(ans) } best.alpha &lt;- optimize(LogProfLik, interval=c(0,10), x=kidney$age - 17, maximum=TRUE)$maximum best.beta &lt;- best.alpha/mean(kidney$age - 17) We can plot this estimated Gamma density overlaid on the histogram of the ages to see how they compare tt &lt;- seq(17, 90, length.out=500) hist(kidney$age, breaks=&quot;FD&quot;, probability=TRUE, las=1, xlab=&quot;Age&quot;, main=&quot;Estimate Gamma Density for the Kidney Age Data&quot;, col=&quot;grey&quot;) lines(tt, dgamma(tt - 17, shape=best.alpha, rate=best.beta), lwd=2) Suppose we are interested in constructing a confidence interval for the median age. To use the parametric boostrap with the parametric model \\(X_{i} - 17 \\sim \\textrm{Gamma}(\\alpha, \\beta)\\) and using our estimates \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) computed above, we would use the following steps. For \\(r = 1, \\ldots, R\\): Draw an i.i.d. sample: \\(X_{1}^{*}, \\ldots, X_{n}^{*} \\sim 17 + \\textrm{Gamma}(\\hat{\\alpha}, \\hat{\\beta})\\). Compute \\(T_{n,r}^{*} = \\textrm{median}(X_{1}^{*}, \\ldots, X_{n}^{*})\\). The code for implementing this parametric bootstrap is given below R &lt;- 500 med.boot.par &lt;- rep(0, R) med.boot.np &lt;- rep(0, R) for(r in 1:R) { xx.boot.par &lt;- 17 + rgamma(157, shape=best.alpha, rate=best.beta) xx.boot.np &lt;- sample(kidney$age, size=157, replace=TRUE) med.boot.par[r] &lt;- median(xx.boot.par) ## rth par. bootstrap replication med.boot.np[r] &lt;- median(xx.boot.np) ## rth par. bootstrap replication } The normal standard error confidence interval using the parametric bootstrap is c(median(kidney$age) - 1.96*sd(med.boot.par), median(kidney$age) + 1.96*sd(med.boot.par)) ## [1] 28.39858 33.60142 The normal standard error confidence interval using the nonparametric bootstrap is c(median(kidney$age) - 1.96*sd(med.boot.np), median(kidney$age) + 1.96*sd(med.boot.np)) ## [1] 28.64083 33.35917 9.6 Additional Reading Additional reading which covers the material discussed in this chapter includes: Chapter 3 from Wasserman (2006) Chapter 2 from Davison and Hinkley (1997) 9.7 Exercises Exercise 9.1 With the regular bootstrap we generate bootstrap samples by sampling from the empirical distribution function \\(\\hat{F}_{n}\\). An alternative approach is to sample from a smooth estimate of \\(F\\) instead of the non-smooth estimate \\(\\hat{F}_{n}\\). Consider the following smooth estimate of \\(F\\) which is just the indefinite integral of a kernel density estimate \\[\\begin{equation} \\hat{F}_{h}^{KD}(t) = \\frac{1}{n} \\sum_{i=1}^{n} \\int_{-\\infty}^{t} \\frac{1}{h} K\\Big( \\frac{x - X_{i}}{h} \\Big) dx \\nonumber \\end{equation}\\] What is \\(\\hat{F}_{h}^{KD}\\) when \\(K( \\cdot )\\) is the Gaussian kernel? For the case of a Gaussian kernel, how do you generate an i.i.d. sample \\((X_{1}^{*}, \\ldots, X_{n}^{*})\\) from \\(\\hat{F}_{h}^{KD}\\)? Using the dataset from the package, compute a \\(95\\%\\) ``smooth bootstrap&quot; confidence interval for the standard deviation \\(\\sigma_{v}\\) of the velocities by using the following steps: For \\(r = 1,\\ldots, R\\): Draw an i.i.d. sample: \\(X_{1}^{*}, \\ldots, X_{n}^{*} \\sim \\hat{F}_{h}^{KD}\\). Compute \\(T_{n,r}^{*} = \\textrm{sd}( X_{1}^{*}, \\ldots, X_{n}^{*})\\). Using \\(T_{n,1}^{*}, \\ldots, T_{n,R}^{*}\\), construct the confidence interval for \\(\\sigma_{v}\\) using the normal bootstrap standard error approach. For the bandwidth \\(h\\) in \\(\\hat{F}_{h}^{KD}\\), you can use Silverman’s rule-of-thumb: \\(h = 0.9 n^{-1/5}\\min\\{ \\hat{\\sigma}, IQR/1.34 \\}\\). Using the usual bootstrap where we sample from \\(\\hat{F}_{n}\\), construct three \\(95\\%\\) bootstrap confidence intervals for \\(\\sigma_{v}\\) using the following methods Normal bootstrap standard error confidence interval. A pivotal bootstrap confidence interval based on \\(T_{n,r}^{*} - T_{n}\\) (not the studentized bootstrap confidence interval). Bootstrap percentile confidence interval. References "],
["ci.html", "Chapter 10 Bootstrap Examples and the Jackknife 10.1 The Parametric Bootstrap for an AR(1) model 10.2 Using the Bootstrap in Regression 10.3 Pointwise Confidence Intervals for a Density Function 10.4 When can the Bootstrap Fail? 10.5 The Jackknife", " Chapter 10 Bootstrap Examples and the Jackknife 10.1 The Parametric Bootstrap for an AR(1) model Consider the time series \\(X_{1}, X_{2}, \\ldots, X_{m}\\). Here, \\(X_{t}\\) denotes an observation made at time \\(t\\). An autoregressive model of order 1 (usually called an AR(1) model) for this time series is \\[\\begin{eqnarray} X_{1} &amp;=&amp; \\frac{c_{0}}{1 - \\alpha} + \\varepsilon_{1} \\nonumber \\\\ X_{t} &amp;=&amp; c_{0} + \\alpha X_{t-1} + \\varepsilon_{t}, \\qquad t=2,\\ldots,m. \\nonumber \\end{eqnarray}\\] It is usually assumed that \\(|\\alpha| &lt; 1\\). In the AR(1) model, it is assumed that \\(E(\\varepsilon_{t}) = 0\\) \\(\\textrm{Var}(\\varepsilon_{t}) = \\sigma^{2}\\), \\(\\varepsilon_{2}, \\ldots, \\varepsilon_{m}\\) are i.i.d. \\(\\varepsilon_{t}\\) and \\(X_{t-1}\\) are independent. In addition to these assumptions, we will assume that \\[\\begin{equation} \\varepsilon_{t} \\sim \\textrm{Normal}(0, \\sigma^{2}) \\nonumber \\end{equation}\\] The AR(1) model implies that \\[\\begin{equation} \\textrm{Corr}(X_{t}, X_{t-1}) = \\alpha \\nonumber \\end{equation}\\] and, more generally, that \\[\\begin{equation} \\textrm{Corr}(X_{t}, X_{t-p}) = \\alpha^{p} \\nonumber \\end{equation}\\] For known values of \\(c_{0}, \\alpha\\), and \\(\\sigma^{2}\\), we can simulate an AR(1) time series with the following R code: SimulateParAR1 &lt;- function(m, c0, alpha, sig.sq) { xx &lt;- numeric(m) xx[1] &lt;- c0/(1 - alpha) + rnorm(1, sd=sqrt(sig.sq)) for(t in 2:m) { xx[t] &lt;- c0 + alpha*xx[t-1] + rnorm(1, sd=sqrt(sig.sq)) } return(xx) } In R, estimates of \\(c_{0}, \\alpha,\\) and \\(\\sigma^{2}\\) can be found by using the ar function. For example, x &lt;- SimulateParAR1(1000, 1, 0.8, sig.sq=.25) ar1.fit &lt;- ar(x, aic=FALSE, order.max = 1, method=&quot;mle&quot;) c0.est &lt;- ar1.fit$x.mean*(1 - ar1.fit$ar) alpha.est &lt;- ar1.fit$ar sigsq.est &lt;- ar1.fit$var.pred Suppose we want to construct confidence intervals for \\(\\alpha\\) and \\(\\sigma\\) using a bootstrap method. Using the direct, nonparametric bootstrap described in the previous chapter will not work because our observations are not independent. There are “block bootstraps” that are designed to work for time series, but we will not discuss those here (see e.g., Bühlmann (2002) or Chapter 8 of Davison and Hinkley (1997) for more details). With the parametric bootstrap, we only have to use the following steps to generate bootstrap replications \\(\\hat{\\alpha}_{r}^{*}\\) and \\(\\hat{\\sigma}_{r}^{2,*}\\) for estimates of \\(\\alpha\\) and \\(\\hat{\\sigma}^{2}\\). For \\(r = 1, \\ldots, R\\): Simulate a time series \\(X_{1}^{*}, \\ldots, X_{m}^{*}\\) from an AR(1) model with parameters \\((\\hat{c}_{0}, \\hat{\\alpha}, \\hat{\\sigma}^{2})\\). Compute \\(\\hat{\\alpha}_{r}^{*} = \\hat{\\alpha}(X_{1}^{*}, \\ldots, X_{m}^{*})\\). Compute \\(\\hat{\\sigma}_{r}^{2,*} = \\hat{\\sigma}^{2}(X_{1}^{*}, \\ldots, X_{m}^{*})\\) To see how this parametric bootstrap works, we will use the nhtemp dataset that is available in R. The nhtemp dataset contains the mean annual temperature in New Haven, Connecticut from the years 1912-1971 head(nhtemp) ## [1] 49.9 52.3 49.4 51.1 49.4 47.9 The estimated autocorrelation parameter \\(\\alpha\\) is about \\(0.31\\) for this data ar1.temp &lt;- ar(nhtemp, aic=FALSE, order.max = 1) c0.hat &lt;- ar1.temp$x.mean*(1 - ar1.temp$ar) alpha.hat &lt;- ar1.temp$ar sigsq.hat &lt;- ar1.temp$var.pred alpha.hat ## [1] 0.3148269 Now, that we have estimated all the parameter of the AR(1) model, we can run our parametric bootstrap for \\(\\hat{\\alpha}\\) and \\(\\hat{\\sigma}\\): R &lt;- 500 alpha.boot &lt;- numeric(R) sigsq.boot &lt;- numeric(R) for(r in 1:R) { x &lt;- SimulateParAR1(60, c0=c0.hat, alpha=alpha.hat, sig.sq=sigsq.hat) ar1.fit &lt;- ar(x, aic=FALSE, order.max = 1) alpha.boot[r] &lt;- ar1.fit$ar sigsq.boot[r] &lt;- ar1.fit$var.pred } Normal bootstrap standard error confidence intervals for \\(\\alpha\\) and \\(\\sigma^{2}\\) are round(c(alpha.hat - 1.96*sd(alpha.boot), alpha.hat + 1.96*sd(alpha.boot)), 3) ## [1] 0.068 0.561 round(c(sigsq.hat - 1.96*sd(sigsq.boot), sigsq.hat + 1.96*sd(sigsq.boot)), 3) ## [1] 0.915 2.021 We can compare our confidence interval for \\(\\alpha\\) with the confidence interval obtained from using a large-sample approximation: asymp.se &lt;- sqrt(ar1.temp$asy.var.coef) round(c(alpha.hat - 1.96*asymp.se, alpha.hat + 1.96*asymp.se), 3) ## [1] 0.071 0.559 10.2 Using the Bootstrap in Regression In linear regression with a single, univariate covariate, we work with the following model \\[\\begin{equation} Y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\varepsilon_{i}, \\qquad i = 1, \\ldots, n. \\nonumber \\end{equation}\\] \\(Y_{i}\\) - the responses \\(x_{i}\\) - the covariates \\(\\beta_{0}, \\beta_{1}\\) - the regression coefficients \\(\\varepsilon_{i}\\) - the residuals Typically, confidence intervals for the regression coefficients \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are constructed under the assumption that \\(\\varepsilon_{i} \\sim \\textrm{Normal}(0, \\sigma^{2})\\). The bootstrap allows us to compute confidence intervals for \\((\\beta_{0}, \\beta_{1})\\) without relying on this normality assumption. How to compute bootstrap confidence intervals for \\(\\beta_{0}\\) and \\(\\beta_{1}\\)? The least-squares estimates of \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are \\[\\begin{equation} \\hat{\\beta}_{0} = \\bar{y} - \\hat{\\beta}_{1}\\bar{x} \\qquad \\qquad \\hat{\\beta}_{1} = \\frac{\\sum_{i=1}^{n}(x_{i} - \\bar{x})(y_{i} - \\bar{y})}{S_{xx}} \\nonumber \\end{equation}\\] where \\(S_{xx} = \\sum_{i=1}^{n}( x_{i} - \\bar{x})^{2}\\). Assuming the covariates are fixed design points, the variance of \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) are \\[\\begin{equation} \\textrm{Var}(\\hat{\\beta}_{0}) = \\sigma^{2}\\Big(\\frac{\\tfrac{1}{n}\\sum_{i=1}^{n} x_{i}^{2}}{S_{xx}} \\Big) \\qquad \\textrm{Var}(\\hat{\\beta}_{1}) = \\frac{\\sigma^{2}}{S_{xx}} \\tag{10.1} \\end{equation}\\] 10.2.1 Parametric Bootstrap for Regression With a parametric bootstrap, we will simulate outcomes \\(Y_{i}\\) from the model \\[\\begin{equation} Y_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i} + \\varepsilon_{i}, \\nonumber \\end{equation}\\] \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) are the least-squares estimates of \\(\\beta_{0}\\) and \\(\\beta_{1}\\), \\(\\varepsilon_{1}, \\ldots, \\varepsilon_{n}\\) are i.i.d. random variables with mean zero and variance \\(\\hat{\\sigma}^{2}\\). It is most common to assume that \\(\\varepsilon_{i} \\sim \\textrm{Normal}(0, \\hat{\\sigma}^{2})\\), where \\(\\hat{\\sigma}^{2}\\) is an estimate of the residual variance. However, we could easily use an alternative parametric model for \\(\\varepsilon_{i}\\) if we thought it was appropriate. A t-distribution with a small number of degrees of freedom can be useful when the residuals are thought to follow a distribution with “heavier tails”. If we assume \\(\\varepsilon_{i} \\sim \\sigma \\times t_{3}\\), then \\(\\textrm{Var}(\\varepsilon_{i}) = 3\\sigma^{2}\\). So, with a \\(t_{3}\\) residual distribution we want to simulate from the model \\[\\begin{equation} Y_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i} + \\frac{\\hat{\\sigma}}{\\sqrt{3}}u_{i}, \\qquad u_{i} \\sim t_{3}, \\nonumber \\end{equation}\\] where \\(\\hat{\\sigma}^{2}\\) is the following estimate of the residual variance: \\[\\begin{equation} \\hat{\\sigma}^{2} = \\frac{1}{n-2}\\sum_{i=1}^{n} (Y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1})^{2} \\nonumber \\end{equation}\\] To show how this parametric-t bootstrap works in practice we will look at the kidney function data. We will look at a linear regression where the measure of kidney function is the outcome and age is the covariate. kidney &lt;- read.table(&quot;https://web.stanford.edu/~hastie/CASI_files/DATA/kidney.txt&quot;, header=TRUE) Bootstrap replications of \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) can be computed using the following R code: ## First find the parameter estimates lm.kidney &lt;- lm(tot ~ age, data=kidney) beta0.hat &lt;- lm.kidney$coef[1] beta1.hat &lt;- lm.kidney$coef[2] sigsq.hat &lt;- sum(lm.kidney$residuals^2)/(157 - 2) ## Using these estimates, run a parametric bootstrap to generate ## bootstrap replications of beta0.hat and beta1.hat R &lt;- 500 beta0.boot &lt;- numeric(R) beta1.boot &lt;- numeric(R) se.beta0.boot &lt;- numeric(R) se.beta1.boot &lt;- numeric(R) for(r in 1:R) { ysim &lt;- beta0.hat + beta1.hat*kidney$age + sqrt(sigsq.hat/3)*rt(157, df=3) lm.boot &lt;- lm(ysim ~ kidney$age) beta0.boot[r] &lt;- lm.boot$coef[1] beta1.boot[r] &lt;- lm.boot$coef[2] ## This code can be used to find the standard errors from this bootstrap sample sig.hatr &lt;- summary(lm.boot)$sigma se.beta0.boot[r] &lt;- sig.hatr*sqrt(summary(lm.boot)$cov.unscaled[1,1]) se.beta1.boot[r] &lt;- sig.hatr*sqrt(summary(lm.boot)$cov.unscaled[2,2]) } Because we have the formulas for the standard errors of \\(\\beta_{0}\\) and \\(\\beta_{1}\\), we can use studentized bootstrap confidence intervals without using the double bootstrap approach. Estimates of the standard error for the \\(r^{th}\\) bootstrap replication are \\[\\begin{eqnarray} \\hat{se}_{r}(\\beta_{0}) &amp;=&amp; \\hat{\\sigma}_{r}\\sqrt{\\frac{\\tfrac{1}{n}\\sum_{i=1}^{n} x_{i}^{2}}{S_{xx}}} \\nonumber \\\\ \\hat{se}_{r}(\\beta_{1}) &amp;=&amp; \\hat{\\sigma}_{r}/\\sqrt{S_{xx}} \\end{eqnarray}\\] These standard error estimates come from applying the formulas in (10.1) to the \\(r^{th}\\) bootstrap sample. Recall from Chapter 9 that the studentized confidence intervals are found by using the following formula. \\[\\begin{equation} \\Big[ T_{n} - se_{boot} \\times \\hat{K}_{R}^{-1}(1 - \\alpha/2), T_{n} - se_{boot} \\times \\hat{K}_{R}^{-1}(\\alpha/2) \\Big] \\nonumber \\end{equation}\\] R code to compute the studentized confidence intervals is given below: ## First get estimates of the standard error of our estimates ## I use the formulas for the regression standard errors, but ## we could have used a bootstrap estimate. se.est0 &lt;- summary(lm.kidney)$sigma*sqrt(summary(lm.boot)$cov.unscaled[1,1]) se.est1 &lt;- summary(lm.kidney)$sigma*sqrt(summary(lm.boot)$cov.unscaled[2,2]) stu.quants0 &lt;- quantile( (beta0.boot - beta0.hat)/se.beta0.boot, probs=c(0.025, 0.975)) stu.quants1 &lt;- quantile( (beta1.boot - beta1.hat)/se.beta1.boot, probs=c(0.025, 0.975)) The studentized bootstrap confidence intervals are then ## Confidence interval for beta0 c(beta0.hat - stu.quants0[2]*se.est0, beta0.hat - stu.quants0[1]*se.est0) ## (Intercept) (Intercept) ## 2.14 3.56 ## Confidence interval for beta1 c(beta1.hat - stu.quants1[2]*se.est1, beta1.hat - stu.quants1[1]*se.est1) ## age age ## -0.0954 -0.0608 Compare these studentized bootstrap confidence intervals with the confidence intervals computed under the normality assumption for the residuals: confint(lm.kidney) ## 2.5 % 97.5 % ## (Intercept) 2.1497 3.5703 ## age -0.0965 -0.0607 Exercise 10.1 Using the parametric bootstrap, compute studentized bootstrap confidence for \\(\\beta_{0}\\) and \\(\\beta_{1}\\) in the kidney data example. This time, assume that \\(\\varepsilon_{i} \\sim \\textrm{Normal}(0, \\hat{\\sigma}^{2})\\). 10.2.2 Nonparametric Bootstrap for Regression If we think of the \\(x_{i}\\) as fixed values, the \\(Y_{i}\\) in a linear regression are not i.i.d. because the means are not the same. This suggests that we cannot use the usual nonparametric bootstrap to construct confidence intervals for \\(\\beta_{0}\\) and \\(\\beta_{1}\\). However, if we also view the \\(x_{i}\\) as random, we can view the pairs of observations \\((Y_{1}, x_{1}), \\ldots, (Y_{n}, x_{n})\\) as i.i.d. observations from a bivariate distribution. In this case, you can also think of \\(\\hat{\\beta}_{1}\\) as an estimate of the following quantity \\[\\begin{equation} \\rho_{YX}\\frac{\\sigma_{y}}{\\sigma_{x}} \\nonumber \\end{equation}\\] where \\(\\rho_{YX} = \\textrm{Corr}(Y_{i}, x_{i})\\). In the case when \\((Y_{i}, x_{i})\\) are bivariate normal, the conditional expectation of \\(Y_{i}\\) given \\(x_{i}\\) has the linear regression structure: \\[\\begin{equation} E(Y_{i}| x_{i}) = \\beta_{0} + \\beta_{1}x_{i}, \\nonumber \\end{equation}\\] where \\(\\beta_{0} = \\mu_{y} - \\rho_{YX}\\frac{\\sigma_{y}\\mu_{x}}{\\sigma_{x}}\\) and \\(\\beta_{1} = \\rho_{YX}\\frac{\\sigma_{y}}{\\sigma_{x}}\\). So, even if the linear model is not exactly true, our estimate and confidence interval still has a clear interpretation. If the linear model assumption is true, the true standard error of \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) will be slightly different than the formulas shown in (10.1). Nevertheless, (10.1) can be thought of as consistent estimates of the true standard error. If we are thinking of the observations \\((Y_{1}, x_{1}), \\ldots, (Y_{n}, x_{n})\\) as i.i.d. pairs, we can use the nonparametric bootstrap by just subsampling pairs of observations. So, to generate bootstrap replications \\(\\hat{\\beta}_{0,r}^{*}\\), \\(\\hat{\\beta}_{1, r}^{*}\\) for \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\), we just use the following procedure For \\(r = 1, \\ldots, R\\): Draw a sample of size \\(n\\): \\(\\big((Y_{1}^{*}, x_{1}^{*}), \\ldots, (Y_{n}^{*}, x_{n}^{*}) \\big)\\) by sampling with replacement from the original data. Compute \\(\\hat{\\beta}_{0,r}^{*}\\) and \\(\\hat{\\beta}_{1,r}^{*}\\) from this bootstrap sample. R code for generating these bootstrap replications for the kidney data is below: R &lt;- 500 beta0.boot.np &lt;- numeric(R) beta1.boot.np &lt;- numeric(R) se.beta0.boot.np &lt;- numeric(R) se.beta1.boot.np &lt;- numeric(R) for(r in 1:R) { subsamp.ind &lt;- sample(1:157, size=157, replace=TRUE) kidney.tmp &lt;- kidney[subsamp.ind,] lm.boot &lt;- lm(tot ~ age, data=kidney.tmp) beta0.boot.np[r] &lt;- lm.boot$coef[1] beta1.boot.np[r] &lt;- lm.boot$coef[2] ## This code can be used to find the standard errors from this bootstrap sample sig.hatr &lt;- summary(lm.boot)$sigma se.beta0.boot.np[r] &lt;- sig.hatr*sqrt(summary(lm.boot)$cov.unscaled[1,1]) se.beta1.boot.np[r] &lt;- sig.hatr*sqrt(summary(lm.boot)$cov.unscaled[2,2]) } To find the studentized confidence intervals for this nonparametric bootstrap, we can use the following code: se.est0 &lt;- summary(lm.kidney)$sigma*sqrt(summary(lm.boot)$cov.unscaled[1,1]) se.est1 &lt;- summary(lm.kidney)$sigma*sqrt(summary(lm.boot)$cov.unscaled[2,2]) stu.quants0.np &lt;- quantile( (beta0.boot.np - beta0.hat)/se.beta0.boot.np, probs=c(0.025, 0.975)) stu.quants1.np &lt;- quantile( (beta1.boot.np - beta1.hat)/se.beta1.boot.np, probs=c(0.025, 0.975)) The studentized bootstrap confidence intervals for \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are then ## Confidence interval for beta0 c(beta0.hat - stu.quants0.np[2]*se.est0, beta0.hat - stu.quants0.np[1]*se.est0) ## (Intercept) (Intercept) ## 2.14 3.56 ## Confidence interval for beta1 c(beta1.hat - stu.quants1.np[2]*se.est1, beta1.hat - stu.quants1.np[1]*se.est1) ## age age ## -0.096 -0.060 Exercise 10.2 Another way of using the bootstrap in a regression context is to resample the residuals from the fitted regression model. Speficially, we first fit the linear regression model and compute residuals \\(\\hat{e}_{1}, \\ldots, \\hat{e}_{n}\\) via \\[\\begin{equation} \\hat{e}_{i} = Y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{i} \\nonumber \\end{equation}\\] One then generates a bootstrap sample by first subsampling \\((\\hat{e}_{1}^{*}, \\ldots, \\hat{e}_{n}^{*})\\) from the vector of “original” residuals \\((\\hat{e}_{1}, \\ldots, \\hat{e}_{n})\\) and then setting \\(Y_{i}^{*} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i} + \\hat{e}_{i}^{*}\\). You then compute the bootstrap replications \\(\\hat{\\beta}_{0,r}^{*}\\) and \\(\\hat{\\beta}_{1,r}^{*}\\) by fitting a linear regression with data: \\((Y_{1}^{*}, x_{1}), \\ldots, (Y_{n}^{*}, x_{n})\\). Using the kidney data, try using this procedure to construct \\(95\\%\\) bootstrap confidence intervals for \\(\\beta_{0}\\) and \\(\\beta_{1}\\). Regression with more than 1 covariate If we have more than one covariate in our model, for example, \\[\\begin{equation} Y_{i} = \\beta_{0} + \\beta_{1}x_{i1} + \\ldots + \\beta_{p}x_{ip} + \\varepsilon_{i}, \\nonumber \\end{equation}\\] the bootstrap works essentially the same as for the case with a single covariate. For the parametric bootstrap with a Normal residual distribution, you would just simulate \\(Y_{i}^{*} \\sim \\textrm{Normal}(\\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i1} + \\ldots + \\hat{\\beta}_{p}x_{ip}, \\hat{\\sigma}^{2})\\). For the nonparametric bootstrap, you would subsample pairs \\((Y_{1}^{*}, x_{1}^{*}), \\ldots, (Y_{n}^{*}, x_{n}^{*})\\) as described before, and compute your regression coefficients \\(\\hat{\\beta}_{0,r}^{*}, \\hat{\\beta}_{1,r}^{*}, \\ldots, \\hat{\\beta}_{p,r}^{*}\\) from this bootstrap sample. 10.3 Pointwise Confidence Intervals for a Density Function Recall that a kernel density estimate of an unknown probability density \\(f(x)\\) has the form \\[\\begin{equation} \\hat{f}_{h_{n}}(x) = \\frac{1}{n h_{n}}\\sum_{i=1}^{n} K\\Big( \\frac{x - X_{i}}{h_{n}} \\Big) \\nonumber \\end{equation}\\] We cannot naively apply the Central Limit Theorem, because \\(h_{n}\\) is changing as \\(n \\longrightarrow \\infty\\). Nevertheless, you can show (see, e.g., Tsybakov (2008)) that \\[\\begin{equation} \\sqrt{nh_{n}}\\Big( \\hat{f}_{h_{n}}(x) - E\\{ \\hat{f}_{h_{n}}(x) \\} \\Big) \\longrightarrow \\textrm{Normal}\\big( 0, \\kappa_{2}(K) f(x) \\big) \\nonumber \\end{equation}\\] provided that \\(h_{n} \\longrightarrow 0\\) and \\(nh_{n} \\longrightarrow \\infty\\). Here, \\(\\kappa_{2}(K) = \\int_{-\\infty}^{\\infty} K^{2}(u) du\\). This suggests that a standard error estimate for \\(\\hat{f}_{h_{n}}(x)\\) is \\(\\sqrt{\\kappa_{2}(K)\\hat{f}_{h_{n}}(x)/nh_{n}}\\) and a \\(95\\%\\) confidence interval for \\(E\\{ \\hat{f}_{h_{n}}(x) \\}\\) is \\[\\begin{equation} \\Bigg[ \\hat{f}_{h_{n}}(x) - 1.96 \\times \\sqrt{\\frac{\\kappa_{2}(K) \\hat{f}_{h_{n}}(x) }{nh_{n}}}, \\hat{f}_{h_{n}}(x) + 1.96 \\times \\sqrt{\\frac{\\kappa_{2}(K) \\hat{f}_{h_{n}}(x)}{nh_{n}}} \\Bigg] \\nonumber \\end{equation}\\] Notice that this is a confidence interval for \\(E\\{ \\hat{f}_{h_{n}}(x) \\}\\) rather than \\(f(x)\\). So, you can roughly think of this as a confidence interval for a smoothed version of \\(f(x)\\) at \\(x\\): \\[\\begin{equation} E\\{ \\hat{f}_{h_{n}}(x) \\} = \\frac{1}{h_{n}}\\int_{-\\infty}^{\\infty} K\\Big( \\frac{x - t}{h_{n}} \\Big) f(t) dt \\nonumber \\end{equation}\\] Notice also that this is a pointwise confidence interval. It is not a confidence band. Methods for computing “bias-corrected” confidence intervals for \\(f(x)\\) are discussed, for example, in Chen (2017). To get a bootstrap estimate of the standard deviation of \\(\\hat{f}_{h_{n}}(x)\\), we can use the usual steps. For \\(r=1, \\ldots, R\\): Draw a sample of size \\(n\\): \\((X_{1}^{*}, \\ldots, X_{n}^{*})\\) by sampling with replacement from \\(\\mathbf{X}\\). Compute \\(T_{n,r}^{*} = \\tfrac{1}{nh_{n}}\\sum_{i=1}^{n} K(\\tfrac{x - X_{i}^{*}}{ h_{n} } )\\). Then, compute the estimated standard error: \\[\\begin{equation} \\hat{se}_{boot} = \\Big[ \\frac{1}{R-1} \\sum_{r=1}^{R} \\Big( T_{n,r}^{*} - \\frac{1}{R} \\sum_{r=1}^{R} T_{n,r}^{*} )^{2} \\Big]^{1/2} \\end{equation}\\] R code to compute these standard error estimates for the sysBP variable from the framingham dataset is given below framingham &lt;- read.csv(&quot;~/Documents/STAT685Notes/Data/framingham.csv&quot;) R &lt;- 500 BootMat &lt;- matrix(0, nrow=R, ncol=4) for(r in 1:R) { xx.boot &lt;- sample(framingham$sysBP, size=length(framingham$sysBP), replace=TRUE) kk.boot &lt;- density(xx.boot) tmp &lt;- approxfun(kk.boot$x, kk.boot$y) BootMat[r,] &lt;- c(tmp(100), tmp(125), tmp(150), tmp(175)) } bb &lt;- apply(BootMat, 2, sd) Figure 10.1: Bootstrap confidence intervals for the density function at the points x=100, 125, 150, 175 10.4 When can the Bootstrap Fail? While the bootstrap is very automatic and could be used to construct confidence intervals in nearly any situation, these bootstrap confidence intervals may fail to give the correct coverage in some situations. A few situations in which the bootstrap can fail include: If we are interested in estimating a parameter \\(\\theta\\) and the support \\(\\{ x: f_{\\theta}(x) &gt; 0\\}\\) of the density function depends on \\(\\theta\\). If there are parameter constraints and the true value of the parameter lies on the boundary of the parameter space. For example, we estimate \\(\\theta\\) subject to the constraint that \\(\\theta \\geq 0\\), and the true value of \\(\\theta\\) is zero. If \\(T_{n} = g(\\bar{X})\\) and \\(g&#39;(\\mu) = 0\\) where \\(\\mu = E(X_{1})\\). No finite mean. If \\(E(|X_{1}|)\\) is not finite, then the bootstrap may not work well. 10.4.1 Example: The Shifted Exponential Distribution Let us consider observations \\(X_{1}, \\ldots, X_{n}\\) that follow the shifted exponential distribution whose density function is \\[\\begin{equation} f(x) = \\begin{cases} \\lambda e^{-\\lambda(x - \\eta)} &amp; \\textrm{ if } x \\geq \\eta \\nonumber \\\\ 0 &amp; \\textrm{otherwise} \\nonumber \\end{cases} \\end{equation}\\] where \\(\\lambda &gt; 0\\) and \\(\\eta &gt; 0\\). The maximum likelihood estimates of \\(\\lambda\\) and \\(\\eta\\) are \\[\\begin{equation} \\hat{\\lambda} = \\frac{1}{\\bar{X}} - X_{(1)} \\qquad \\hat{\\eta} = X_{(1)} \\nonumber \\end{equation}\\] where \\(X_{(1)} = \\min\\{ X_{1}, \\ldots, X_{n} \\}\\) is the smallest observation. Notice that this is an example where the support of the density function depends on the parameter \\(\\eta\\). Suppose we use the bootstrap to construct confidence intervals for \\(\\lambda\\) and \\(\\eta\\). What will happen? Let us consider an example where we have i.i.d. data \\(X_{1}, \\ldots, X_{n}\\) that follow a shifted Exponential distribution with \\(\\lambda = 1/3\\) and \\(\\eta = 2\\). The following code can estimate the coverage proportion of a bootstrap confidence interval for \\(\\eta\\): n &lt;- 200 R &lt;- 500 eta.true &lt;- 2 nreps &lt;- 500 Cover.bootsd.ci &lt;- numeric(nreps) for(k in 1:nreps) { ## Step 1: Generate the Data and compute the estimate of eta xx &lt;- 2 + rexp(n, rate=1/3) eta.hat &lt;- min(xx) ## Step 2: Find bootstrap confidence intervals using R bootstrap replications eta.boot &lt;- numeric(R) for(r in 1:R) { boot.xx &lt;- sample(xx, size=n, replace = TRUE) eta.boot[r] &lt;- min(boot.xx) } boot.ci.sd &lt;- c(eta.hat - 1.96*sd(eta.boot), eta.hat + 1.96*sd(eta.boot)) ## Step 3: Record if the true parameter is covered or not: Cover.bootsd.ci[k] &lt;- ifelse(boot.ci.sd[1] &lt; eta.true &amp; boot.ci.sd[2] &gt;= eta.true, 1, 0) } The estimated coverage for this bootstrap confidence interval is mean(Cover.bootsd.ci) ## [1] 0.82 10.5 The Jackknife The jackknife is also a nonparametric method for estimating standard errors. Like the bootstrap, the jackknife also uses the idea of looking at multiple subsets of the data. Also like the bootstrap, the jackknife is completely automatic in the sense that we only need to be able to compute our statistic of interest, and we do not need to do any formal calculations to find the standard error. While the jackknife was actually developed before the bootstrap, it is used much less than the bootstrap is in applications - at least in the context of finding confidence intervals. We will define \\(\\mathbf{X}_{-i}\\) to be the vector of observations that has the \\(i^{th}\\) observation deleted: \\[\\begin{equation} \\mathbf{X}_{(-i)} = (X_{1}, \\ldots, X_{i-1}, X_{i+1}, \\ldots, X_{n}) \\nonumber \\end{equation}\\] Define \\(T_{n,(-i)}\\) to be the value of the statistic \\(T_{n}\\) when using data which has the \\(i^{th}\\) observation removed \\[\\begin{equation} T_{n, (-i)} = h(X_{1}, \\ldots, X_{i-1}, X_{i+1}, \\ldots, X_{n}) \\nonumber \\end{equation}\\] The jackknife estimate of the standard error of \\(T_{n}\\) is \\[\\begin{equation} \\hat{se}_{jack} = \\Big[ \\frac{n-1}{n} \\sum_{i=1}^{n} ( T_{n, (-i)} - \\bar{T}_{n, jack} )^{2} \\Big]^{1/2}, \\nonumber \\end{equation}\\] where \\(\\bar{T}_{n,jack} = \\tfrac{1}{n} \\sum_{i=1}^{n} T_{n, (-i)}\\). An advantage of the jackknife is that, like the bootstrap, it does not make any particular parametric assumptions about the distribution of the data. However, the jackknife is more dependent on a smoothness assumption (that is smoothness across slightly perturbed datasets) than the bootstrap. An example of this is the sample median where, if we delete one observation, the sample median has a different definition due to the sample size being even vs. odd. References "],
["kernel-regression-and-local-regression.html", "Chapter 11 Kernel Regression and Local Regression 11.1 Introduction 11.2 Kernel Regression 11.3 Local Linear Regression 11.4 Selecting the Bandwidth/Smoothing Parameter 11.5 Additional functions in R 11.6 Multivariate Problems 11.7 Additional Reading 11.8 Exercises", " Chapter 11 Kernel Regression and Local Regression 11.1 Introduction In regression we are interested in characterizing, in some way, the relationship between a collection of responses \\(Y_{1},\\ldots,Y_{n}\\) and covariate vectors \\((\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n})\\). Linear regression is one way of approaching this problem. This assumes the expectation of \\(Y_{i}\\) can be expressed as a linear combination of the covariates: \\[\\begin{equation} E(Y_{i}| \\mathbf{x}_{i}) = \\beta_{0} + \\sum_{j=1}^{p} x_{ij}\\beta_{j} \\nonumber \\end{equation}\\] More generally, we can consider the following model \\[\\begin{equation} Y_{i} = m( \\mathbf{x}_{i} ) + \\varepsilon_{i} \\nonumber \\end{equation}\\] \\(m(\\mathbf{x}_{i})\\) - the “mean function” or “regression function” \\(\\mathbf{x}_{i} = (x_{i1}, \\ldots, x_{ip})\\) - the \\(i^{th}\\) covariate vector The residuals \\(\\varepsilon_{1}, \\ldots, \\varepsilon_{n}\\) are assumed to be i.i.d. and have mean zero. In a nonparametric approach, we will try to estimate \\(m(\\mathbf{x})\\) without making any strong assumptions about the form of \\(m( \\mathbf{x} )\\). The regression function \\(m(\\mathbf{x})\\) can be thought of as the function which returns the expectation of \\(Y_{i}\\) given that \\(\\mathbf{x}_{i} = \\mathbf{x}\\) \\[\\begin{equation} m(\\mathbf{x} ) = E(Y_{i}|\\mathbf{x}_{i}=\\mathbf{x}) \\end{equation}\\] Let \\(f_{Y|X}(y|\\mathbf{x})\\) denote the conditional density of \\(Y_{i}\\) given \\(\\mathbf{x}_{i}\\). \\(f_{Y,X}(y, \\mathbf{x})\\) denote the joint density of \\((Y_{i}, \\mathbf{x}_{i})\\) \\(f_{X}(\\mathbf{x})\\) denote the density of \\(\\mathbf{x}_{i}\\) We can express the regression function as \\[\\begin{equation} m(\\mathbf{x}) = \\int_{-\\infty}^{\\infty} y f_{Y|X}(y|\\mathbf{x}) dy = \\frac{\\int y f_{Y,X}(y, \\mathbf{x}) dy}{ f_{X}(\\mathbf{x}) } \\nonumber \\end{equation}\\] 11.2 Kernel Regression In this section, we will assume that the covariates are univariate. That is, \\(p=1\\) and \\(\\mathbf{x}_{i} = x_{i}\\) where \\(x_{i}\\) is a real number. 11.2.1 The Regressogram The regressogram is an estimate of the mean function \\(m(x)\\) which is has many similarities in its construction to the histogram. Similar to how we constructed the histogram, let us think about an estimate \\(m(x)\\) that will be constant within each of a series of bins \\(B_{1}, \\ldots, B_{D_{n}}\\) \\[\\begin{eqnarray} B_{1} &amp;=&amp; [ x_{0}, x_{0} + h_{n}) \\nonumber \\\\ B_{2} &amp;=&amp; [x_{0} + h_{n}, x_{0} + 2h_{n}) \\nonumber \\\\ &amp;\\vdots&amp; \\nonumber \\\\ B_{D_{n}} &amp;=&amp; [x_{0} + (D_{n} - 1)h_{n}, x_{0} + D_{n}h_{n}) \\nonumber \\end{eqnarray}\\] Suppose we want to estimate \\(m(x)\\), where \\(x\\) belongs to the \\(k^{th}\\) bin. A direct estimate of this is the average of the \\(Y_{i}&#39;s\\) among those \\(x_{i}&#39;s\\) which fall into the \\(k^{th}\\) bin. Specifically, if \\(x \\in B_{k}\\), then we estimate \\(m(x)\\) with \\[\\begin{equation} \\hat{m}_{h_{n}}^{R}(x) = \\frac{ \\sum_{i=1}^{n} Y_{i} I\\big( x_{i} \\in B_{k} \\big) }{ \\sum_{i=1}^{n} I\\big( x_{i} \\in B_{k} \\big) } = \\frac{1}{n_{k,h_{n}}} \\sum_{i=1}^{n} Y_{i} I\\big( x_{i} \\in B_{k} \\big), \\nonumber \\end{equation}\\] where \\(n_{k,h_{n}}\\) is the number of \\(x_{i}\\) that fall into the \\(k^{th}\\) bin when using bin width \\(h_{n}\\). The estimate \\(\\hat{m}_{h_{n}}^{R}(x)\\) of the regression function is called the regressogram. The intuition for this estimate is: if \\(x \\in B_{k}\\), then taking an average of the reponses for \\(x_{i}\\) in a small bin containing \\(x\\) should give us a reasonable approximation for the expectation of \\(Y_{i}\\) given that \\(x_{i} = x\\). Another way of looking at the regressogram is to note that if we think of the \\(x_{i}\\) as random variables, then for \\(x \\in B_{k}\\) \\[\\begin{eqnarray} E\\Big\\{ \\frac{1}{n} \\sum_{i=1}^{n} Y_{i} I\\big( x_{i} \\in B_{k} \\big) \\Big\\} &amp;=&amp; E\\Big\\{ Y_{1} I\\big( x_{1} \\in B_{k} \\big) \\Big\\} \\nonumber \\\\ &amp;=&amp; \\int_{-\\infty}^{\\infty} \\int_{x_{0} + (k-1)h_{n}}^{x_{0} + kh_{n}} y f_{Y,X}(y, t) dt dy \\nonumber \\\\ &amp;\\approx&amp; h_{n} \\int_{-\\infty}^{\\infty} y f_{Y,X}(y, x) dy \\tag{11.1} \\end{eqnarray}\\] and, similarly, \\[\\begin{eqnarray} E\\Big\\{ \\frac{1}{n} \\sum_{i=1}^{n} I\\big( x_{i} \\in B_{k} \\big) \\Big\\} &amp;=&amp; E\\Big\\{ I\\big( x_{1} \\in B_{k} \\big) \\Big\\} \\nonumber \\\\ &amp;=&amp; \\int_{x_{0} + (k-1)h_{n}}^{x_{0} + kh_{n}} f_{X}(t) dt \\nonumber \\\\ &amp;\\approx&amp; h_{n} f_{X}(x) \\tag{11.2} \\end{eqnarray}\\] Equations (11.1) and (11.2) suggest that \\(\\hat{m}_{h_{n}}^{R}(x)\\) should be a reasonable estimate of the ratio \\[\\begin{equation} \\int_{-\\infty}^{\\infty} y f_{Y,X}(y, x) dy \\big/ f_{X}(x) \\nonumber \\end{equation}\\] Figure 8.1: Framingham Data. Regressogram estimate for a regression model with diastolic blood pressure as the response and age as the covariate. Ages from 31-71 were separated into bins of width 5 years. 11.2.2 The Local Average Estimator The regressogram can be thought of as a regression analogue of the histogram. The local average estimator can be thought of as a regression analogue of the “box-type” density estimator that we described in Chapter 8. For each point \\(x\\), we are going to use a regression function estimate which has a bin “centered” at \\(x\\). Specifically, for each \\(x\\), we will form a bin of width \\(2h_{n}\\) around \\(x\\) and compute the mean of the \\(Y_{i}\\) among those observations where the \\(x_{i}\\) fall into this bin. In other words, we are computing an average of the \\(Y_{i}\\) in a small region around \\(x\\). The local average estimator \\(\\hat{m}_{h_{n}}^{loc}(x)\\) at \\(x\\) is defined as: \\[\\begin{eqnarray} \\hat{m}_{h_{n}}^{loc}(x) &amp;=&amp; \\frac{ \\sum_{i=1}^{n} Y_{i}I\\big( x - h_{n} &lt; x_{i} &lt; x + h_{n} \\big) }{ \\sum_{i=1}^{n} I\\big( x - h_{n} &lt; x_{i} &lt; x + h_{n} \\big) } \\nonumber \\\\ &amp;=&amp; \\frac{1}{n_{h_{n}}(x)} \\sum_{i=1}^{n} Y_{i}I\\big( x - h_{n} &lt; x_{i} &lt; x + h_{n} \\big) \\nonumber \\end{eqnarray}\\] where \\(n_{h_{n}}(x) = \\sum_{i=1}^{n} I\\big( x - h_{n} &lt; x_{i} &lt; x + h_{n} \\big)\\). The local average estimator does not need to have a constant value within each of a few pre-specified bins. We can also express the local average estimator in the following way: \\[\\begin{equation} \\hat{m}_{h_{n}}^{loc}(x) = \\frac{\\sum_{i=1}^{n} Y_{i} w\\Big( \\frac{x - X_{i}}{h_{n}} \\Big)}{\\sum_{i=1}^{n} w\\Big( \\frac{x - X_{i}}{h_{n}} \\Big)}, \\tag{11.3} \\end{equation}\\] where \\(w(t)\\) is the “box” function defined as \\[\\begin{equation} w(t) = \\begin{cases} \\frac{1}{2} &amp; \\textrm{ if } |t| &lt; 1 \\nonumber \\\\ 0 &amp; \\textrm{ otherwise} \\nonumber \\end{cases} \\end{equation}\\] While a local average estimate will not be a “step function” like the regressogram, the local average estimate will typically be non-smooth and have a jagged appearance. Like kernel density estimation, there is a bias/variance tradeoff to the choice of \\(h_{n}\\). Smaller values of \\(h_{n}\\) usually imply higher variance because you will be taking an average over a relatively small number of observations. Larger values of \\(h_{n}\\) usually imply higher bias because you will be esitmating \\(m(x)\\) by averaging over a wide range of \\(x_{i}\\) values, and \\(m(x)\\) could vary substantially over this range of \\(x_{i}\\) values. Our experience in Chapter 8 suggests that we can get a smoother estimate of the regression if we simply replace the “box function” \\(w(t)\\) in (11.3) with a smoother kernel function \\(K(t)\\). R code for computing a local average estimate \\(\\hat{m}_{2}^{loc}(x)\\) at the points \\(x = 31, 32, 33, ...., 71\\) is given below xseq &lt;- seq(31, 71, by=1) hn &lt;- 2 nx &lt;- length(xseq) m.hat.loc &lt;- numeric(nx) for(k in 1:nx) { in.bin &lt;- framingham$age &gt; xseq[k] - hn &amp; framingham$age &lt; xseq[k] + hn m.hat.loc[k] &lt;- mean(framingham$diaBP[in.bin]) } plot(framingham$age, framingham$diaBP, las=1, ylab=&quot;Diastolic Blood Pressure&quot;, xlab=&quot;Age&quot;, main=&quot;Local Average Estimate with hn=2&quot;, type=&quot;n&quot;) points(framingham$age, framingham$diaBP, pch=16, cex=0.7) lines(xseq, m.hat.loc, lwd=3, col=&quot;red&quot;) Let’s also look at a local average estimate of the regression function for the bone mineral density dataset. The responses in this dataset are relative changes in the bone mineral density of adolescents. Specifically, reponses \\(Y_{i}\\) and covariates \\(x_{i}\\) are defined as \\[\\begin{eqnarray} Y_{i} &amp;=&amp; \\frac{\\textrm{Mineral Density at Visit 2}_{i} - \\textrm{Mineral Density at Visit 1}_{i}}{\\tfrac{1}{2}(\\textrm{Mineral Density at Visit 2}_{i} + \\textrm{Mineral Density at Visit 1}_{i})} \\nonumber \\\\ x_{i} &amp;=&amp; \\frac{1}{2}(\\textrm{Age at Visit 2}_{i} + \\textrm{Age at Visit 1}_{i}) \\nonumber \\end{eqnarray}\\] tmp &lt;- read.table(&quot;https://web.stanford.edu/~hastie/ElemStatLearn/datasets/bone.data&quot;, header=TRUE) bonedat &lt;- tmp[!duplicated(tmp$idnum),] ## only keep the first observation of a person xseq &lt;- seq(9.4, 25.2, by=.1) hn &lt;- 1 nx &lt;- length(xseq) m.hat.loc &lt;- numeric(nx) for(k in 1:nx) { in.bin &lt;- bonedat$age &gt; xseq[k] - hn &amp; bonedat$age &lt; xseq[k] + hn m.hat.loc[k] &lt;- mean(bonedat$spnbmd[in.bin]) } plot(bonedat$age, bonedat$spnbmd, las=1, ylab=&quot;Relative Change in Bone MD&quot;, xlab=&quot;Age&quot;, main=&quot;Bone Data: Local Average Estimate with hn=1&quot;, type=&quot;n&quot;) points(bonedat$age, bonedat$spnbmd, pch=16, cex=0.7) lines(xseq, m.hat.loc, lwd=3, col=&quot;red&quot;) abline(0, 0) 11.2.3 k-Nearest Neighbor (k-NN) Regression k-nearest neighbor regression is fairly similar to the local average estimator of the regression function. With k-NN, we still estimate the regression function at a particular point by taking a type of local average around this point. However, k-NN takes the average over the k “nearest observations” to \\(x\\) rather than taking an average over all the observations which fall into a bin centered at \\(x\\). The k-NN estimator of the regression function \\(\\hat{m}_{k}^{kNN}(x)\\) is defined as \\[\\begin{equation} \\hat{m}_{k}^{kNN}(x) = \\frac{1}{k}\\sum_{i=1}^{n} y_{i} I\\big( x_{i} \\in N_{k}(x) \\big) \\nonumber \\end{equation}\\] Here, \\(N_{k}(x)\\) is defined as the set of the k \\(x_{i}&#39;s\\) which are closest to \\(x\\). That is, \\(N_{k}(x)\\) is the set of the k “nearest neighbors” to \\(x\\). Mathematically, if we define \\[\\begin{equation} d_{i}(x) = |x_{i} - x| \\nonumber \\end{equation}\\] and order them so that \\(d_{(1)}(x) \\leq d_{(2)}(x) \\leq \\ldots \\leq d_{(n)}(x)\\). Then, the k nearest neighbors of \\(x\\) would be those observations which correspond to the \\(d_{(1)}(x)\\) through \\(d_{(k)}(x)\\). Like the local average estimator, increasing the value of \\(k\\) will increase the bias of the k-NN regression function estimate while decreasing the value of \\(k\\) will increase the variance of the k-NN regression function estimate. 11.2.4 The Nadaraya-Watson Estimator The Nadaraya-Watson estimator \\(\\hat{m}_{h_{n}}^{NW}\\) of the regression function with bandwidth \\(h_{n}\\) is defined as \\[\\begin{equation} \\hat{m}_{h_{n}}^{NW}(x) = \\frac{ \\sum_{i=1}^{n} Y_{i}K\\Big( \\frac{x - x_{i}}{ h_{n} }\\Big) }{ \\sum_{i=1}^{n} K\\Big( \\frac{x - x_{i}}{ h_{n} }\\Big) } \\nonumber \\end{equation}\\] The Nadaraya-Watson estimator has the same basic form as the local average estimator. We have just replaced the “box” function \\(w(t)\\) with the kernel function \\(K(t)\\). You can think of \\(\\hat{m}_{h_{n}}^{NW}(x)\\) as a weighted average of the \\(Y_{i}\\). That is, \\[\\begin{equation} \\hat{m}_{h_{n}}^{NW}(x) = \\sum_{i=1}^{n} a_{i}(x) Y_{i} \\nonumber \\end{equation}\\] The bandwidth \\(h_{n}\\) can also be referred to as the “smoothing parameter” since its value affects how smooth the fitted regression curve appears. The weights \\(a_{1}(x), \\ldots, a_{n}(x)\\), in this case, are defined as \\[\\begin{equation} a_{i}(x) = \\frac{ K(\\tfrac{x - x_{i}}{h_{n}})}{ \\sum_{i=1}^{n} K(\\tfrac{x - x_{i}}{ h_{n}}) } \\nonumber \\end{equation}\\] So, we are using weights which are larger the closer you are to \\(x\\). The Nadaraya-Watson estimator suffers from two main drawbacks. These are design bias and boundary bias. Design bias refers to the effect of the spacing of the \\(x_{i}\\) on the performance of the Nadaraya-Watson estimator. Boundary bias refers to the performance of the Nadaraya-Watson estimator near the smallest and largest \\(x_{i}\\). If we assume that the \\(x_{i}\\) are random and have probability density \\(f_{X}(x)\\), then it can be shown that the mean-squared error of the Nadaraya-Watson estimator at a particular point \\(x\\) has the following approximation \\[\\begin{eqnarray} \\textrm{MSE}(x) &amp;=&amp; E\\Big[ \\{ m(x) - \\hat{m}_{h_{n}}^{NW}(x) \\}^{2} \\Big] \\nonumber \\\\ &amp;\\approx&amp; \\frac{h_{n}^{4}\\mu_{2}^{2}(K)}{4}\\Bigg\\{ m&#39;&#39;(x) + \\frac{2m&#39;(x)f_{X}&#39;(x)}{f_{X}(x)} \\Bigg\\}^{2} + \\frac{\\sigma^{2}}{n h_{n} f_{X}(x) }, \\nonumber \\end{eqnarray}\\] where \\(\\mu_{2}(K) = \\int_{-\\infty}^{\\infty} u^{2} K(u) du\\) and \\(\\kappa_{2}(K) = \\int_{-\\infty}^{\\infty} K^{2}(u) du\\). The term \\(2m&#39;(x)f_{X}&#39;(x)/f_{X}(x)\\) is referred to as the design bias. Notice that this should be zero if the \\(x_{i}\\) are drawn from a Uniform distribution. In other words, if the \\(x_{i}\\) are roughly equally spaced, then the design bias should be small. The Nadaraya-Watson estimator in R The Nadaraya-Watson estimator can be computed in R with the ksmooth function. ksmooth(x, y, kernel, bandwidth, x.points, ...) x - vector of covariate values y - vector of responses kernel - choice of kernel function; default is box; use normal if you want a Gaussian kernel bandwidth - value of the bandwidth; default is \\(0.5\\) x.points - points at which to estimate the regression function; default is to use \\(n\\) equally spaced points. The x vector from the fitted ksmooth object will be the vector of points at which the regression function is estimated. The y vector from the fitted ksmooth object will be a vector containing the estimated values of the regression function. Note that the bandwidth used by this function for the Gaussian kernel is approximately \\(2.7\\) times smaller than the bandwith in our definition of the Gaussian kernel. If you wanted to write your own function that computed the Nadaraya-Watson estimate at a vector of desired points \\(x.points = (t_{1}, \\ldots, t_{q})\\), you could use something like MyNWEst &lt;- function(x, y, bandwidth, x.points) { q &lt;- length(x.points) nw.est &lt;- numeric(q) for(k in 1:q) { ww &lt;- dnorm(x.points[k], mean=x, sd=bandwidth) nw.est[k] &lt;- sum(ww*y)/sum(ww) } return(nw.est) } To compute the Nadraya-Watson estimate at a set of equally spaced of points from \\(10\\) to \\(25\\) using bandwidth \\(0.5\\) and plot the result, you could use the following code: xseq &lt;- seq(10, 25, by=.1) bone.nwest &lt;- ksmooth(x=bonedat$age, y=bonedat$spnbmd, kernel=&quot;normal&quot;, bandwidth=2.7*0.5, x.points=xseq) plot(bonedat$age, bonedat$spnbmd, las=1, ylab=&quot;Relative Change in Bone MD&quot;, xlab=&quot;Age&quot;, main=&quot;Bone Data: Nadaraya-Watson Estimate with hn=0.5 and Gaussian Kernel&quot;, type=&quot;n&quot;) points(bonedat$age, bonedat$spnbmd, pch=16, cex=0.7) lines(bone.nwest$x, bone.nwest$y, lwd=3, col=&quot;red&quot;) ## Note that bone.nwest$x should equal xseq 11.3 Local Linear Regression 11.3.1 Definition You can think of both the regressogram and the local average as methods which fit local intercept models. For the regressogram, we fit an intercept model (that is a flat line curve) within a small bin that contains \\(x\\). For the local average estimator, we fit an intercept model for a small bin around \\(x\\). The Nadaraya-Watson estimator can be thought of as just smoothing out the local intercept approach of the local average estimator. Instead of fitting local intercept models, we could fit local linear models that have an intercept and a slope term. To be specific, suppose we estimated the regression function at \\(x\\) by fitting a linear model using only data from the \\(x_{i}\\) that fell into the bin \\((x - h_{n}, x + h_{n})\\). In this case, we would first fit the linear model \\(\\hat{s}_{x}(x_{i}) = \\hat{\\beta}_{0x} + \\hat{\\beta}_{1x}(x_{i} - x)\\) where \\(\\hat{\\beta}_{0x}\\), \\(\\hat{\\beta}_{1x}\\) solved the following local least-squares problem \\[\\begin{equation} \\hat{\\beta}_{0x}, \\hat{\\beta}_{1x} \\textrm{ minimize: } \\quad \\sum_{i=1}^{n}\\{ Y_{i} - \\beta_{0x} - \\beta_{1x}(x_{i} - x) \\}^{2}I\\big( x - h_{n} &lt; x_{i} &lt; x + h_{n} \\big) \\tag{11.4} \\end{equation}\\] Then, we would estimate \\(m(x)\\) by using the value of \\(\\hat{s}_{x}(\\cdot)\\) at \\(x\\). That is, \\(\\hat{s}_{x}(x) = \\hat{\\beta}_{0x}\\). Local Linear Regression uses the same idea as (11.4), but replaces the indicator function \\(I( x - h_{n} &lt; x_{i} &lt; x + h_{n})\\) with a smooth kernel function. So, the local linear regression estimate of the regression function at \\(x\\) is \\[\\begin{eqnarray} \\hat{m}_{h_{n}}^{loclin}(x) &amp;=&amp; \\hat{\\beta}_{0x} \\quad \\textrm{ where } \\nonumber \\\\ \\hat{\\beta}_{0x}, \\hat{\\beta}_{1x} &amp;=&amp; \\textrm{argmin}_{\\beta_{0x},\\beta_{1x}} \\sum_{i=1}^{n}\\{ Y_{i} - \\beta_{0x} - \\beta_{1x}(x_{i} - x) \\}^{2}K\\Big( \\frac{x - x_{i}}{h_{n}} \\Big) \\nonumber \\end{eqnarray}\\] 11.3.2 Advantages of the Local Linear Estimator The local linear regression estimator can reduce the effects of design and boundary bias. If we write the local linear estimate at the point \\(x\\) as \\(\\hat{m}_{h_{n}}^{loclin}(x) = \\sum_{i=1}^{n} a_{i}^{h_{n}}(x)Y_{i}\\), then the bias is appoximately \\[\\begin{equation} E\\{ \\hat{m}_{h_{n}}^{loclin}(x) \\} - m(x) \\approx m&#39;(x)\\sum_{i=1}^{n} (x_{i} - x)a_{i}^{h_{n}}(x) + \\frac{m&#39;&#39;(x)}{2}\\sum_{i=1}^{n} (x_{i} - x)^{2}a_{i}^{h_{n}}(x) \\nonumber \\end{equation}\\] For local linear regression, the term \\(m&#39;(x)\\sum_{i=1}^{n} (x_{i} - x)a_{i}^{h_{n}}(x)\\) equals zero. If the weights \\(a_{i}^{h_{n}}(x)\\) were the weights from the Nadaraya-Watson estimator, this term would not necessarily equal zero. Also, the local linear estimator can help to reduce the boundary bias that arises from asymmetry near the boundary (draw a picture). 11.3.3 An Example in R An R function which implements local linear regression is the following. The input for this function has the same structure as our earlier Nadaraya-Watson R function. MyLocLinear &lt;- function(x, y, bandwidth, x.points) { q &lt;- length(x.points) loclin.est &lt;- numeric(q) for(k in 1:q) { ## First create weights with Gaussian kernel xtmp &lt;- x - x.points[k] ww &lt;- dnorm(xtmp, mean=0, sd=bandwidth) ## Now, compute the intercept with a weighted linear regression loclin.est[k] &lt;- lm(y ~ xtmp, weights=ww)$coef[1] } return(loclin.est) } Let’s try this function with the bonedat dataset again. Using age as the covariate, we will estimate the regression function at the points \\(10, 10.1, 10.2, ..., 25\\): xseq &lt;- seq(10, 25, by=.1) bone.loclin &lt;- MyLocLinear(x=bonedat$age, y=bonedat$spnbmd, bandwidth=0.5, x.points=xseq) plot(bonedat$age, bonedat$spnbmd, las=1, ylab=&quot;Relative Change in Bone MD&quot;, xlab=&quot;Age&quot;, main=&quot;Local Linear Estimator with hn=0.5&quot;, type=&quot;n&quot;) points(bonedat$age, bonedat$spnbmd, pch=16, cex=0.7) lines(xseq, bone.loclin, lwd=3, col=&quot;red&quot;) Let’s compare this with the Nadaraya-Watson esitmate that we computed earlier 11.3.4 Local Polynomial Regression There is no reason why we must restrict ourselves to local linear fits. We could also fit local polynomial models. Similar to the way we approached local linear regression, for a fixed \\(x\\) we will fit the local model \\[\\begin{equation} \\hat{s}_{x}^{p}(x_{i}) = \\hat{\\beta}_{0x,p} + \\hat{\\beta}_{1x,p}(x_{i} - x) + \\hat{\\beta}_{2x,p}(x_{i} - x)^{2} + \\ldots + \\beta_{px,p}(x_{i} - x)^{p}, \\nonumber \\end{equation}\\] where the estimated regression coefficients \\(\\hat{\\beta}_{0x,p}, \\hat{\\beta}_{1x,p}, \\ldots, \\hat{\\beta}_{px,p}\\) are found by solving the least-squares problem \\[\\begin{equation} \\sum_{i=1}^{n}\\{ Y_{i} - \\beta_{0x,p} - \\beta_{1x,p}(x_{i} - x) - \\ldots - \\beta_{px,p}(x_{i} - x)^{p} \\}^{2}K\\Big( \\frac{x - x_{i}}{h_{n}} \\Big) \\end{equation}\\] Then, we estimate the regression function at \\(x\\) with \\(\\hat{m}_{h_{n}}^{locpoly}(x) = \\hat{s}_{x}^{p}(x) = \\hat{\\beta}_{0x,p}\\). Note that the local linear regression estimate is just a special case of local polynomial regression with \\(p=1\\). To find the estimates of \\(\\beta_{0x,p}\\) for linear and polynomial regression, you can use the formulas for the regression coefficient estimates in weighted least squares. Define the \\(n \\times n\\) diagonal matrix of weights \\(\\mathbf{W}_{x, h_{n}}\\) as \\[\\begin{equation} \\mathbf{W}_{x, h_{n}} = \\begin{bmatrix} K\\Big( \\frac{x - x_{1}}{h_{n}} \\Big) &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; K\\Big( \\frac{x - x_{2}}{h_{n}} \\Big) &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\ldots &amp; K\\Big( \\frac{x - x_{n}}{h_{n}} \\Big) \\end{bmatrix} \\nonumber \\end{equation}\\] and define the \\(n \\times (p+1)\\) matrix \\(\\mathbf{X}_{x,p}\\) as \\[\\begin{equation} \\mathbf{X}_{x, p} = \\begin{bmatrix} 1 &amp; (x_{1} - x) &amp; \\ldots &amp; (x_{1} - x)^{p} \\\\ 1 &amp; (x_{2} - x) &amp; \\ldots &amp; (x_{2} - x)^{p} \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ 1 &amp; (x_{n} - x) &amp; \\ldots &amp; (x_{n} - x)^{p} \\end{bmatrix} \\nonumber \\end{equation}\\] The vector of estimated regression coefficients is obtained from the following formula \\[\\begin{equation} \\begin{bmatrix} \\hat{\\beta}_{0x,p} \\\\ \\hat{\\beta}_{1x,p} \\\\ \\ldots \\\\ \\hat{\\beta}_{px,p} \\end{bmatrix} = (\\mathbf{X}_{x,p}^{T}\\mathbf{W}_{x,h_{n}}\\mathbf{X}_{x,p})^{-1}\\mathbf{X}_{x,p}^{T}\\mathbf{W}_{x,h_{n}}\\mathbf{Y} \\nonumber \\end{equation}\\] While using local polynomial regression with higher order polynomials offer more flexibility, they come at the price of more variance. For fixed \\(h_{n}\\), increasing the degree \\(p\\) can decrease bias but will increase variance. In practice, \\(p = 1\\) or \\(p = 2\\) seems to be most common in practice. There is often not much benefit to using a degree of \\(3\\) or more. 11.4 Selecting the Bandwidth/Smoothing Parameter 11.4.1 Representing in Linear Form Let \\(\\mathbf{Y} = (Y_{1}, \\ldots, Y_{n})\\) and let \\(\\hat{\\mathbf{m}} = (\\hat{m}(x_{1}), \\ldots, \\hat{m}(x_{n}))\\) denote the vector of “fitted values” from a vector of estimates of the regression function at \\(x_{1}, \\ldots, x_{n}\\). You can represent the fitted values all of the nonparametric estimators discussed thus far as \\[\\begin{equation} \\hat{\\mathbf{m}} = \\mathbf{A}_{h_{n}}\\mathbf{Y} \\nonumber \\end{equation}\\] for an appropriately chosen \\(n \\times n\\) matrix \\(\\mathbf{A}_{h_{n}}\\). For the local average estimator, we have \\(\\hat{\\mathbf{m}} = \\mathbf{A}_{h_{n}}\\mathbf{Y}\\) where \\(\\mathbf{A}_{h_{n}}\\) is defined as \\[\\begin{equation} \\mathbf{A}_{h_{n}} = \\begin{bmatrix} \\frac{1}{n_{h_{n}}(x_{1})}I(x_{1} - h_{n} &lt; x_{1} &lt; x_{1} + h_{n}) &amp; \\ldots &amp; \\frac{1}{n_{h_{n}}(x_{1})}I(x_{1} - h_{n} &lt; x_{n} &lt; x_{1} + h_{n}) \\\\ \\frac{1}{n_{h_{n}}(x_{2})}I(x_{2} - h_{n} &lt; x_{1} &lt; x_{2} + h_{n}) &amp; \\ldots &amp; \\frac{1}{n_{h_{n}}(x_{2})}I(x_{2} - h_{n} &lt; x_{n} &lt; x_{2} + h_{n}) \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{1}{n_{h_{n}}(x_{n})}I(x_{n} - h_{n} &lt; x_{1} &lt; x_{n} + h_{n}) &amp; \\ldots &amp; \\frac{1}{n_{h_{n}}(x_{n})}I(x_{n} - h_{n} &lt; x_{n} &lt; x_{n} + h_{n}) \\end{bmatrix} \\end{equation}\\] where \\(n_{h_{n}}(x) = \\sum_{i=1}^{n}I(x - h_{n} &lt; x_{i} &lt; x + h_{n})\\). In other words, the \\((i,j)\\) element of \\(\\mathbf{A}_{h_{n}}\\) is \\(a_{j}(x_{i})\\) where \\[\\begin{equation} a_{j}(x_{i}) = \\frac{1}{n_{h_{n}}(x_{i})}I(x_{i} - h_{n} &lt; x_{j} &lt; x_{i} + h_{n}) \\nonumber \\end{equation}\\] For the Nadaraya-Watson estimator, the \\(\\mathbf{A}_{h_{n}}\\) matrix is \\[\\begin{equation} \\mathbf{A}_{h_{n}} = \\begin{bmatrix} \\frac{1}{K_{h_{n}}(x_{1}, \\cdot) }K(0) &amp; \\frac{1}{K_{h_{n}}(x_{1}, \\cdot) }K(\\tfrac{x_{2} - x_{1}}{h_{n}}) &amp; \\ldots &amp; \\frac{1}{K_{h_{n}}(x_{1}, \\cdot) }K(\\tfrac{x_{n} - x_{1}}{h_{n}}) \\\\ \\frac{1}{K_{h_{n}}(x_{2}, \\cdot) }K(\\tfrac{x_{1} - x_{2}}{h_{n}}) &amp; \\frac{1}{K_{h_{n}}(x_{2}, \\cdot) }K(0) &amp; \\ldots &amp; \\frac{1}{K_{h_{n}}(x_{2}, \\cdot) }K(\\tfrac{x_{n} - x_{2}}{h_{n}}) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{1}{K_{h_{n}}(x_{n}, \\cdot) }K(\\tfrac{x_{1}-x_{n}}{h_{n}}) &amp; \\frac{1}{K_{h_{n}}(x_{n}, \\cdot) }K(\\tfrac{x_{2} - x_{n}}{h_{n}}) &amp; \\ldots &amp; \\frac{1}{K_{h_{n}}(x_{n}, \\cdot) }K(0) \\end{bmatrix} \\nonumber \\end{equation}\\] where \\[\\begin{equation} K_{h_{n}}(x_{i}, \\cdot) = \\sum_{j=1}^{n}K\\Big( \\frac{x_{i} - x_{j}}{h_{n}} \\Big) \\nonumber \\end{equation}\\] For the local linear regression estimator, the \\(i^{th}\\) row of \\(\\mathbf{A}_{h_{n}}\\) equals the first row of the following \\(2 \\times n\\) matrix: \\[\\begin{equation} (\\mathbf{X}_{x_{i},1}^{T}\\mathbf{W}_{x_{i},h_{n}}\\mathbf{X}_{x_{i},1})^{-1}\\mathbf{X}_{x_{i},1}^{T}\\mathbf{W}_{x_{i},h_{n}} \\nonumber \\end{equation}\\] Here, \\(\\mathbf{X}_{x,1}\\) and \\(\\mathbf{W}_{x, h_{n}}\\) are as defined in the section on local linear regression. In “classic” linear regression where you would try to fit the straight line model \\(Y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\varepsilon_{i}\\), the vector of fitted values would be \\[\\begin{equation} \\hat{\\mathbf{m}} = (\\hat{m}(x_{1}), \\ldots, \\hat{m}(x_{n})) = (\\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{1}, \\ldots, \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{n}) \\nonumber \\end{equation}\\] In this case, you can represent \\(\\hat{\\mathbf{m}}\\) as \\[\\begin{equation} \\hat{\\mathbf{m}} = \\mathbf{X}(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{Y} \\nonumber \\end{equation}\\] where \\(\\mathbf{X}\\) is the following \\(n \\times 2\\) “design” matrix \\[\\begin{equation} \\mathbf{X} = \\begin{bmatrix} 1 &amp; x_{1} \\\\ 1 &amp; x_{2} \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_{n} \\end{bmatrix} \\nonumber \\end{equation}\\] 11.4.2 The Cp Statistic Theorem: If a random vector \\(\\mathbf{Z}\\) has mean vector \\(\\mathbf{\\mu}\\) and covariance matrix \\(\\mathbf{\\Sigma}\\), then \\[\\begin{equation} E\\{ \\mathbf{Z}^{T}\\mathbf{Z} \\} = E\\Big\\{ \\sum_{i=1}^{n} Z_{i}^{2} \\Big\\} = \\mathbf{\\mu}^{T}\\mathbf{\\mu} + \\textrm{tr}( \\mathbf{\\Sigma} ) \\nonumber \\end{equation}\\] Notice that the vector \\(\\mathbf{m} - \\mathbf{A}_{h_{n}}\\mathbf{Y}\\) has \\[\\begin{equation} E( \\mathbf{m} - \\mathbf{A}_{h_{n}}\\mathbf{Y} ) = (\\mathbf{I} - \\mathbf{A}_{h_{n}})\\mathbf{m} \\qquad \\qquad \\textrm{Var}(\\mathbf{m} - \\mathbf{A}_{h_{n}}\\mathbf{Y}) = \\sigma^{2}\\mathbf{A}_{h_{n}}\\mathbf{A}_{h_{n}}^{T} \\nonumber \\end{equation}\\] Also, the vector \\(\\mathbf{Y} - \\mathbf{A}_{h_{n}}\\mathbf{Y} = (\\mathbf{I} - \\mathbf{A}_{h_{n}})\\mathbf{Y}\\) has \\[\\begin{equation} E\\{ (\\mathbf{I} - \\mathbf{A}_{h_{n}})\\mathbf{Y} \\} = (\\mathbf{I} - \\mathbf{A}_{h_{n}})\\mathbf{m} \\qquad \\qquad \\textrm{Var}\\{ (\\mathbf{I} - \\mathbf{A}_{h_{n}})\\mathbf{Y}) = \\sigma^{2} (\\mathbf{I} - \\mathbf{A}_{h_{n}})(\\mathbf{I} - \\mathbf{A}_{h_{n}})^{T} \\nonumber \\end{equation}\\] Ideally, we would like to choose the smoothing parameter \\(h_{n}\\) to minimize the following mean averaged squared error \\[\\begin{eqnarray} \\textrm{MASE}(h_{n}) &amp;=&amp; \\frac{1}{n}\\sum_{i=1}^{n} E\\Big[ \\{ m(x_{i}) - \\hat{m}(x_{i}) \\}^{2} \\Big] \\nonumber \\\\ &amp;=&amp; E\\{ \\frac{1}{n}[\\mathbf{m} - \\mathbf{A}_{h_{n}}\\mathbf{Y}]^{T}[\\mathbf{m} - \\mathbf{A}_{h_{n}}\\mathbf{Y}] \\} \\nonumber \\end{eqnarray}\\] If we apply the above Theorem to the vector \\(\\mathbf{m} - \\mathbf{A}_{h_{n}}\\mathbf{Y}\\), we can notice that \\[\\begin{eqnarray} \\textrm{MASE}( h_{n} ) &amp;=&amp; E\\{ \\frac{1}{n}( \\mathbf{m} - \\mathbf{A}_{h_{n}}\\mathbf{Y} )^{T}(\\mathbf{m} - \\mathbf{A}_{h_{n}}\\mathbf{Y}) \\} \\nonumber \\\\ &amp;=&amp; \\frac{1}{n}[(\\mathbf{I} - \\mathbf{A}_{h_{n}})\\mathbf{m}]^{T}[(\\mathbf{I} - \\mathbf{A}_{h_{n}})\\mathbf{m}] + \\frac{\\sigma^{2}}{n}\\textrm{tr}(\\mathbf{A}_{h_{n}}\\mathbf{A}_{h_{n}}^{T}) \\end{eqnarray}\\] Now, using the mean and covariance matrix for \\((\\mathbf{I} - \\mathbf{A}_{h_{n}})\\mathbf{Y}\\), we also have that \\[\\begin{eqnarray} &amp;&amp; E\\{\\frac{1}{n}[(\\mathbf{I} - \\mathbf{A}_{h_{n}})\\mathbf{Y}]^{T}[(\\mathbf{I} - \\mathbf{A}_{h_{n}})\\mathbf{Y}] \\} \\nonumber \\\\ &amp;=&amp; \\frac{1}{n}[(\\mathbf{I} - \\mathbf{A}_{h_{n}})\\mathbf{m}]^{T}[(\\mathbf{I} - \\mathbf{A}_{h_{n}})\\mathbf{m}] + \\frac{\\sigma^{2}}{n}\\textrm{tr}\\{ (\\mathbf{I} - \\mathbf{A}_{h_{n}})(\\mathbf{I} - \\mathbf{A}_{h_{n}})^{T} \\} \\nonumber \\\\ &amp;=&amp; \\textrm{MASE}(h_{n}) + \\sigma^{2} - \\frac{2\\sigma^{2}}{n}\\textrm{tr}( \\mathbf{A}_{h_{n}}) \\end{eqnarray}\\] So, if \\(\\sigma^{2}\\) is known, then \\(\\widehat{\\textrm{MASE}}( h_{n} )\\) is an unbiased estimate of \\(\\textrm{MASE}( h_{n} )\\) \\[\\begin{eqnarray} \\widehat{\\textrm{MASE}}( h_{n} ) &amp;=&amp; \\frac{1}{n}[(\\mathbf{I} - \\mathbf{A}_{h_{n}})\\mathbf{Y}]^{T}[(\\mathbf{I} - \\mathbf{A}_{h_{n}})\\mathbf{Y}] - \\sigma^{2} + \\frac{2\\sigma^{2}}{n}\\textrm{tr}( \\mathbf{A}_{h_{n}}) \\nonumber \\\\ &amp;=&amp; \\frac{1}{n}\\sum_{i=1}^{n} \\{ Y_{i} - \\hat{m}_{h_{n}}(x_{i}) \\}^{2} - \\sigma^{2} + \\frac{2\\sigma^{2}}{n}\\textrm{tr}( \\mathbf{A}_{h_{n}}) \\nonumber \\end{eqnarray}\\] The predictive mean averaged squared error (PMASE) is defined as \\[\\begin{equation} \\textrm{PMASE}(h_{n}) = E\\Big[ \\frac{1}{n} \\sum_{i=1}^{n} \\{ Y_{i}&#39; - \\hat{m}_{h_{n}}(x_{i}) \\}^{2} \\Big] \\end{equation}\\] where \\(Y_{i}&#39;\\) is a “future” independent observation that has the same covariate as \\(Y_{i}\\). We assume that \\(Y_{i}&#39; = m(x_{i}) + \\varepsilon_{i}&#39;\\) where \\(\\varepsilon_{i}&#39;\\) is independent of \\(\\varepsilon_{i}\\). So, \\[\\begin{eqnarray} \\textrm{PMASE}(h_{n}) &amp;=&amp; E\\Big[ \\frac{1}{n} \\sum_{i=1}^{n} \\{ m(x_{i}) - \\hat{m}_{h_{n}}(x_{i}) + \\varepsilon_{i}&#39; \\}^{2} \\Big] \\nonumber \\\\ &amp;=&amp; E\\Big[ \\frac{1}{n} \\sum_{i=1}^{n} \\{ m(x_{i}) - \\hat{m}_{h_{n}}(x_{i}) \\}^{2} \\Big] + E\\Big[ \\frac{1}{n} \\sum_{i=1}^{n} (\\varepsilon_{i}&#39;)^{2} \\Big] \\nonumber \\\\ &amp;=&amp; \\textrm{MASE}( h_{n} ) + \\sigma^{2} \\nonumber \\end{eqnarray}\\] The \\(C_{p}\\) statistic is based on the idea that, if \\(\\sigma^{2}\\) was known, then the following quantity would be an unbiased estimate of \\(\\textrm{PMASE}( h_{n} )\\): \\[\\begin{equation} \\frac{1}{n}\\sum_{i=1}^{n} \\{ Y_{i} - \\hat{m}_{h_{n}}(x_{i}) \\}^{2} + \\frac{2\\sigma^{2}}{n}\\textrm{tr}( \\mathbf{A}_{h_{n}}) \\nonumber \\end{equation}\\] The \\(C_{p}\\) statistic formula is obtained by plugging in an estimate \\(\\hat{\\sigma}^{2}\\) of the residual variance into the above formula: \\[\\begin{equation} C_{p}(h_{n}) = \\frac{1}{n}\\sum_{i=1}^{n} \\{ Y_{i} - \\hat{m}_{h_{n}}(x_{i}) \\}^{2} + \\frac{2\\hat{\\sigma}^{2}}{n}\\textrm{tr}( \\mathbf{A}_{h_{n}}) \\nonumber \\end{equation}\\] The reason this is called the “\\(C_{p}\\) statistic” is that in the case of a linear regression model with \\(p\\) columns in the design matrix, we have \\(\\hat{\\mathbf{m}} = \\mathbf{X}(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{Y}\\) and \\(\\textrm{tr}\\{ \\mathbf{X}(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T} \\} = p\\) so an estimator of the PMASE would be \\[\\begin{equation} C_{p} = \\frac{1}{n}\\sum_{i=1}^{n}\\{ Y_{i} - \\hat{m}_{h_{n}}(x_{i}) \\}^{2} + \\frac{2\\hat{\\sigma}^{2}}{n}p \\nonumber \\end{equation}\\] For this reason, \\(\\textrm{tr}( \\mathbf{A}_{h_{n}} )\\) is often referred to as the “degrees of freedom” of a nonparametric estimator of the form \\(\\hat{\\mathbf{m}} = \\mathbf{A}_{h_{n}}\\mathbf{Y}\\). The main drawback of the \\(C_{p}\\) statistic is that it requires an estimate of the residual variance \\(\\sigma^{2}\\). So, if we choose a fairly small bandwidth \\(\\tilde{h}_{n}\\) so that the bias is close to zero, we could use the following estimate of \\(\\sigma^{2}\\) \\[\\begin{equation} \\hat{\\sigma}^{2}( \\tilde{h}_{n} ) = \\frac{ \\sum_{i=1}^{n}\\{ Y_{i} - \\hat{m}_{\\tilde{h}_{n}}(x_{i}) \\}^{2} }{ n - 2\\textrm{tr}(\\mathbf{A}_{\\tilde{h}_{n}}) + \\textrm{tr}(\\mathbf{A}_{\\tilde{h}_{n}}\\mathbf{A}_{\\tilde{h_{n}}}^{T}) } \\nonumber \\end{equation}\\] 11.4.3 Leave-one-out Cross Validation Similar to the way we defined a leave-on-out density estimate in Chapter 8, we will define the leave-one-out estimate of the regression function at \\(x\\) as: \\[\\begin{equation} \\hat{m}_{h_{n},-i}(x) - \\textrm{ estimate of $m(x)$ found by using all data except $(Y_{i}, x_{i})$.} \\end{equation}\\] The leave-one-out cross validation (LOO-CV) estimate of the PMASE is defined to be \\[\\begin{equation} \\textrm{LOOCV}(h_{n}) = \\frac{1}{n}\\sum_{i=1}^{n} \\{ Y_{i} - \\hat{m}_{h_{n}, -i}(x_{i}) \\}^{2} \\nonumber \\end{equation}\\] The intuition here is that; because the estimate \\(\\hat{m}_{h_{n}, -i}(x_{i})\\) is computed without the \\(i^{th}\\) observation, \\(Y_{i}\\) plays the role of a “future observation” (relative to the dataset that does not contain \\(Y_{i}\\)). Hence, \\(\\{ Y_{i} - \\hat{m}_{h_{n}, -i}(x_{i}) \\}^{2}\\) should be a sensible replacement for the unobservable \\(\\{ Y_{i}&#39; - \\hat{m}_{h_{n}}(x_{i}) \\}^{2}\\). While we could compute \\(\\textrm{LOOCV}(h_{n})\\) by computing \\(\\hat{m}_{h_{n}, -i}(x_{i})\\) separately for \\(i = 1,\\ldots,n\\), there is a much more efficient way of computing \\(\\textrm{LOOCV}(h_{n})\\). We are assuming that \\(\\hat{m}_{h_{n}}(x)\\) can be represented as a linear combination of the responses \\[\\begin{equation} \\hat{m}_{h_{n}}(x) = \\sum_{j=1}^{n} a_{j}^{h_{n}}(x)Y_{j}, \\nonumber \\end{equation}\\] where we can think of \\(a_{j}^{h_{n}}(x)\\) as weights that sum to \\(1\\). If we did not use \\(Y_{i}\\) to compute \\(\\hat{m}_{h_{n}}(x)\\), this estimate would look like \\[\\begin{equation} \\hat{m}_{h_{n}, -i}(x) = \\sum_{j=1}^{n} a_{j,-i}^{h_{n}}(x)Y_{j} \\end{equation}\\] where \\[\\begin{equation} a_{j,-i}^{h_{n}}(x) = \\begin{cases} 0 &amp; \\textrm{ if } j = i \\nonumber \\\\ \\frac{ a_{j}^{h_{n}}(x)}{ \\sum_{k \\neq i} a_{k}^{h_{n}}(x) } \\nonumber \\end{cases} \\end{equation}\\] If you want to better convince yourself that the above formula for \\(a_{j,-i}^{h_{n}}(x_{i})\\) is true, try an example using the Nadaraya-Watson estimator with \\(n=3\\). Because \\(\\sum_{k \\neq i} a_{k}^{h_{n}}(x_{i}) = 1 - a_{i}^{h_{n}}( x_{i} )\\), we can express \\(Y_{i} - \\hat{m}_{h_{n},-i}(x_{i})\\) as \\[\\begin{eqnarray} Y_{i} - \\hat{m}_{h_{n}, -i}(x_{i}) &amp;=&amp; Y_{i} - \\sum_{j \\neq i}^{n} a_{j,-i}^{h_{n}}(x_{i})Y_{j} \\nonumber \\\\ &amp;=&amp; Y_{i} - \\frac{1}{1 - a_{i}^{h_{n}}( x_{i} ) } \\sum_{j \\neq i}^{n} a_{j}^{h_{n}}(x_{i})Y_{j} \\nonumber \\\\ &amp;=&amp; Y_{i} - \\Big[ \\frac{1}{1 - a_{i}^{h_{n}}( x_{i} ) } \\sum_{j = 1}^{n} a_{j}^{h_{n}}(x_{i})Y_{j} \\Big] + \\frac{a_{j}^{h_{n}}(x_{i})Y_{i} }{1 - a_{i}^{h_{n}}( x_{i} ) } \\nonumber \\\\ &amp;=&amp; \\frac{Y_{i} - \\hat{m}_{h_{n}}(x_{i}) }{1 - a_{i}^{h_{n}}( x_{i} ) } \\tag{11.5} \\end{eqnarray}\\] Using (11.5), we can re-write the LOOCV estimate as \\[\\begin{equation} \\textrm{LOOCV}(h_{n}) = \\frac{1}{n}\\sum_{i=1}^{n} \\Big( \\frac{ Y_{i} - \\hat{m}_{h_{n}}(x_{i})}{ 1 - a_{i}^{h_{n}}( x_{i} ) } \\Big)^{2} \\nonumber \\end{equation}\\] where \\(a_{i}^{h_{n}}(x_{i})\\) are just the diagonal elements of our original matrix \\(\\mathbf{A}_{h_{n}}\\). 11.4.4 Example: Choosing the Best Bin Width for the Local Average Estimator. Cp Statistic The diagonal entries of the \\(\\mathbf{A}_{h_{n}}\\) matrix for the local average estimator are \\(1/n_{h_{n}}(x_{1}), \\ldots, 1/n_{h_{n}}(x_{n})\\). So, the “degrees of freedom” for the local average estimator is \\[\\begin{equation} \\textrm{tr}( \\mathbf{A}_{h_{n}}) = \\sum_{i=1}^{n} \\frac{1}{n_{h_{n}}(x_{i}) } \\end{equation}\\] Notice that if we choose \\(h_{n}\\) large enough so that \\(n_{h_{n}}(x_{i}) = n\\) for all \\(x_{i}\\), then the degrees of freedom is equal to \\(1\\). The \\(C_{p}\\) statistic for the local average estimator is \\[\\begin{equation} \\frac{1}{n}\\sum_{i=1}^{n} \\{ Y_{i} - \\hat{m}_{h_{n}}(x_{i}) \\}^{2} + \\frac{ 2\\hat{\\sigma}^{2} }{n}\\sum_{i=1}^{n} \\frac{1}{n_{h_{n}}(x_{i}) } \\nonumber \\end{equation}\\] Let’s try to compute \\(C_{p}(h_{n})\\) for the bonedat dataset. The first step is to write a function that computes the \\(n_{h_{n}}(x_{i})\\) for a given value of \\(h_{n}\\). This will allow us to find the degrees of freedom and will also be helpful later when computing LOOCV. NumInBins &lt;- function(hh, xx) { ## This function returns a vector of length n ## Elements of this vector will be: n_[h_n](x_1), n_[h_n](x_2), ... n &lt;- length(xx) num.bin &lt;- numeric(n) for(k in 1:n) { num.bin[k] &lt;- sum(xx &gt; xx[k] - hh &amp; xx &lt; xx[k] + hh) } return(num.bin) } We also want a function that returns the vector with elements \\(\\hat{m}_{h_{n}}(x_{1}), \\hat{m}_{h_{n}}(x_{2}) , \\ldots \\hat{m}_{h_{n}}(x_{n})\\). MyLocAvgEst &lt;- function(xx, yy, hh) { n &lt;- length(xx) m.hat.loc &lt;- numeric(n) for(k in 1:n) { in.bin &lt;- xx &gt; xx[k] - hh &amp; xx &lt; xx[k] + hh m.hat.loc[k] &lt;- mean(yy[in.bin]) } return(m.hat.loc) } The final step is to compute an estimate of \\(\\sigma^{2}\\). Using the estimate \\[\\begin{equation} \\hat{\\sigma}^{2}( \\tilde{h}_{n} ) = \\frac{ \\sum_{i=1}^{n}\\{ Y_{i} - \\hat{m}_{\\tilde{h}_{n}}(x_{i}) \\}^{2} }{ n - 2\\textrm{tr}(\\mathbf{A}_{\\tilde{h}_{n}}) + \\textrm{tr}(\\mathbf{A}_{\\tilde{h}_{n}}\\mathbf{A}_{\\tilde{h_{n}}}^{T}) } \\end{equation}\\] that we mentioned before with \\(\\tilde{h}_{n} = 0.1\\), I got a an estimate of \\(\\sigma^{2}\\) which was quite close to \\(0.0015\\) sigsq.est &lt;- 0.0015 Now, we are ready to compute the \\(C_{p}\\) statistic. We will compute \\(C_{p}(h_{n})\\) for \\(h_{n} = 0.01, 0.11, \\ldots, 10.01\\). This can be done with the following code: hseq &lt;- seq(.01, 10.01, by=.1) ngrid &lt;- length(hseq) n &lt;- length(bonedat$age) Cp &lt;- numeric(ngrid) for(k in 1:ngrid) { m.hat &lt;- MyLocAvgEst(bonedat$age, bonedat$spnbmd, hseq[k]) dfval &lt;- sum(1/NumInBins(hseq[k], bonedat$age)) Cp[k] &lt;- mean( (bonedat$spnbmd - m.hat)^2 ) + (2*sigsq.est*dfval)/n } We can plot the values of \\(C_{p}(h_{n})\\) vs. \\(h_{n}\\) to roughly see where the minimum value is. From the graph, it looks to be slighly less than \\(1\\). plot(hseq, Cp, ylim=c(0.001,.003), main=&quot;Bone Data: Cp Stat for Loc. Avg. Est.&quot;, xlab=&quot;hn&quot;, ylab=&quot;Cp&quot;) lines(hseq, Cp) More precisely, the value of \\(h_{n}\\) from our sequence which has the smallest value of \\(C_{p}(h_{n})\\) is \\(0.81\\). hseq[which.min(Cp)] ## [1] 0.81 LOOCV We can use the functions that we have written to compute \\(\\textrm{LOOCV}(h_{n})\\). It is useful to notice that \\(1 - a_{i}^{h_{n}}( x_{i} ) = 1 - 1/n_{h_{n}}(x_{i})\\) using the notation we used in the description of the LOOCV. R code to compute \\(\\textrm{LOOCV}(h_{n})\\) at the same sequence of \\(h_{n}\\) values used for the \\(C_{p}\\) statistic is given below: LOOCV &lt;- numeric(ngrid) for(k in 1:ngrid) { m.hat &lt;- MyLocAvgEst(bonedat$age, bonedat$spnbmd, hseq[k]) n.hn &lt;- NumInBins(hseq[k], bonedat$age) dd &lt;- 1 - 1/n.hn LOOCV[k] &lt;- mean( ((bonedat$spnbmd - m.hat)/dd)^2 ) } We can plot the values of \\(\\textrm{LOOCV}(h_{n})\\) vs. \\(h_{n}\\) to roughly see where the minimum value is. plot(hseq, LOOCV, ylim=c(0.001,.003), main=&quot;Bone Data: LOOCV Stat for Loc. Avg. Est.&quot;, xlab=&quot;hn&quot;, ylab=&quot;LOOCV&quot;) lines(hseq, LOOCV) The value of \\(h_{n}\\) from our sequence which has the smallest value of \\(\\textrm{LOOCV}(h_{n})\\) is \\(0.81\\). hseq[which.min(LOOCV)] ## [1] 0.81 11.5 Additional functions in R The R functions lowess and loess are widely used functions for smoothing via local regression. loess is basically an expanded version of lowess. The function loess has more options than the lowess function and is written to resemble the lm in function in R. Note that loess and lowess have different default settings for some of the model fitting parameters so they can differ somewhat in the values they return unless you set these parameters to be equal. loess allows for multivariate covariates \\(\\mathbf{x}_{i}\\) while lowess does not. lowess does local quadratic and local linear regression. The format of the lowess function is the following: loess(formula, data, span) formula - usally of the form y ~ x if using a single response vector y and covariate x data - the dataset from which y and x are drawn from span - the “span” of the smoother. This can be thought of as playing the role of the bandwidth. Default of span \\(= \\alpha\\) is set to \\(0.75\\). Usually, \\(0 &lt; \\alpha &lt; 1\\). degree - the degree of the polynomial used for local regression. The default is set to \\(2\\). The loess function will return two vectors $x and $fitted. The $x is just the vector of covariates from the original data, and the $fitted vector is the value of the estimated regression function \\(\\hat{m}( x_{i} )\\) at these points. The lowess function performs local linear regression with a few extra steps included to make the fitting procedure more robust. For weights \\(W_{i}^{\\alpha}(x)\\) in the local linear regression problem, loess and lowess uses the following tri-cube function \\[\\begin{equation} W_{i}^{\\alpha}(x) = \\begin{cases} \\Bigg( 1 - \\Big( \\frac{| x - x_{i}| }{| x - x_{(q)}| } \\Big)^{3} \\Bigg)^{3} &amp; \\text{ if } |x - x_{i}| \\leq |x - x_{(q)}| \\\\ 0 &amp; \\textrm{ otherwise } \\end{cases} \\end{equation}\\] The weights \\(W_{i}^{\\alpha}(x)\\) here play the same role as \\(K( \\tfrac{x - x_{i}}{ h_{n} } )\\) did in our description of local linear regression in Section 10.3. Here, \\(x_{(q)}\\) is the \\(\\lfloor \\alpha \\times n \\rfloor\\) furthest observations away from \\(x\\), and \\(\\lfloor \\alpha \\times n \\rfloor\\) denotes \\(\\alpha n\\) rounded down to the nearest integer. So, values of the span \\(\\alpha\\) which are closer to \\(0\\) mean that you are using a smaller number of observations when performing each local regression while values close to \\(1\\) mean that nearly all the observations receive positive weight when performing each local regression. After fitting a local regression with weights \\(W_{i}^{\\alpha}(x)\\), lowess actually does an additional local regression with updated weights that reduce the influence of outliers. loess will do the same thing if family is set to “symmetric” rather than “gaussian”. Let’s try plotting a loess fit using the bone data. We will set span = 2/3 instead of 3/4 bone.low.fit &lt;- loess(spnbmd ~ age, data=bonedat, span=2/3) plot(bone.low.fit, ylab=&quot;Relative Change in Bone MD&quot;, xlab=&quot;Age&quot;, main=&quot;Bone Data: Lowess Fit&quot;, las=1, pch=16) ## plot a line of the the fitted values vs. x. Need to sort ## the x_i&#39;s first though if we want a nice looking line: lines(bone.low.fit$x[order(bone.low.fit$x)], bone.low.fit$fitted[order(bone.low.fit$x)], col=&quot;red&quot;, lwd=3) abline(0,0, lty=2) If you want to change the settings for lowess and loess so that they are using the exact same fitting procedure, you can use the following approach: lowess.fit &lt;- lowess(x=bonedat$age, y=bonedat$spnbmd, iter=3, delta=0, f=2/3) loess.fit &lt;- loess(spnbmd ~ age, data=bonedat, span=2/3, degree=1, family=&quot;symmetric&quot;, iterations=4, surface=&quot;direct&quot;) The locpoly function from the KernSmooth package implements local polynomial regression as it was described in Section 10.3. locpoly(x, y, degree, kernel = &quot;normal&quot;, bandwidth) You can vary the degree of the polynomical used in the local polynomial regression with the degree argument. For local linear regression, you should get the same answer as our function MyLocLinear written in Section 10.3 if you use degree=1 with the locpoly function: library(KernSmooth) ## KernSmooth 2.23 loaded ## Copyright M. P. Wand 1997-2009 locpoly.fit &lt;- locpoly(x = bonedat$age, y=bonedat$spnbmd, degree=1, bandwidth=0.5) mylocpoly.fit &lt;- MyLocLinear(x=bonedat$age, y=bonedat$spnbmd, bandwidth=0.5, x.points=locpoly.fit$x) plot(locpoly.fit$x, locpoly.fit$y, main=&quot;Local Linear Regression with locpoly and MyLocLinear&quot;, las=1, xlab=&quot;age&quot;, ylab=&quot;Relative Change&quot;) lines(locpoly.fit$x, mylocpoly.fit, col=&quot;red&quot;, lwd=2) The supsmu function implements Friedman’s “super smoother” (Friedman (1984)). supsmu(x, y, span=&quot;cv&quot;, bass=0) x - vector of covariate values. y - vector of responses. span - the “span” of the smoother. As with loess, this is the fraction of observations included when estimating the regression function at a particular point. The best span is chosen through cross-validation. bass - tuning parameter to further control the smoothness of the fitted curve. The smallest value is \\(0\\) (lowest level of smoothness). The largest value is \\(10\\) (most smoothness). The supsmu function just returns a list with components $x and $y. The $x component will be a sorted vector of the inputted x values and the $y component will contain the corresponding estimates of the regression function. 11.6 Multivariate Problems All of the methods discussed here can be directly extended to the case of multivariate covariates \\(\\mathbf{x}_{i} \\in \\mathbb{R}^{p}\\) where \\(\\mathbf{x}_{i} = (x_{i1}, \\ldots, x_{ip})\\). For the methods that use a kernel function, we just use the Euclidean distance between two points in the kernel function. For example, with the Gaussian kernel for the Nadaraya-Watson estimator, we would use the following weights when estimating the regression function at \\(\\mathbf{x} \\in \\mathbb{R}^{p}\\) \\[\\begin{equation} K\\Big( \\frac{ \\mathbf{x} - \\mathbf{x}_{i} }{h_{n}} \\Big) = \\frac{1}{\\sqrt{2\\pi}h_{n}}\\exp\\Big\\{ -\\frac{||\\mathbf{x} - \\mathbf{x}_{i}||^{2} }{2h_{n}^{2}} \\Big\\} = \\frac{1}{\\sqrt{2\\pi}h_{n}}\\exp\\Big\\{ -\\frac{1}{2h_{n}^{2}}\\sum_{j=1}^{p} (x_{j} - x_{ij})^{2} \\Big\\} \\nonumber \\end{equation}\\] If we wanted to do k-nearest neighbors regression to estimate the regression function at \\(\\mathbf{x}\\), we would just pick the k closest observation in terms of the distance \\(|| \\mathbf{x} - \\mathbf{x}_{i}||^{2}\\). Similarly, if we wanted to compute the local average estimator in \\(\\mathbf{R}^{p}\\), we would use \\[\\begin{equation} \\hat{m}_{h_{n}}^{loc}( \\mathbf{x} ) = \\frac{1}{ n_{h_{n}}( \\mathbf{x} )}\\sum_{i=1}^{n} Y_{i}I\\big( || \\mathbf{x}_{i} - \\mathbf{x} || &lt; h_{n} \\big), \\nonumber \\end{equation}\\] where \\(n_{h_{n}}( \\mathbf{x} ) = \\sum_{i=1}^{n} I\\big( || \\mathbf{x}_{i} - \\mathbf{x} || &lt; h_{n} \\big)\\). The performance of kernel and local regression methods can degrade quickly as we move to higher dimensions. The convergence rate of the estimated regression function to the true regression function slows substantially as we increase \\(p\\). “Curse of dimensionality” - need very large datasets to have a sufficient number of observations near a given point \\(\\mathbf{x}\\). The methods discussed here are most commonly used for \\(p = 1\\) or maybe \\(p = 2\\) or \\(p = 3\\). For higher dimensions, modifications of these methods could be useful. For example: Incorporating correlation between the covariates in the kernel function. Modeling the regression function as an additive model \\(m(\\mathbf{x}) = m_{1}(x_{1}) + \\ldots + m_{j}(x_{p})\\) and using local regression for each function \\(m_{j}(x)\\). “Sparsity”: try to first discard unimportant covariates and estimate the regression function using a smaller subset of the covariates. 11.7 Additional Reading Additional reading which covers the material discussed in this chapter includes: Chapter 4 from Härdle et al. (2012) Chapter 5 from Wasserman (2006) 11.8 Exercises Exercise 11.1 Let \\[\\begin{equation} \\hat{\\mathbf{m}} = \\big( \\hat{m}_{h_{n}}^{R}(x_{1}), \\ldots, \\hat{m}_{h_{n}}^{R}(x_{n}) \\big) \\nonumber \\end{equation}\\] denote the vector of “fitted values” from a regressogram estimate that has \\(D_{n}\\) bins. If \\(\\mathbf{Y} = (Y_{1}, \\ldots, Y_{n})\\), show that you can express \\(\\hat{\\mathbf{m}}\\) as \\[\\begin{equation} \\hat{\\mathbf{m}} = \\mathbf{A}\\mathbf{Y}, \\nonumber \\end{equation}\\] for an appropriately chosen \\(n \\times n\\) matrix \\(\\mathbf{A}\\). What is the value of \\(\\textrm{tr}(\\mathbf{A})\\)? Exercise 11.2 Suppose \\(n=6\\) and that we have the following covariate values and responses \\[\\begin{eqnarray} (x_{1}, x_{2}, x_{3}, x_{4}, x_{5}, x_{6}) &amp;=&amp; (1/7, 2/7, 3/7, 4/7, 5/7, 6/7) \\nonumber \\\\ (Y_{1}, Y_{2}, Y_{3}, Y_{4}, Y_{5}, Y_{6}) &amp;=&amp; (1.4, 0.7, 1.1, 1.3, 0.9, 1.7) \\nonumber \\end{eqnarray}\\] Compute the local average estimate of the regression function at \\(x = 0.25\\) and \\(x=0.75\\) assuming that \\(h_{n} = 1/2\\). Compute the k nearest neighbors estimate of the regression function at \\(x = 0.25\\) and \\(x = 0.75\\) assuming that \\(k = 2\\). Exercise 11.3 Suppose we define an estimator \\(\\tilde{m}_{h_{n}}(x)\\) of the regression function as \\[\\begin{eqnarray} \\tilde{m}_{h_{n}}(x) &amp;=&amp; \\hat{\\beta}_{0x} \\quad \\textrm{ where } \\nonumber \\\\ \\hat{\\beta}_{0x} &amp;=&amp; \\textrm{argmin}_{\\beta_{0x}} \\sum_{i=1}^{n}\\{ Y_{i} - \\beta_{0x} \\}^{2}K\\Big( \\frac{x - x_{i}}{h_{n}} \\Big) \\nonumber \\end{eqnarray}\\] Show that \\(\\tilde{m}_{h_{n}}(x) = \\hat{m}_{h_{n}}^{NW}(x)\\). Exercise 11.4 Suppose the \\(n \\times n\\) matrix \\(\\mathbf{A}_{h_{n}}\\) satifies \\(\\mathbf{A}_{h_{n}}\\mathbf{m} = \\mathbf{m}\\). Show that \\[\\begin{equation} \\frac{\\mathbf{Y}^{T}(\\mathbf{I} - \\mathbf{A}_{h_{n}})^{T}(\\mathbf{I} - \\mathbf{A}_{h_{n}})\\mathbf{Y} }{ n - 2\\textrm{tr}( \\mathbf{A}_{h_{n}}) + \\textrm{tr}(\\mathbf{A}_{h_{n}}\\mathbf{A}_{h_{n}}^{T}) } \\nonumber \\end{equation}\\] is an unbiased estimator of \\(\\sigma^{2}\\). References "],
["inference-for-regression.html", "Chapter 12 Splines and Penalized Regression 12.1 Introduction 12.2 Piecewise Linear Estimates with Continuity (Linear Splines) 12.3 Cubic Splines and Regression with Splines 12.4 Smoothing Splines 12.5 Knot/Penalty Term Selection for Splines 12.6 Fitting Smoothing Splines in R", " Chapter 12 Splines and Penalized Regression 12.1 Introduction In this chapter, we will focus on using basis functions for estimating the regression function. That is, we will look at regression function estimates of the form \\[\\begin{equation} \\hat{m}(x) = \\hat{\\beta}_{0}\\varphi_{0}(x) + \\sum_{j=1}^{p} \\hat{\\beta}_{j}\\varphi_{j}(x) \\nonumber \\end{equation}\\] where \\(\\varphi_{0}(x), \\varphi_{1}(x), \\ldots, \\varphi_{p}(x)\\) will be referred to as basis functions. We will usually either ignore \\(\\varphi_{0}(x)\\) or assume that \\(\\varphi_{0}(x) = 1\\). If you use a relatively large number of appropriately chosen basis functions, you can represent even quite complicated functions with some linear combination of the basis functions. Examples Basis functions for a straight-line linear regression \\[\\begin{equation} \\varphi_{0}(x) = 1 \\qquad \\varphi_{1}(x) = x \\nonumber \\end{equation}\\] Basis functions for a polynomial regression with degree 3 \\[\\begin{equation} \\varphi_{0}(x) = 1 \\qquad \\varphi_{1}(x) = x \\qquad \\varphi_{2}(x) = x^{2} \\qquad \\varphi_{3}(x) = x^{3} \\nonumber \\end{equation}\\] Basis functions for a regressogram with bins \\([l_{k}, u_{k})\\), \\(k = 1, \\ldots, K\\): \\[\\begin{equation} \\varphi_{k}(x) = I\\big( l_{k} \\leq x &lt; u_{k} \\big), \\quad k = 1, \\ldots, K \\nonumber \\end{equation}\\] 12.1.1 Regressogram (Piecewise Constant Estimate) Let’s consider the regressogram again. The regressogram estimate could be written as \\[\\begin{equation} \\hat{m}_{h_{n}}^{R}(x) = \\sum_{k=1}^{D_{n}} a_{k, h_{n}}\\varphi_{k}(x) = \\sum_{k=1}^{D_{n}} a_{k,h_{n}} I\\big( x \\in B_{k} \\big) \\end{equation}\\] where the coefficents \\(a_{k, h_{n}}\\) are given by \\[\\begin{equation} a_{k, h_{n}} = \\frac{1}{ n_{k,h_{n}} } \\sum_{i=1}^{n} Y_{i}I\\big( x_{i} \\in B_{k} \\big) \\nonumber \\end{equation}\\] We can think of the regressogram as a basis function estimate with the basis functions \\[\\begin{equation} \\varphi_{1}(x) = I\\big( x \\in B_{1} \\big), \\ldots, \\varphi_{D_{n}}(x) = I\\big( x \\in B_{D_{n}} \\big) \\nonumber \\end{equation}\\] The regressogram estimate will be a piecewise constant function that is constant within each of the bins. Figure 8.1: Basis functions for a regressogram with the following 3 bins: [0,1/3), [1/3, 2/3), [2/3, 1) Figure 12.1: Regressogram estimate of a regression function with 3 bins. 12.1.2 Piecewise Linear Estimates Instead of a piecewise constant estimate of \\(m(x)\\), we could use an estimate which is piecewise linear by using the following \\(2p\\) basis functions \\[\\begin{eqnarray} \\varphi_{1}(x) &amp;=&amp; I(x \\in B_{1}) \\nonumber \\\\ &amp;\\vdots&amp; \\nonumber \\\\ \\varphi_{p}(x) &amp;=&amp; I(x \\in B_{p}) \\nonumber \\\\ \\varphi_{p+1}(x) &amp;=&amp; x I(x \\in B_{1}) \\nonumber \\\\ &amp;\\vdots&amp; \\nonumber \\\\ \\varphi_{2p}(x) &amp;=&amp; x I(x \\in B_{p}) \\nonumber \\end{eqnarray}\\] if we now let \\(p = D_{n}\\) denote the number of “bins”. The figure below shows an example of a regression function that is piecewise linear with 3 different bins. While a piecewise linear model is perhaps a more flexible method than the regressogram, the piecwise linear model will still have big jumps at the bin boundaries and have an overall unpleasant appearance. Figure 12.2: Example of a regression function estimate that is piecewise linear within 3 bins. 12.1.3 Piecewise Cubic Estimates If we wanted to allow for more flexible forms of the regression function estimate within each bin we could fit a higher order polynomial model within each bin. That is, the regression function estimate within the \\(k^{th}\\) bin will have the form \\[\\begin{equation} \\hat{m}(x)I(x \\in B_{k}) = \\hat{\\beta}_{0k} + \\hat{\\beta}_{1k}x + \\hat{\\beta}_{2k}x^{2} + \\hat{\\beta}_{3k}x^{3} \\nonumber \\end{equation}\\] To fit a piecewise cubic model with \\(p\\) bins, we would need the following \\(4p\\) basis functions \\[\\begin{eqnarray} \\varphi_{1}(x) &amp;=&amp; I(x \\in B_{1}), \\ldots, \\varphi_{p}(x) = I(x \\in B_{p}) \\nonumber \\\\ \\varphi_{p+1}(x) &amp;=&amp; xI(x \\in B_{1}), \\ldots, \\varphi_{2p}(x) = xI(x \\in B_{p}) \\nonumber \\\\ \\varphi_{2p+1}(x) &amp;=&amp; x^{2}I(x \\in B_{1}), \\ldots, \\varphi_{3p}(x) = x^{2}I(x \\in B_{p}) \\nonumber \\\\ \\varphi_{3p+1}(x) &amp;=&amp; x^{3}I(x \\in B_{1}), \\ldots, \\varphi_{4p}(x) = x^{3}I(x \\in B_{p}) \\nonumber \\\\ \\end{eqnarray}\\] Figure 12.3: Example of a regression function estimate that is piecewise cubic within 3 bins. 12.2 Piecewise Linear Estimates with Continuity (Linear Splines) In the spline world, one typically talks about “knots” rather than “bins”. You can think of knots as the dividing points between the bins. We will let \\(u_{1} &lt; u_{2} &lt; \\ldots &lt; u_{q}\\) denote the choice of knots. The bins corresponding to this set of knots would then be \\(B_{1} = (-\\infty, u_{1}), B_{2} = [u_{1}, u_{2}), B_{3} = [u_{2}, u_{3}), \\ldots, B_{q+1} = [u_{q}, \\infty)\\). In other words, \\(q\\) knots defines \\(B_{q+1}\\) “bins” of the form \\(B_{k} = [u_{k-1}, u_{k+1})\\), for \\(k = 2, \\ldots, q\\). Let’s return to the piecewise linear estimate shown in Figure 12.2. This has two knots \\(u_{1} = 1/3\\) and \\(u_{2} = 2/3\\) and hence \\(3\\) bins. Also, this piecewise linear model has \\(6\\) parameters. If we let \\((\\beta_{0k}, \\beta_{1k})\\) denote the intercept and slope parameters for the \\(k^{th}\\) bin, there are \\(6\\) parameters in total because we have \\(3\\) bins, and we could write a piecewise linear model as \\[\\begin{equation} m(x) = \\begin{cases} \\beta_{01} + \\beta_{11}x &amp; \\text{ if } x &lt; u_{1} \\nonumber \\\\ \\beta_{02} + \\beta_{12}x &amp; \\text{ if } u_{1} \\leq x &lt; u_{2} \\nonumber \\\\ \\beta_{03} + \\beta_{13}x &amp; \\text{if } x \\geq u_{2} \\end{cases} \\end{equation}\\] We can make the estimated regression function look better by ensuring that it is continuous and does not have discontinuities at the knots. To make the estimated regression curve continuous, we just need to make sure it is continuous at the knots. That is, the regression coefficients need to satisfy the following two constraints: \\[\\begin{equation} \\beta_{01} + \\beta_{11}u_{1} = \\beta_{02} + \\beta_{12}u_{1} \\qquad \\textrm{and} \\qquad \\beta_{02} + \\beta_{12}u_{2} = \\beta_{03} + \\beta_{03}u_{2} \\nonumber \\end{equation}\\] Because we have two linear constraints, we should expect that the number of “free parameters” in a piecewise linear model with continuity constraints should equal \\(6 - 2 = 4\\). Indeed, if we use the fact that under the continuity constraints: \\(\\beta_{02} = \\beta_{01} + \\beta_{11}u_{1} - \\beta_{12}u_{1}\\) and \\(\\beta_{03} = \\beta_{02} + \\beta_{12}u_{2} - \\beta_{13}u_{2}\\), then we can rewrite the piecewise linear model as \\[\\begin{equation} m(x) = \\begin{cases} \\beta_{01} + \\beta_{11}x &amp; \\text{ if } x &lt; u_{1} \\nonumber \\\\ \\beta_{01} + \\beta_{11}x + (\\beta_{12} - \\beta_{11})(x - u_{1}) &amp; \\text{ if } u_{1} \\leq x &lt; u_{2} \\nonumber \\\\ \\beta_{01} + \\beta_{11}x + (\\beta_{12} - \\beta_{11})(x - u_{1}) + (\\beta_{13} - \\beta_{12})(x - u_{2}) &amp; \\text{ if } x \\geq u_{2} \\end{cases} \\end{equation}\\] We can rewrite the above more compactly as: \\[\\begin{equation} m(x) = \\beta_{01} + \\beta_{11}x + (\\beta_{12} - \\beta_{11})(x - u_{1})_{+} + (\\beta_{13} - \\beta_{12})(x - u_{2})_{+} \\nonumber \\end{equation}\\] where \\((x - u_{1})_{+} = \\max\\{ x - u_{1}, 0\\}\\). So, the functions \\(\\varphi_{0}(x) = 1\\), \\(\\varphi_{1}(x) = x\\), \\(\\varphi_{2}(x) = (x - u_{1})_{+}\\), \\(\\varphi_{3}(x) = (x - u_{2})_{+}\\) form a basis for the set of piecewise linear function with continuity constraints and knots \\(u_{1}\\) and \\(u_{2}\\). In general, a linear spline with \\(q\\) knots \\(u_{1} &lt; u_{2} &lt; \\ldots &lt; u_{q}\\) is a funcion \\(m(x)\\) that can be expressed as \\[\\begin{equation} m(x) = \\beta_{0} + \\beta_{1}x + \\sum_{k=1}^{q} \\beta_{k+1} (x - u_{k})_{+} \\nonumber \\end{equation}\\] So, the following \\(q + 2\\) functions form a basis for the set of linear splines with knots \\(u_{1} &lt; u_{2} &lt; \\ldots &lt; u_{q}\\) \\[\\begin{eqnarray} \\varphi_{0}(x) &amp;=&amp; 1 \\nonumber \\\\ \\varphi_{1}(x) &amp;=&amp; x \\nonumber \\\\ \\varphi_{2}(x) &amp;=&amp; (x - u_{1})_{+} \\nonumber \\\\ \\varphi_{3}(x) &amp;=&amp; (x - u_{2})_{+} \\nonumber \\\\ &amp;\\vdots&amp; \\nonumber \\\\ \\varphi_{q + 1}(x) &amp;=&amp; (x - u_{q})_{+} \\nonumber \\end{eqnarray}\\] Hence, if we want to fit a linear spline with \\(q\\) knots, we will need to estimate \\(q + 2\\) parameters. Figure 12.4: A linear spline with knots at 1/3 and 2/3. A linear spline is a piecewise linear function that is constrained to be continuous. 12.3 Cubic Splines and Regression with Splines 12.3.1 Example: Smooth Piecewise Cubic Model with 2 Knots Let us go back to the piecewise cubic model shown in Figure 12.3. This model assumes that the regression function is of the form: \\[\\begin{equation} m(x) = \\begin{cases} \\beta_{01} + \\beta_{11}x + \\beta_{21}x^{2} + \\beta_{31}x^{3} &amp; \\text{ if } 0 &lt; x &lt; u_{1} \\nonumber \\\\ \\beta_{02} + \\beta_{12}x + \\beta_{22}x^{2} + \\beta_{32}x^{3} &amp; \\text{ if } u_{1} \\leq x &lt; u_{2} \\nonumber \\\\ \\beta_{03} + \\beta_{13}x + \\beta_{23}x^{2} + \\beta_{33}x^{3} &amp; \\text{ if } 1 &gt; x \\geq u_{2} \\end{cases} \\end{equation}\\] Notice that this model has 12 parameters. Like in the linear spline example, if we wanted to make this piecewise cubic model continuous at the knots \\(u_{1}\\) and \\(u_{2}\\) we would need to impose the following constraints on the coefficients \\(\\beta_{jk}\\): \\[\\begin{eqnarray} \\beta_{01} + \\beta_{11}u_{1} + \\beta_{21}u_{1}^{2} + \\beta_{31}u_{1}^{3} &amp;=&amp; \\beta_{02} + \\beta_{12}u_{1} + \\beta_{22}u_{1}^{2} + \\beta_{32}u_{1}^{3} \\nonumber \\\\ \\beta_{02} + \\beta_{12}u_{2} + \\beta_{22}u_{2}^{2} + \\beta_{21}u_{2}^{3} &amp;=&amp; \\beta_{03} + \\beta_{13}u_{2} + \\beta_{23}u_{2}^{2} + \\beta_{33}u_{2}^{3} \\tag{12.1} \\end{eqnarray}\\] However, only forcing the piecewise cubic model to be continuous is not enough if we want a smooth estimate for the regression function that will not have obvious changes at the knots. We actually need the first and the second derivatives of the function to be continuous if we want a function that is smooth and does not have changes at the knots that we can detect visually. So, for the example that we have in Figure 12.3 with the knots \\(u_{1}\\) and \\(u_{2}\\), we need to enforce the additional four constraints: \\[\\begin{eqnarray} \\beta_{11} + 2\\beta_{21}u_{1} + 3\\beta_{31}u_{1}^{2} &amp;=&amp; \\beta_{12} + 2\\beta_{22}u_{1} + 3\\beta_{32}u_{1}^{2} \\nonumber \\\\ \\beta_{12} + 2\\beta_{22}u_{2} + 3\\beta_{21}u_{2}^{2} &amp;=&amp; \\beta_{13} + 2\\beta_{23}u_{2} + 3\\beta_{33}u_{2}^{2} \\nonumber \\\\ 2\\beta_{21} + 6\\beta_{31}u_{1} &amp;=&amp; 2\\beta_{22} + 6\\beta_{32}u_{1} \\nonumber \\\\ 2\\beta_{22} + 6\\beta_{21}u_{2} &amp;=&amp; 2\\beta_{23} + 6\\beta_{33}u_{2} \\nonumber \\end{eqnarray}\\] Hence, the piecewise cubic model that has the two continuity constraints (12.1) and the four first and second derivative constraints will have \\(12 - 2 - 4 = 6\\) parameters in total. In a similar way to how we derived the basis for the linear spline with knots \\(u_{1}\\) and \\(u_{2}\\), you can show that the following \\(6\\) functions form a basis for the set of piecewise cubic functions with knots \\(u_{1}\\) and \\(u_{2}\\) that are continuous and have continuous first and second derivatives: \\[\\begin{eqnarray} \\varphi_{0}(x) &amp;=&amp; 1 \\qquad \\varphi_{1}(x) = x \\qquad \\varphi_{2}(x) = x^{2} \\qquad \\varphi_{3}(x) = x^{3} \\nonumber \\\\ \\varphi_{4}(x) &amp;=&amp; (x - u_{1})_{+}^{3} \\qquad \\varphi_{5}(x) = (x - u_{2})_{+}^{3} \\nonumber \\end{eqnarray}\\] 12.3.2 Cubic Splines The above example with knots \\(u_{1}\\) and \\(u_{2}\\) is an example of a cubic spline. Definition: A cubic spline with knots \\(u_{1} &lt; u_{2} &lt; \\ldots &lt; u_{q}\\) is a function \\(f(x)\\) such that \\(f(x)\\) is a cubic function over each of the intervals \\((-\\infty, u_{1}], [u_{1}, u_{2}], \\ldots, [u_{q-1}, u_{q}], [u_{q}, \\infty)\\). \\(f(x)\\), \\(f&#39;(x)\\), and \\(f&#39;&#39;(x)\\) are all continuous functions. One basis for the set of cubic splines with knots \\(u_{1} &lt; u_{2} &lt; \\ldots &lt; u_{q}\\) is the following truncated power basis which consists of \\(q + 4\\) basis functions: \\[\\begin{eqnarray} \\varphi_{0}(x) &amp;=&amp; 1 \\quad \\varphi_{1}(x) = x \\quad \\varphi_{2}(x) = x^{2} \\quad \\varphi_{3}(x) = x^{3} \\nonumber \\\\ \\varphi_{k+3}(x) &amp;=&amp; (x - u_{k})_{+}^{3}, \\quad k=1,\\ldots, q \\end{eqnarray}\\] A common basis for the set of cubic splines with knots \\(u_{1} &lt; u_{2} &lt; \\ldots &lt; u_{q}\\) is the B-spline basis. Using the B-spline basis functions is mathematically equivalent to using the truncated power basis functions. Mathematically, using either basis would give you the same fitted curve. However, there are computational advantages to using the B-spline basis functions, and using the B-spline seems to be much more common in software implementations of spline fitting procedures. For a cubic spline with knots \\(u_{1}, \\ldots, u_{q}\\), we will let the following \\(q + 4\\) denote the corresponding set of B-spline basis functions \\[\\begin{equation} \\varphi_{1, B}(x), \\ldots, \\varphi_{q+4, B}(x) \\nonumber \\end{equation}\\] 12.3.3 Estimating the Coefficients of a Cubic Spline When fitting a cubic spline, we assume that the knots \\(\\mathbf{u} = (u_{1}, u_{2}, \\ldots, u_{q})\\) are fixed first. Because the B-spline functions form a basis for the set of cubic splines with these knots, we can assume that our estimated regression function will have the form \\[\\begin{equation} m(x) = \\sum_{k=1}^{q + 4} \\beta_{k}\\varphi_{k,B}(x) \\nonumber \\end{equation}\\] To find the best coeffients \\(\\beta_{k}\\) in this cubic spline model, we will minimize the residual sum-of-residuals-squared criterion \\[\\begin{equation} \\sum_{i=1}^{n} \\Big( Y_{i} - \\hat{m}(x_{i}) \\Big)^{2} = \\sum_{i=1}^{n} \\Big( Y_{i} - \\sum_{k=1}^{q + 4} \\beta_{k}\\varphi_{k,B}(x_{i}) \\Big)^{2} \\end{equation}\\] Finding the coefficients \\(\\hat{\\beta}_{1}, \\ldots, \\hat{\\beta}_{q+4}\\) that minimize this sum-of-residuals-squared criterion can be viewed as solving a regression problem with response vector \\(\\mathbf{Y} = (Y_{1}, \\ldots, Y_{n})\\) and “design matrix” \\(\\mathbf{X}_{\\mathbf{u}}\\) \\[\\begin{equation} \\mathbf{X}_{\\mathbf{u}} = \\begin{bmatrix} \\varphi_{1, B}(x_{1}) &amp; \\varphi_{2, B}(x_{1}) &amp; \\ldots &amp; \\varphi_{q+4, B}(x_{1}) \\\\ \\varphi_{1, B}(x_{2}) &amp; \\varphi_{2, B}(x_{2}) &amp; \\ldots &amp; \\varphi_{q+4,B}(x_{2}) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\varphi_{1,B}(x_{n}) &amp; \\varphi_{2,B}(x_{n}) &amp; \\ldots &amp; \\varphi_{q+4,B}(x_{n}) \\end{bmatrix} \\nonumber \\end{equation}\\] When written in this form, the vector of estimated regression coeffients can be expressed as \\[\\begin{equation} \\begin{bmatrix} \\hat{\\beta}_{1} \\\\ \\hat{\\beta}_{2} \\\\ \\vdots \\\\ \\hat{\\beta}_{q + 4} \\end{bmatrix} = (\\mathbf{X}_{\\mathbf{u}}^{T}\\mathbf{X}_{\\mathbf{u}})^{-1}\\mathbf{X}_{\\mathbf{u}}^{T}\\mathbf{Y} \\nonumber \\end{equation}\\] and hence the vector of fitted values \\(\\hat{\\mathbf{m}} = \\big( \\hat{m}(x_{1}), \\ldots, \\hat{m}(x_{n}) \\big)\\) can be written as \\[\\begin{equation} \\hat{\\mathbf{m}} = \\mathbf{X}_{\\mathbf{u}}(\\mathbf{X}_{\\mathbf{u}}^{T}\\mathbf{X}_{\\mathbf{u}})^{-1}\\mathbf{X}_{\\mathbf{u}}^{T}\\mathbf{Y} \\nonumber \\end{equation}\\] 12.3.4 An example in R Regression splines can be fitted in R by using the splines package library(splines) The bs function from the splines package is useful for fitting a linear or cubic spline. This function generates the B-spline “design” matrix \\(\\mathbf{X}_{\\mathbf{u}}\\) described above. bs(x, df, knots, degree) x - vector of covariates values. This can also just be the name of a variable when bs is used inside the lm function. df - the “degrees of freedom”. For a cubic spline this is actually \\(q + 3\\) rather than \\(q + 4\\). If you just enter df, the bs function will pick the knots for you. knots - the vector of knots. If you don’t want to pick the knots, you can just enter a number for the df. degree - the degree of the piecewise polynomial. degree=1 for a linear spline and degree=3 for a cubic spline. As an example of what the bs function returns, suppose we input the vector of covariates \\((x_{1}, \\ldots, x_{10}) = (1, 2, \\ldots, 10)\\) with knots \\(u_{1} = 3.5\\) and \\(u_{2} = 6.5\\). The bs function will return the “design matrix” \\(\\mathbf{X}_{\\mathbf{u}}\\) for this setup without the intercept column. When degree is 3, the dimensions of \\(\\mathbf{X}_{\\mathbf{u}}\\) should be \\(10 \\times 6\\) (because in this case \\(q = 2\\)). So, the bs function will return a \\(10 \\times 5\\) matrix (because the first column of \\(\\mathbf{X}_{u}\\) is dropped by bs): xx &lt;- 1:10 Xu &lt;- bs(xx, knots=c(3.5, 6.5)) dim(Xu) ## [1] 10 5 head(Xu) ## 1 2 3 4 5 ## [1,] 0.00000 0.000 0.00000 0.000000 0 ## [2,] 0.60813 0.168 0.00808 0.000000 0 ## [3,] 0.45779 0.470 0.06465 0.000000 0 ## [4,] 0.17218 0.612 0.21463 0.000986 0 ## [5,] 0.03719 0.515 0.42131 0.026627 0 ## [6,] 0.00138 0.309 0.56631 0.123274 0 Let’s try an example with a linear spline to see how to use the bs function to fit a spline within the lm function. We will use the bone data again with age as the covariate. We will use the knots \\(\\mathbf{u} = (12, 15, 18, 21, 24)\\). bonedat &lt;- read.csv(&quot;~/Documents/STAT685Notes/Data/bone.csv&quot;) knot.seq &lt;- c(12, 15, 18, 21, 24) linspline.bone &lt;- lm(spnbmd ~ bs(age, knots=knot.seq, degree=1), data=bonedat) Because we are using \\(5\\) knots, based on our discussion in Section 12.2 we should expect that there should be \\(7\\) columns in the design matrix \\(\\mathbf{X}_{u}\\) for this linear spline model. You can check this by using the following R code: XX &lt;- model.matrix(linspline.bone) dim(XX) ## [1] 261 7 If you want to compute the estimated spline function \\(\\hat{m}(t_{j})\\) at a sequence of points \\(t_{1}, \\ldots, t_{l}\\), you can use the predict function on the fitted lm object. This is done with the following code for the points \\(9.5, 10, 10.5, 11, \\ldots, 24.5, 25\\). tt &lt;- seq(9.5, 25, by=0.5) plot(bonedat$age, bonedat$spnbmd, xlab=&quot;age&quot;, ylab=&quot;Relative Change in Bone MD&quot;, main=&quot;Bone Data: Fitted Linear Spline&quot;, las=1) lines(tt, predict(linspline.bone, data.frame(age=tt)), lwd=2) for(k in 1:5) { ## Plot vertical lines at the knots abline(v=knot.seq[k], lty=2) } Now let’s try fitting a cubic spline model to the bone data. The procedure is almost exactly the same as fitting the linear spline model. We only need to change degree=1 to degree=3 in the bs function. We will use the same knots as we did for the linear spline. knot.seq &lt;- c(12, 15, 18, 21, 24) cubspline.bone &lt;- lm(spnbmd ~ bs(age, knots=knot.seq, degree=3), data=bonedat) Because we are using \\(5\\) knots, we should expect that there will be \\(5 + 4 = 9\\) columns in the design matrix \\(\\mathbf{X}_{u}\\) for this cubic spline model. You can check this by using the following R code: XX &lt;- model.matrix(cubspline.bone) dim(XX) ## [1] 261 9 Using predict again, we will compute the estimated regression function \\(\\hat{m}(x)\\) at the points: \\(9.5, 9.6, \\ldots, 24.9, 25\\) and plot the result. tt &lt;- seq(9.5, 25, by=0.1) plot(bonedat$age, bonedat$spnbmd, xlab=&quot;age&quot;, ylab=&quot;Relative Change in Bone MD&quot;, main=&quot;Bone Data: Fitted Cubic Spline&quot;, las=1) lines(tt, predict(cubspline.bone, data.frame(age=tt)), lwd=2) for(k in 1:5) { ## Plot vertical lines at the knots abline(v=knot.seq[k], lty=2) } 12.3.5 Natural Cubic Splines Cubic splines can often have highly variable behavior near the edges of the data (i.e., for points near the smallest and largest \\(x_{i}\\)). One approach for addressing this problem is to use a spline which is linear for both \\(x &lt; u_{1}\\) and \\(x &gt; u_{q}\\). A natural cubic spline is a function that is still a piecewise cubic function over each of the intervals \\([u_{j}, u_{j+1}]\\) for \\(j=1, \\ldots, q-1\\), but is linear for \\(x &lt; u_{1}\\) and \\(x &gt; u_{q}\\). A natural cubic spline is still assumed to satisfy all of the continuity and derivative continuity conditions of the usual cubic spline. Natural cubic splines are also useful for fitting smoothing splines. Because we are using linear rather than cubic functions in the regions \\((-\\infty, u_{1})\\) and \\((u_{q}, \\infty)\\) this should reduce the number of necessary basis functions by \\(4\\) (so we would only need \\(q\\) rather \\(q + 4\\) basis functions). Indeed, one basis for the set of natural cubic splines with knots \\(u_{1}, \\ldots, u_{q}\\) is the following collection of \\(q\\) basis functions: \\[\\begin{equation} N_{1}(x) = 1 \\qquad N_{2}(x) = x \\qquad N_{k+2} = d_{k}(x) - d_{q-1}(x), k = 1, \\ldots, q-2, \\nonumber \\end{equation}\\] where the functions \\(d_{k}(x)\\) are defined as \\[\\begin{equation} d_{k}(x) = \\frac{ (x - u_{k})_{+}^{3} - (x - u_{q})_{+}^{3} }{ u_{q} - u_{k}} \\nonumber \\end{equation}\\] natural.cubspline.bone &lt;- lm(spnbmd ~ ns(age, df=8), data=bonedat) tt &lt;- seq(9.5, 25, by=0.1) plot(bonedat$age, bonedat$spnbmd, xlab=&quot;age&quot;, ylab=&quot;Relative Change in Bone MD&quot;, main=&quot;Bone Data: Fitted Natural Cubic Spline&quot;, las=1) lines(tt, predict(natural.cubspline.bone, data.frame(age=tt)), lwd=2) 12.4 Smoothing Splines When we used cubic splines for regression, we placed knots at a number of fixed points in between the smallest and largest \\(x_{i}\\) values. With what are referred to as “smoothing splines” every point \\(x_{i}\\) is treated as a potential knot, and the overall smoothness of the regression function estimate is determined through penalizing the roughness of the function. Motivation for smoothing splines can come from considering the following minimization problem: For \\(\\lambda \\geq 0\\), suppose we want to find the function \\(m(x)\\) which solves the optimization problem: \\[\\begin{equation} \\textrm{minimize:} \\quad \\sum_{i=1}^{n} \\{ Y_{i} - m( x_{i} ) \\}^{2} + \\lambda \\int \\{ m&#39;&#39;(x) \\}^{2} dx \\tag{12.2} \\end{equation}\\] subject to the constraint that \\(m&#39;(x)\\) and \\(m&#39;&#39;(x)\\) are both continuous. The second derivative \\(m&#39;&#39;(x)\\) represents the curvature of \\(m\\) at \\(x\\). So, the penalty \\(\\int \\{ m&#39;&#39;(x) \\}^{2} dx\\) is like an average squared curvature of the function \\(m(x)\\). If \\(\\lambda = 0\\), then the function \\(m(x)\\) which minimizes (12.2) is any function such that \\(m(x_{i}) = Y_{i}\\) for each \\(i\\). This will usually be an extremely “wiggly” function. If \\(\\lambda = \\infty\\), then we want a function \\(m(x)\\) such that \\(m&#39;&#39;(x) = 0\\) for all \\(x\\). The best function in this case would be a linear function \\(m(x) = \\beta_{0} + \\beta_{1}x\\). By choosing \\(0 &lt; \\lambda &lt; 0\\), we will get a function that can be nonlinear and capture some curvature but cannot be extremely “wiggly”. It is possible to show that the function \\(\\hat{m}_{\\lambda}(x)\\) which minimizes (12.2) is a natural cubic spline with \\(n\\) knots at the covariate values \\((x_{1}, \\ldots, x_{n})\\). So, we can restrict our search to functions which can be written as \\[\\begin{equation} m(x) = \\sum_{j=1}^{n} \\beta_{j}N_{j}(x), \\nonumber \\end{equation}\\] where \\(N_{1}(x), \\ldots, N_{n}(x)\\) is the set of basis functions for the set of natural cubic splines with these knots. So, assuming that \\(m(x) = \\sum_{j=1}^{n} \\beta_{j}N_{j}(x)\\), we can re-write the minimization problem as \\[\\begin{equation} \\textrm{minimize:}_{\\beta_{1}, \\ldots, \\beta_{n}} \\sum_{i = 1}^{n} \\{ Y_{i} - \\sum_{j=1}^{n} \\beta_{j}N_{j}(x_{i}) \\}^{2} + \\lambda \\sum_{j=1}^{n} \\sum_{k=1}^{n} \\beta_{j}\\beta_{k} \\int N_{j}&#39;&#39;(x) N_{k}&#39;&#39;(x) dx \\nonumber \\end{equation}\\] In matrix-vector notation, this minimization problem can be written as \\[\\begin{equation} \\textrm{minimize:}_{\\boldsymbol\\beta} \\quad (\\mathbf{Y} - \\mathbf{N}\\boldsymbol\\beta)^{T}(\\mathbf{Y} - \\mathbf{N}\\boldsymbol\\beta) + \\lambda \\boldsymbol\\beta^{T}\\boldsymbol\\Omega\\boldsymbol\\beta, \\tag{12.3} \\end{equation}\\] where \\(\\mathbf{N}\\) is the \\(n \\times n\\) matrix whose \\((j,k)\\) element is \\(N_{k}(x_{j})\\) and \\(\\boldsymbol\\Omega\\) is the \\(n \\times n\\) matrix whose \\((j,k)\\) element is \\(\\Omega_{jk} = \\int N_{j}&#39;&#39;(x)N_{k}&#39;&#39;(x) dx\\). The vector of coefficients which solves the minimization problem (12.3) is given by \\[\\begin{equation} \\hat{\\boldsymbol\\beta} = \\begin{bmatrix} \\hat{\\beta}_{1} \\\\ \\vdots \\\\ \\hat{\\beta}_{n} \\end{bmatrix} = (\\mathbf{N}^{T}\\mathbf{N} + \\lambda \\boldsymbol\\Omega)^{-1}\\mathbf{N}^{T}\\mathbf{Y} \\nonumber \\end{equation}\\] 12.5 Knot/Penalty Term Selection for Splines For both regression splines and smoothing splines, we can write the vector of fitted values \\(\\hat{\\mathbf{m}} = \\big( \\hat{m}(x_{1}), \\ldots, \\hat{m}(x_{n}) \\big)\\) as \\[\\begin{equation} \\hat{\\mathbf{m}} = \\mathbf{A}\\mathbf{Y}, \\nonumber \\end{equation}\\] for an appropriately chosen \\(n \\times n\\) matrix \\(\\mathbf{A}\\). For the case of a cubic regression spline with fixed knot sequence \\(\\mathbf{u} = (u_{1}, \\ldots, u_{q})\\), we have that \\(\\hat{\\mathbf{m}} = \\mathbf{A}_{\\mathbf{u}}\\mathbf{Y}\\) where \\[\\begin{equation} \\mathbf{A}_{\\mathbf{u}} = \\mathbf{X}_{\\mathbf{u}}(\\mathbf{X}_{\\mathbf{u}}^{T}\\mathbf{X}_{\\mathbf{u}})^{-1}\\mathbf{X}_{\\mathbf{u}}^{T} \\end{equation}\\] Note that, in this case, \\[\\begin{equation} \\textrm{tr}( \\mathbf{A}_{\\mathbf{u}} ) = \\textrm{tr}( \\mathbf{X}_{\\mathbf{u}}(\\mathbf{X}_{\\mathbf{u}}^{T}\\mathbf{X}_{\\mathbf{u}})^{-1}\\mathbf{X}_{\\mathbf{u}}^{T} ) = \\textrm{tr}( (\\mathbf{X}_{\\mathbf{u}}^{T}\\mathbf{X}_{\\mathbf{u}})^{-1}\\mathbf{X}_{\\mathbf{u}}^{T}\\mathbf{X}_{u} ) = q + 4 \\end{equation}\\] For the case of a smoothing spline with penalty term \\(\\lambda &gt; 0\\), we have that \\(\\hat{\\mathbf{m}} = \\mathbf{A}_{\\lambda}\\mathbf{Y}\\) where \\[\\begin{equation} \\mathbf{A}_{\\lambda} = \\mathbf{N}(\\mathbf{N}^{T}\\mathbf{N} + \\lambda\\boldsymbol\\Omega)^{-1}\\mathbf{N}^{T} \\nonumber \\end{equation}\\] 12.5.1 The Cp Statistic As with kernel and local regression method described in Chapter 11, the \\(C_{p}\\) statistic is defined as the mean residual sum of squares plus a penalty which depends on the matrix \\(\\mathbf{A}\\). In the context of regression splines where \\(\\hat{\\mathbf{m}} = \\mathbf{A}_{\\mathbf{u}}\\mathbf{Y}\\), the \\(C_{p}\\) statistic can be written as: \\[\\begin{eqnarray} C_{p}(q) &amp;=&amp; \\frac{1}{n}\\sum_{i=1}^{n}\\{ Y_{i} - \\hat{m}(x_{i}) \\}^{2} + \\frac{2\\hat{\\sigma}^{2}}{n}\\textrm{tr}(\\mathbf{A}_{u}) \\nonumber \\\\ &amp;=&amp; \\frac{1}{n}\\sum_{i=1}^{n}\\{ Y_{i} - \\hat{m}(x_{i}) \\}^{2} + \\frac{2\\hat{\\sigma}^{2}(q + 4)}{n} \\nonumber \\end{eqnarray}\\] In the context of smoothing splines where \\(\\hat{\\mathbf{m}} = \\mathbf{A}_{\\lambda}\\mathbf{Y}\\), the \\(C_{p}\\) statistic can be written as \\[\\begin{eqnarray} C_{p}(\\lambda) &amp;=&amp; \\frac{1}{n}\\sum_{i=1}^{n}\\{ Y_{i} - \\hat{m}(x_{i}) \\}^{2} + \\frac{2\\hat{\\sigma}^{2}}{n}\\textrm{tr}(\\mathbf{A}_{\\lambda}) \\nonumber \\\\ &amp;=&amp; \\frac{1}{n}\\sum_{i=1}^{n}\\{ Y_{i} - \\hat{m}(x_{i}) \\}^{2} + \\frac{2\\hat{\\sigma}^{2}}{n}\\textrm{tr}\\Big( (\\mathbf{N}^{T}\\mathbf{N} + \\lambda\\boldsymbol\\Omega)^{-1}\\mathbf{N}^{T}\\mathbf{N} \\Big) \\nonumber \\end{eqnarray}\\] 12.5.2 Leave-one-out Cross-Validation As mentioned in Chapter 11, the leave-one-out cross-validation can be expressed as a weighted sum-of-squared residuals with the weights coming from the diagonals of the “smoothing” matrix \\(\\mathbf{A}\\). For the smoothing spline, the leave-one-out cross-validation criterion is \\[\\begin{equation} \\textrm{LOOCV}(\\lambda) = \\frac{1}{n}\\sum_{i=1}^{n} \\Big( \\frac{ Y_{i} - \\hat{m}_{\\lambda}(x_{i})}{ 1 - a_{i}^{\\lambda}( x_{i} ) } \\Big)^{2} \\nonumber \\end{equation}\\] where \\(a_{i}^{\\lambda}(x_{i})\\) denotes the \\(i^{th}\\) diagonal of the matrix \\(\\mathbf{A}_{\\lambda}\\). 12.5.3 Generalized Cross-Validation A criterion which we did not mention in Chapter 11 is Generalized Cross-Validation (GCV). For the smoothing parameter \\(\\lambda\\), the GCV criterion is defined as \\[\\begin{equation} \\textrm{GCV}(\\lambda) = \\frac{1}{n}\\sum_{i=1}^{n} \\Big( \\frac{ Y_{i} - \\hat{m}_{\\lambda}(x_{i})}{ 1 - \\textrm{tr}(\\mathbf{A}_{\\lambda})/n } \\Big)^{2} = \\Big(\\frac{n}{n - \\textrm{tr}(\\mathbf{A}_{\\lambda})} \\Big)^{2} \\frac{1}{n} \\sum_{i=1}^{n} \\{Y_{i} - \\hat{m}_{\\lambda}(x_{i}) \\}^{2} \\nonumber \\end{equation}\\] The GCV criterion can be seen as replacing the individual diagonal elements in the \\(\\textrm{LOOCV}\\) criterion with their average value \\(\\textrm{tr}(\\mathbf{A}_{\\lambda})/n\\). GCV can often perform better when some of the diagonal elements of \\(\\mathbf{A}_{\\lambda}\\) are close to \\(1\\). 12.6 Fitting Smoothing Splines in R The R function smooth.spline will fit smoothing splines. smooth.spline(x, y, df, lambda, cv=FALSE) x - the vector of covariate values. y - the vector of responses. df - the trace of the “smoother” matrix. This is the trace of the matrix \\(\\mathbf{A}_{\\lambda} = \\mathbf{N}(\\mathbf{N}^{T}\\mathbf{N} + \\lambda\\boldsymbol\\Omega)^{-1}\\mathbf{N}^{T}\\mathbf{Y}\\). This must be less than or equal to \\(n\\) (where \\(n\\) here is the number of unique values of the \\(x_{i}\\)). lambda - the value of \\(\\lambda\\) in the matrix \\(\\mathbf{A}_{\\lambda} = \\mathbf{N}(\\mathbf{N}^{T}\\mathbf{N} + \\lambda\\boldsymbol\\Omega)^{-1}\\mathbf{N}^{T}\\mathbf{Y}\\). Note that you should only enter one of the df or lambda arguments. cv - the smooth.spline function will perform cross-validation whenever both the df and lambda arguments are left empty. When both of these arguments are left empty and cv=FALSE, the function will use GCV to select the smoothing parameter. When both df and lambda are left empty and cv = TRUE, the function will use LOOCV to select the smoothing parameter. Notice that when only input the x and y vectors, the smooth.spline will automatically use generalized cross-validation to select the smoothing parameter. To see how to use the smooth.spline function and how to use the \\(C_{p}\\) statistic, LOOCV, or GCV for selecting the smoothing parameter in smoothing splines or the number of knots in regression splines, we will again consider the bone data. To start, let’s use the smooth.spline function on the bone data using GCV to find the smoothing parameter: ss.bone &lt;- smooth.spline(x=bonedat$age, y=bonedat$spnbmd) plot(bonedat$age, bonedat$spnbmd, las=1, pch=16, xlab=&quot;age&quot;, ylab=&quot;Relative Change in Bone MD&quot;, main=&quot;Bone Data: Smoothing Spline using GCV&quot;) lines(ss.bone$x, ss.bone$y, lwd=3, col=&quot;red&quot;) Figure 12.5: Smoothing spline fit for the bone data. This used the smooth.spline function with all the default settings. If you just type in ss.bone, this will show the degrees of freedom used, the value of \\(\\lambda\\) used, and the value of the GCV criterion for the chosen \\(\\lambda\\) ss.bone ## Call: ## smooth.spline(x = bonedat$age, y = bonedat$spnbmd) ## ## Smoothing Parameter spar= 0.712 lambda= 0.000239 (15 iterations) ## Equivalent Degrees of Freedom (Df): 12.2 ## Penalized Criterion (RSS): 0.222 ## GCV: 0.00152 According to the GCV criterion, the best value for \\(\\lambda\\) is about \\(\\lambda^{*} \\approx 0.0002\\) and the corresponding value for the degrees of freedom is \\(df^{*} = \\textrm{tr}(\\mathbf{A}_{\\lambda^{*}}) \\approx 12.15\\). Using the LOOCV criterion, the best value for the degrees of freedom is \\(13\\). ss.bone2 &lt;- smooth.spline(x=bonedat$age, y=bonedat$spnbmd, cv=TRUE) ## Warning in smooth.spline(x = bonedat$age, y = bonedat$spnbmd, cv = TRUE): cross- ## validation with non-unique &#39;x&#39; values seems doubtful ss.bone2 ## Call: ## smooth.spline(x = bonedat$age, y = bonedat$spnbmd, cv = TRUE) ## ## Smoothing Parameter spar= 0.694 lambda= 0.000177 (13 iterations) ## Equivalent Degrees of Freedom (Df): 13 ## Penalized Criterion (RSS): 0.219 ## PRESS(l.o.o. CV): 0.00149 12.6.1 Smoothing Parameter Selection for the Smoothing Spline with the Cp statistic Suppose we wanted to find the best value of \\(\\textrm{tr}( \\mathbf{A}_{\\lambda})\\) of the smoothing spline using the \\(C_{p}\\) statistic. The first thing we want to find is an estimate of \\(\\sigma^{2}\\) that we can keep fixed across different values of the smoothing parameter. Let’s use the same value of \\(\\hat{\\sigma}^{2} = 0.0015\\) that we used in Chapter 11: sigsq.est &lt;- 0.0015 Now, let’s write a function that computes the \\(C_{p}\\) statistic for a smoothing spline model given that we input the data, the degrees of freedom \\(\\textrm{tr}(\\mathbf{A}_{\\lambda})\\), and \\(\\hat{\\sigma}^{2}\\). CpStatSmoothSpline &lt;- function(x, y, df, sigsq.hat) { n &lt;- length(x) ss.obj &lt;- smooth.spline(x=x, y=y, df=df) ## Now, compute the vector of residuals from this fitted smoothing spline residu &lt;- y - fitted(ss.obj) ans &lt;- mean(residu^2) + (2*sigsq.hat/n)*df return(ans) } Now, compute the \\(C_{p}\\) statistic for values of the degrees of freedom between \\(4\\) and \\(20\\). From the plot of the \\(C_{p}\\) vs. the degrees of freedom it looks like the best value for the degrees of freedom is about \\(12\\) df.seq &lt;- seq(4, 20, by=.1) nx &lt;- length(df.seq) Cp.seq &lt;- numeric(nx) for(k in 1:nx) { Cp.seq[k] &lt;- CpStatSmoothSpline(x=bonedat$age, y=bonedat$spnbmd, df=df.seq[k], sigsq.hat=sigsq.est) } plot(df.seq, Cp.seq, ylab=&quot;CP Stat&quot;, main=&quot;Bone Data: Cp(df) vs. df for the Smoothing Spline&quot;) More specifically, it’s about \\(11.6\\): df.seq[which.min(Cp.seq)] ## [1] 11.6 12.6.2 Knot Selection for Regression Splines with the Cp statistic Let’s go back to the regression spline methods we described in Section 12.3 also use the bone data to try to find the best number of knots for the case of regression splines. We will consider knots \\(u_{1}, \\ldots, u_{q}\\) that are computed automatically by the bs function when we specify the degrees of freedom. These are based on the quantiles of the covariate \\(x_{i}\\). We will consider values of \\(q\\) between \\(0\\) and \\(20\\). sigsq.hat &lt;- .0015 n &lt;- nrow(bonedat) qqseq &lt;- 0:20 Cp.seq &lt;- rep(0, length(qqseq)) nq &lt;- length(qqseq) for(k in 1:nq) { q = qqseq[k] tt &lt;- 1:q #uu &lt;- 9.4 + (15.7/(q+1))*tt if(q == 0) { tmp &lt;- lm(bonedat$spnbmd ~ bs(bonedat$age, df=q+3)) } else { tmp &lt;- lm(bonedat$spnbmd ~ bs(bonedat$age, df=q+3)) } RSS &lt;- mean((bonedat$spnbmd - tmp$fitted.values)^2) Cp.seq[k] &lt;- RSS + (2*sigsq.hat*(q + 4))/n } plot(qqseq, Cp.seq, xlab=&quot;q&quot;, ylab=&quot;Cp&quot;, main=&quot;Bone Data with Regression Splines: Cp Statistic for Different Number of Knots&quot;) From the plot, it looks like the best number of knots when using the \\(C_{p}\\) statistic is \\(q = 10\\). "],
["decision-tree.html", "Chapter 13 Regression Trees and CART 13.1 Introduction 13.2 Regression Trees with a Single Covariate 13.3 Regression Trees With Multiple Covariates", " Chapter 13 Regression Trees and CART 13.1 Introduction Let’s think about the regressogram estimate again. The regressogram estimate of the regression function is a piecewise constant function that is constant within each of \\(p\\) “bins” \\[\\begin{equation} \\hat{m}_{h_{n}}^{R}(x) = \\frac{1}{a}\\sum_{i=1}^{n} Y_{i} I(x_{i} \\in B_{k}), \\qquad \\textrm{ if } x \\in B_{k} \\nonumber \\end{equation}\\] Figure 13.1 shows an example of a regressogram estimate with 3 bins. Figure 13.1: Regressogram estimate with the 3 bins: [0,1/3), [1/3, 2/3), [2/3, 1). Suppose we were forced to combined two adjacent bins to estimate the regression function. For the data shown in 13.1, which two bins should we combine if we were forced to do so? The only options here are to combine bins 1 and 2 or to combine bins 2 and 3. I would say we should combine bins 1 and 2. Look at Figures 13.2 and 13.3 for a comparison of these two choices. The responses \\(Y_{i}\\) change much more over the range of the third bin than they do over the first and second bins. Hence, an intercept model for the first two bins is not all that bad. In contrast, an intercept model for the last two bins is a terrible model. Figure 13.2: Regressogram estimate with the 2 bins: [0,1/3), [1/3, 1). Figure 13.3: Regressogram estimate with the 2 bins: [0,2/3), [2/3, 1). The intuition for why the choice of bins \\([0,2/3), [2/3, 1)\\) is better than the choice of bins \\([0,1/3), [1/3, 1)\\) can be formalized by considering the within-bin variation of \\(Y_{i}\\). For two bins \\(B_{1}\\) and \\(B_{2}\\), the within-bin sum of squares (WBSS) of \\(Y_{i}\\) is \\[\\begin{equation} \\textrm{WBSS} = \\sum_{k=1}^{2} \\sum_{i=1} (Y_{i} - \\bar{Y}_{k})^{2}I(x_{i} \\in B_{k}) \\nonumber \\end{equation}\\] where \\(\\bar{Y}_{k} = \\frac{1}{n_{k}}\\sum_{i=1}^{n} Y_{i}\\) denotes the mean of the responses within the \\(k^{th}\\) bin. You want to choose the bins in order to minimize the within-bin sum of squares. The reason for this is that: if the within-bin sum of squares is low, an intercept model for each bin will fit the data very well. For the data shown in Figures 13.1 - 13.3, the WBSS when using the bins \\([0, 1/3), [1/3, 1)\\) is sum( (yy[xx &lt; 1/3] - mean(yy[xx &lt; 1/3]))^2 ) + sum( (yy[xx &gt;= 1/3] - mean(yy[xx &gt;= 1/3]))^2 ) ## [1] 62.72984 The WBSS when using the bins \\([0,2/3), [2/3, 1)\\) is sum( (yy[xx &lt; 2/3] - mean(yy[xx &lt; 2/3]))^2 ) + sum( (yy[xx &gt;= 2/3] - mean(yy[xx &gt;= 2/3]))^2 ) ## [1] 20.19249 13.2 Regression Trees with a Single Covariate For a single covariate, regression trees estimate the regression function by a piecewise constant function that is constant within each of several bins. We will focus on the well-known CART (Classification and Regression Trees) method for using regression trees. More generally, with multivariate covariates CART will fit a regression function that is constant within each of many multi-dimensional “rectangles”. The main difference between CART and the regressogram is that the placements and widths of the bins in CART are chosen in a more selective manner than the regressogram. Specifically, rather than just using a collection of bins of fixed width, CART chooses where to place the bin boundaries by considering the resulting within-bin sum of squares. CART constructs the bins through sequential binary splits of the x axis. In the first step, CART will divide the covariates into two bins \\(B_{1}\\) and \\(B_{2}\\). These bins will have the form \\(B_{1} = (-\\infty, t_{1})\\) and \\(B_{2} = [t_{1}, \\infty)\\). At the next step, CART will create \\(4\\) bins by further dividing each of these two bins into two more bins. So, we will have four bins \\(B_{11}, B_{12}, B_{21}, B_{22}\\). Bins \\(B_{11}\\) and \\(B_{12}\\) will have the form \\(B_{11} = (-\\infty, t_{11})\\) and \\(B_{12} = [t_{11}, t_{1})\\), and bins \\(B_{21}\\) and \\(B_{22}\\) will have the form \\(B_{21} = [t_{1}, t_{21})\\) and \\(B_{22} = [t_{21}, \\infty)\\). You can repeat this process to get smaller and smaller bins. Usually, this process will stop once a threshold for the minimum number of observations in a bin has been reached. This sequential process for constructing bins is typically depicted with a binary decision tree. Figure 13.4 shows the decision tree representation of a CART regression function estimate with \\(4\\) bins. Figure 13.4: Binary decision tree representing a regression function estimate with 4 bins where it is assumed that all the covariates are between \\(0\\) and \\(1\\). The 4 bins here are [0, 0.6), [0.6, 0.74), [0.74, 0.89), [0.89, 1). Figure 13.5 shows the regression function estimate which corresponds to the decision tree shown in Figure 13.4. Figure 13.5: Regression function estimate that corresponds to the decision tree shown in the previous figure. 13.2.1 Determining the Split Points The first two bins are determined by the “split point” \\(t_{1}\\). To find this split point, CART looks at the within-bin sum of squares induced by a splitting point \\(t\\). \\[\\begin{eqnarray} \\textrm{WBSS}(t) &amp;=&amp; \\sum_{k=1}^{2} \\sum_{i=1}^{n} (Y_{i} - \\bar{Y}_{k})^{2}I(x_{i} \\in B_{k}) \\nonumber \\\\ &amp;=&amp; \\sum_{i=1}^{n} (Y_{i} - \\bar{Y}_{1})^{2}I(x_{i} &lt; t) + \\sum_{i=1}^{n} (Y_{i} - \\bar{Y}_{2})^{2}I(x_{i} \\geq t) \\nonumber \\end{eqnarray}\\] The first split point \\(t_{1}\\) is the value of \\(t\\) which minimizes this within-bin sum of squares criterion. That is, \\[\\begin{equation} t_{1} = \\textrm{argmin}_{t} \\textrm{ WBSS}(t) = \\textrm{argmin}_{t \\in \\{x_{1}, \\ldots, x_{n} \\}} \\textrm{ WBSS}(t) \\nonumber \\end{equation}\\] To find \\(t_{1}\\), we only have to take the minimum over the set of covariates since the value of \\(\\textrm{WBSS}(t)\\) only changes at each \\(x_{i}\\). Figure 13.6 shows a plot of \\(\\textrm{WBSS}(t)\\) vs. \\(t\\) for the data shown in Figures 13.1 - 13.3. Figure 13.6 suggests that the value of \\(t_{1}\\) will be around \\(0.75\\). Figure 13.6: Plot of WBSS(t) vs. t for the data shown in the above figures. The splitting point “partitions” the data into two datasets. The indices of these datasets are defined as \\[\\begin{eqnarray} \\mathcal{D}_{1} &amp;=&amp; \\big\\{ (Y_{i}, x_{i}): x_{i} &lt; t_{1} \\big\\} \\nonumber \\\\ \\mathcal{D}_{2} &amp;=&amp; \\big\\{ (Y_{i}, x_{i}): x_{i} \\geq t_{1} \\big\\} \\nonumber \\end{eqnarray}\\] After finding \\(t_{1}\\), we can find the next two splitting points \\(t_{11}\\) and \\(t_{21}\\) by using the exact same procedure we used to find \\(t_{1}\\). That is, \\(t_{11}\\) and \\(t_{21}\\) are given by \\[\\begin{eqnarray} t_{11} &amp;=&amp; \\textrm{argmin}_{t} \\textrm{ WBSS}_{1}(t) \\nonumber \\\\ t_{21} &amp;=&amp; \\textrm{argmin}_{t} \\textrm{ WBSS}_{2}(t) \\nonumber \\end{eqnarray}\\] where \\(\\textrm{WBSS}_{a}(t)\\) is the within-bin sum of squares for dataset \\(\\mathcal{D}_{a}\\): \\[\\begin{equation} \\textrm{WBSS}_{a}(t) = \\sum_{i \\in \\mathcal{D}_{a}} (Y_{i} - \\bar{Y}_{1a})^{2}I(x_{i} &lt; t) + \\sum_{i \\in \\mathcal{D}_{a}} (Y_{i} - \\bar{Y}_{2a})^{2}I(x_{i} \\geq t) \\nonumber \\end{equation}\\] The splitting points \\(t_{11}\\) and \\(t_{21}\\) will further partition the dataset into \\(4\\) datasets. Additional splitting points \\(t_{12}, t_{22}, t_{32}, t_{42}\\) which further partition the dataset, can be found by minimizing the within-bin sum of squares for each of these \\(4\\) datasets. This algorithm for constructing smaller and smaller bins is often referred to as recursive partitioning. 13.3 Regression Trees With Multiple Covariates When we have multivariate covariates where \\(\\mathbf{x}_{i} = (x_{i1}, \\ldots, x_{ip})\\), CART will partition the covariate space into multivariate “rectangles”. An example of a CART-type regression function estimate for the case of two covariates is shown in Figure 13.7 Figure 13.7: An example of a CART-type estimate of a regression function that has two covariates. One advantage of CART is that it can easily handle covariates of different types, for example, both continuous and binary covariates. You can see an example of this by looking at the bone data. This dataset has both sex and age as covariates. A CART regression tree for the bone data using both age and sex is shown in Figure 13.8. Figure 13.8: Regression tree for the bone data. This fitted regression tree has 6 bins. Figure 13.9 plots the regression function estimate which corresponds to the decision tree shown in Figure 13.8. Notice that the regression function estimate for men vs. women only differs for ages \\(&lt; 12\\). Figure 13.9: Plot of regression function estimate that corresponds to the decision tree in the previous figure. "],
["ensemble.html", "Chapter 14 Ensemble Methods for Prediction", " Chapter 14 Ensemble Methods for Prediction "]
]
