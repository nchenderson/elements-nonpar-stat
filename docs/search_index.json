[
["index.html", "Elements of Nonparametric Statistics Preface", " Elements of Nonparametric Statistics Nicholas Henderson 2019-12-25 Preface This is the very first part of the book. "],
["intro.html", "Chapter 1 Introduction 1.1 What is Nonparametric Statistics? 1.2 Outline of Course 1.3 Example 1: Nonparametric vs. Parametric Two-Sample Testing 1.4 Example 2: Nonparametric Estimation 1.5 Example 3: Confidence Intervals 1.6 Example 4: Nonparametric Regression with a Single Covariate 1.7 Example 5: Nonparametric Regression", " Chapter 1 Introduction 1.1 What is Nonparametric Statistics? What is Parametric Statistics? Parametric models refer to probability distributions that can be fully described by a fixed number of parameters that do not grow with the sample size. Typical examples include Gaussian Poisson Exponential Beta Could also refer to a regression setting where the mean function is described by a fixed number of parameters. What is Nonparametric Statistics? Difficult to give a concise, all-encompassing definition, but nonparametric statistics generally refers to statistical methods where there is not a clear parametric component. The uses of nonparametric methods in several common statistical contexts are described in Sections 1.3 - 1.7. 1.2 Outline of Course This course is roughly divided into the following 5 categories. Nonparametric Testing Rank-based Tests Permutation Tests Estimation of Basic Nonparametric Quantities The Empirical Distribution Function Density Estimation Nonparametric Confidence Intervals Bootstrap Jacknife Nonparametric Regression Part I (Smoothing Methods) Kernel Methods Splines Local Regression Nonparametric Regression Part II (Machine Learning Methods) Decision Trees/CART Ensemble Methods 1.3 Example 1: Nonparametric vs. Parametric Two-Sample Testing Suppose we have data from two groups. For example, outcomes from two different treatments. Group 1 outcomes: \\(X_{1}, \\ldots, X_{n}\\) an i.i.d (independent and identically distributed) sample from distribution function \\(F_{X}\\). That is, \\[\\begin{equation} F_{X}(t) = P( X_{i} \\leq t) \\nonumber \\end{equation}\\] Group 2 outcomes: \\(Y_{1}, \\ldots, Y_{m}\\) an i.i.d. sample from distribution function \\(F_{Y}\\). To test the impact of a new treatment, we usually want to test whether or not \\(F_{X}\\) differs from \\(F_{Y}\\) in some way. This can be stated in hypothesis testing language as \\[\\begin{eqnarray} H_{0}&amp;:&amp; F_{X} = F_{Y} \\textrm{( populations are the same)} \\nonumber \\\\ H_{A}&amp;:&amp; F_{X} \\neq F_{Y} \\textrm{( populations are different)} \\nonumber \\end{eqnarray}\\] Parametric Tests A common parametric test for () is the t-test. The t-test assumes that \\[\\begin{equation} F_{X} = \\textrm{Normal}(\\mu_{x}, \\sigma^{2}) \\quad \\textrm{ and } \\quad F_{Y} = \\textrm{Normal}(\\mu_{y}, \\sigma^{2}) \\end{equation}\\] Under this parametric assumption, the hypothesis test () reduces to \\[\\begin{equation} H_{0}: \\mu_{x} = \\mu_{y} \\quad \\textrm{ vs. } \\quad H_{A}: \\mu_{x} \\neq \\mu_{y} \\end{equation}\\] The standard t-statistic (with a pooled estimate of \\(\\sigma^{2}\\)) is the following \\[\\begin{equation} T = \\frac{\\bar{X} - \\bar{Y}}{ s_{p}\\sqrt{\\frac{1}{n} + \\frac{1}{m}} }, \\end{equation}\\] where \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}\\) and \\(\\bar{Y} = \\frac{1}{m}\\sum_{i=1}^{m} Y_{i}\\) are the group-specific sample means and \\(s_{p}^{2}\\) is the pooled estimate of \\(\\sigma^{2}\\) \\[\\begin{equation} s_{p}^{2} = \\frac{1}{m + n - 2}\\Big\\{ \\sum_{i=1}^{n} (X_{i} - \\bar{X})^{2} + \\sum_{i=1}^{m} (Y_{i} - \\bar{Y})^{2} \\Big\\} \\end{equation}\\] The t-test is based on the null distribution of \\(T\\) - the distribution of \\(T\\) under the null hypothesis. *Under the assumption of normality, the null distribution of \\(T\\) is a t distribution with \\(n + m - 2\\) degrees of freedom. Put graph here Notice that this null distribution depends on the parametric assumption that both \\(F_{X} = \\textrm{Normal}(\\mu_{x}, \\sigma^{2})\\) and \\(F_{Y} = \\textrm{Normal}(\\mu_{y}, \\sigma^{2})\\). (Mention CLT argument here) Moreover, we used the parametric assumption in the formulation of the hypothesis test itself because we assumed that any difference between \\(F_{X}\\) and \\(F_{Y}\\) would be fully described by difference in \\(\\mu_{x}\\) and \\(\\mu_{y}\\). Two-sample nonparametric tests are meant to be “distribution-free”. This means that the null distribution of the test statistic does not depend on any parametric assumptions about the two populations \\(F_{X}\\) and \\(F_{Y}\\). Also, the hypotheses tests themselves do not rely on any parametric assumptions. For example, 1.4 Example 2: Nonparametric Estimation Suppose we have \\(n\\) observations \\((X_{1}, \\ldots, X_{n})\\) which are assumed to be i.i.d. (independent and identically distributed). The distribution function of \\(X_{i}\\) is \\(F_{X}\\). Suppose we are interested in estimating \\(F_{X}\\). In a parametric approach to estimating \\(F_{X}\\), we would assume the distribution of \\(X_{i}\\) belongs to some parametric family of distributions. For example, \\(X_{i} \\sim \\textrm{Normal}( \\mu, \\sigma^{2} )\\), \\(X_{i} \\sim \\textrm{Exponential}(\\lambda)\\), or \\(X_{i} \\sim \\textrm{Beta}( \\alpha, \\beta)\\). If we assume that \\(X_{i} \\sim \\textrm{Normal}( \\mu, \\sigma^{2} )\\), we only need to estimate 2 parameters to fully describe the distribution of \\(X_{i}\\), and the number of parameters does not depend on the sample size. 1.5 Example 3: Confidence Intervals 1.6 Example 4: Nonparametric Regression with a Single Covariate 1.7 Example 5: Nonparametric Regression "],
["getting-started.html", "Chapter 2 Working with R", " Chapter 2 Working with R Before we can start exploring data in R, there are some key concepts to understand first: What are R and RStudio? How do I code in R? What are R packages? If you are already familiar with these concepts, feel free to skip to Section ?? below introducing some of the datasets we will explore in depth in this book. Much of this chapter is based on two sources which you should feel free to use as references if you are looking for additional details: "],
["rank-tests.html", "Chapter 3 Rank and Sign Statistics 3.1 Introduction 3.2 Ranks 3.3 Two-Sample Tests 3.4 One Sample Tests 3.5 Comparisons with Parametric Tests 3.6 Thinking about Rank statistics more generally", " Chapter 3 Rank and Sign Statistics 3.1 Introduction Start with t-test example, difference in means is sufficient for superiority Give example of type of tests we are interested in. Why ranks and why nonparametric testing? (reduce influence of outliers) 3.2 Ranks 3.2.1 Definition Suppose we have \\(n\\) observations \\(X_{1}, \\ldots, X_{n}\\). The rank of the \\(i^{th}\\) observation \\(R_{i}\\) is defined as \\[\\begin{equation} R_{i} = R(X_{i}) = \\sum_{j=1}^{n} I( X_{j} \\leq X_{i}) \\tag{3.1} \\end{equation}\\] where \\[\\begin{equation} I(X_{j} \\leq X_{i}) = \\begin{cases} 1 &amp; \\text{ if } X_{j} \\leq X_{i} \\\\ 0 &amp; \\text{ if } X_{j} &gt; X_{i} \\end{cases} \\end{equation}\\] The largest observation has a rank of \\(n\\). The smallest observation has a rank of \\(1\\) (if there are no ties). x &lt;- c(3, 7, 1, 12, 6) ## 5 observations rank(x) ## [1] 2 4 1 5 3 In the definition of ranks shown in (3.1), tied observations receive their maximum possible rank. For example, suppose that \\((X_{1}, X_{2}, X_{3}, X_{4}) = (0, 1, 1, 2)\\). In this case, one could argue whether both observations 2 and 3 should be ranked \\(2^{nd}\\) or \\(3^{rd}\\) while observations \\(1\\) and \\(4\\) should unambiguously receive ranks of \\(1\\) and \\(4\\) respectively. Under definition @rank(eq:rankdef), both observations \\(2\\) and \\(3\\) receive a rank of \\(3\\). In R, such handling of ties is done using the ties.method = “max” argument x &lt;- c(0, 1, 1, 2) rank(x, ties.method=&quot;max&quot;) ## [1] 1 3 3 4 The default in R is to replace the ranks of tied observations with their “average” rank x &lt;- c(0, 1, 1, 2) rank(x) ## [1] 1.0 2.5 2.5 4.0 3.2.2 Properties of Ranks Suppose \\((X_{1}, \\ldots, X_{n})\\) is random sample from a continuous distribution \\(F\\) (so that the probability of ties is zero). Then, the following properties hold for the associated ranks \\(R_{1}, \\ldots, R_{n}\\). Each \\(R_{i}\\) follows a discrete uniform distribution \\[\\begin{equation} P(R_{i} = j) = 1/n, \\quad \\text{for any } j = 1, \\ldots,n. \\end{equation}\\] The expectation of \\(R_{i}\\) is \\[\\begin{equation} E( R_{i} ) = \\sum_{j=1}^{n} j P(R_{i} = j) = \\frac{1}{n}\\sum_{j=1}^{n} j = \\frac{(n+1)}{2} \\end{equation}\\] The variance of \\(R_{i}\\) is \\[\\begin{equation} \\text{Var}( R_{i} ) = E( R_{i}^{2} ) - E(R_{i})^{2} = \\frac{1}{n}\\sum_{j=1}^{n} j^{2} - \\Big( \\frac{n+1}{2} \\Big)^{2} = \\frac{ n^{2} - 1}{12} \\end{equation}\\] The random variables \\(R_{1}, \\ldots, R_{n}\\) are not independent (why?). However, the vector \\(\\mathbf{R}_{n} = (R_{1}, \\ldots, R_{n})\\) is uniformly distributed on the set of \\(n!\\) permutations of \\((1,2,\\ldots,n)\\). 3.3 Two-Sample Tests Wilcoxon statistic and Wilcoxon signed-rank statistic (what is the difference between these two?) 3.3.1 The Wilcoxon Rank Sum Test 3.3.1.1 Purpose The Wilcoxon Rank Sum (WRS) test (sometimes referred to as the Wilcoxon-Mann-Whitney test) is a two-sample test. The WRS test is used to test whether or not observations from one group tend to be larger (or smaller) than observations from the other group. Suppose we have observations from two groups: \\(X_{1}, \\ldots, X_{n} \\sim F_{X}\\) and \\(Y_{1}, \\ldots, Y_{m} \\sim F_{Y}\\). The WRS test will test the hypothesis \\[\\begin{eqnarray} H_{0}: &amp; &amp; F_{X} = F_{Y} \\quad \\textrm{ versus } \\nonumber \\\\ H_{A}: &amp; &amp; \\textrm{Observations from } F_{X} \\textrm{ tend to be larger than observations from } F_{Y} \\nonumber \\end{eqnarray}\\] What is meant by ``tend to be larger&quot;? Two common ways of stating the alternative hypothesis for the WRS include \\[\\begin{eqnarray} H_{0}: &amp; &amp; F_{X} = F_{Y} \\quad \\textrm{ versus } \\nonumber \\\\ H_{A}: &amp; &amp; F_{X} \\textrm{ is stochastically larger than } F_{Y} \\nonumber \\end{eqnarray}\\] or \\[\\begin{eqnarray} H_{0}: &amp; &amp; F_{X} = F_{Y} \\quad \\textrm{ versus } \\nonumber \\\\ H_{A}: &amp; &amp; F_{X}(t) = F_{Y}(t - \\Delta), \\Delta &gt; 0. \\end{eqnarray}\\] A distribution function \\(F_{X}\\) is said to be stochastically larger than \\(F_{Y}\\) if \\(F_{X}(t) \\geq F_{Y}(t)\\) for all \\(t\\) with \\(F_{X}(t) &gt; F_{Y}(t)\\) for at least one value of \\(t\\). Note that the “shift alternative” implies stochastic dominance. Why do we need to specify an alternative? 3.3.1.2 Definition of Test Statistic The WRS test statistic is based on computing the sum of ranks (ranks based on the pooled sample) in one group. If observations from this group tend to be larger, their average rank should exceed the average rank from the other group. Give exercise, compute p-values for Wilcoxon test where we have two populations. both are Normally distributed with mean zero but different variances. 3.4 One Sample Tests 3.4.1 The Sign Test Suppose we have observations \\(W_{1}, \\ldots, W_{n}\\) which arise from the following model \\[\\begin{equation} W_{i} = \\theta + \\varepsilon_{i}, \\nonumber \\end{equation}\\] where \\(\\varepsilon_{i}\\) are iid random variables each with distribution function \\(F\\) that is assumed to have a median of zero. 3.4.2 The Signed-Rank Wilcoxon Test 3.5 Comparisons with Parametric Tests 3.6 Thinking about Rank statistics more generally "],
["tidy.html", "Chapter 4 Rank Tests for Multiple Groups", " Chapter 4 Rank Tests for Multiple Groups In Subsection "],
["permutation.html", "Chapter 5 Permutation Tests (Next Chapter Should be U-statistics)", " Chapter 5 Permutation Tests (Next Chapter Should be U-statistics) Permutation tests are … "],
["ustat.html", "Chapter 6 U-Statistics 6.1 Examples 6.2 Mann-Whitney Statistic 6.3 Kendall’s tau 6.4 Distance Covariance", " Chapter 6 U-Statistics 6.1 Examples A wide range of well-known statistics can also be represented as U-statistics. Sample Mean, Variance, Signed-Rank Statistic, Gini Mean Difference 6.2 Mann-Whitney Statistic 6.3 Kendall’s tau 6.4 Distance Covariance "],
["regression.html", "Chapter 7 The Empirical Distribution Function 7.1 Empirical Distribution Functions 7.2 The Empirical Distribution Function in R 7.3 The empirical distribution functino and statistical functionals", " Chapter 7 The Empirical Distribution Function 7.1 Empirical Distribution Functions 7.1.1 Definition and Basic Properties Every random variable has a cumulative distribution function (cdf). The cdf of a random variable \\(X\\) is defined as \\[\\begin{equation} F(t) = P( X \\leq t) \\end{equation}\\] The empirical distribution function or empirical cumulative distribution function (ecdf) estimates \\(F(t)\\) by just finding the proportion of observations which are less than or equal to \\(t\\). For i.i.d. random variables \\(X_{1}, \\ldots, X_{n}\\), the empirical distribution function is defined as \\[\\begin{equation} \\hat{F}_{n}(t) = \\frac{1}{n}\\sum_{i=1}^{n} I( X_{i} \\leq t) \\nonumber \\end{equation}\\] Note that the empirical distribution function can be computed for any type of data without making any assumptions about the distribution from which the data arose. The only assumption we are making is that \\(X_{1}, \\ldots, X_{n}\\) constitute an i.i.d. sample from some common distribution function \\(F\\). 7.1.2 Confidence intervals for Fhat 7.2 The Empirical Distribution Function in R 7.3 The empirical distribution functino and statistical functionals "],
["multiple-regression.html", "Chapter 8 Density Estimation 8.1 Introduction 8.2 Histograms 8.3 Kernel Density Estimation 8.4 Kernel Density Estimation in Practice", " Chapter 8 Density Estimation 8.1 Introduction In this section, we focus on methods for estimating a probability density function (pdf) \\(f(x)\\). For a continuous random variable \\(X\\), areas under the probability density function are probabilities \\[\\begin{equation} P(a &lt; X &lt; b) = \\int_{a}^{b} f(x) dx \\nonumber \\end{equation}\\] and \\(f(x)\\) is related to the distribution function via \\(f(x) = F&#39;(x)\\). With parametric approaches to density estimation, you only need to estimate a couple of parameters as these parameters completely determine the form of \\(f(x)\\). For example, with a Gaussian distribution you only need to estimate \\(\\mu\\) and \\(\\sigma^{2}\\) to draw the appropriate bell curve. In a nonparametric approach, you assume that your observations \\(X_{1}, \\ldots, X_{n}\\) are an independent sample from a distribution having pdf \\(f(x)\\), but otherwise you make few assumptions about the particular form of \\(f(x)\\). 8.2 Histograms 8.2.1 Definition Histograms are one of the oldest ways to estimate a pdf. To construct a histogram, you first need to define a series of “bins”: \\(B_{1}, \\ldots, B_{D}\\). Each bin is a left-closed interval which is often assumed to have the form \\(B_{k} = [x_{0} + (k-1)h, x_{0} + kh)\\): \\[\\begin{eqnarray} B_{1} &amp;=&amp; [x_{0}, x_{0} +h) \\nonumber \\\\ B_{2} &amp;=&amp; [x_{0} + h, x_{0} + 2h) \\nonumber \\\\ &amp;\\vdots&amp; \\nonumber \\\\ B_{D} &amp;=&amp; [x_{0} + (D-1)h, x_{0} + Dh) \\nonumber \\end{eqnarray}\\] \\(x_{0}\\) - the origin \\(h_{n}\\) - bin width For each bin, you first need to count the number of observations which fall into that bin \\[\\begin{eqnarray} n_{k} &amp;=&amp; \\# \\text{ of observations falling into the $k^{th}$ bin } \\nonumber \\\\ &amp;=&amp; \\sum_{i=1}^{n} I( x_{0} + (k-1)h_{n} \\leq X_{i} &lt; x_{0} + kh_{n} ) \\end{eqnarray}\\] The histogram estimate of the density at a point \\(x\\) in the \\(k^{th}\\) bin is then defined as \\[\\begin{equation} \\hat{f}(x) = \\frac{n_{k}}{nh_{n}} \\end{equation}\\] Note: What is often shown in histogram plots are the actual bin counts \\(n_{k}\\) rather than the values of \\(\\hat{f}(x)\\). To see the motivation for the histogram estimate, notice that if we choose a relatively small value \\(h &gt; 0\\) \\[\\begin{equation} P(x &lt; X_{i} &lt; x+h) = \\int_{x}^{x + h} f(x) dx \\approx hf(x) \\end{equation}\\] The expected value of \\(\\hat{f}(x)\\) is \\[\\begin{equation} E\\{ \\hat{f}(x) \\} = = \\approx f(x) \\end{equation}\\] 8.2.2 Histograms in R In R, use the hist function hist(x, breaks, probability, plot, ...) The breaks argument Default is “Sturges”. This is a method for finding the binwidth. Can be a name giving the name of an algorithm for computing binwidth (e.g., “Scott” and “FD”). Can also be a single number. This gives the number of bins used. Could also be a .. The probability argument The plot argument Note: The default for R, is to use right-closed intervals \\((a, b]\\). This can be changed using the right argument of the hist function. ## Use a real dataset here bodywt.hist &lt;- hist(nhgh$wt, main=&quot;&quot;, xlab=&quot;Body Weight from NHANES&quot;) ## Use a real dataset here bodywt.hist2 &lt;- hist(nhgh$wt, main=&quot;Hist of BW on Probability Scale&quot;, xlab=&quot;Body Weight from NHANES&quot;, probability=TRUE) In addition to generating a histogram plot, the histogram function also returns useful stuff. names(bodywt.hist) ## [1] &quot;breaks&quot; &quot;counts&quot; &quot;density&quot; &quot;mids&quot; &quot;xname&quot; &quot;equidist&quot; breaks counts mids density bodywt.hist$breaks ## [1] 20 40 60 80 100 120 140 160 180 200 220 240 bodywt.hist$counts ## [1] 44 1160 2705 1846 721 212 71 24 7 2 3 binwidth &lt;- bodywt.hist$breaks[2] - bodywt.hist$breaks[1] bodywt.hist$density ## [1] 3.237675e-04 8.535688e-03 1.990434e-02 1.358352e-02 5.305372e-03 ## [6] 1.559971e-03 5.224430e-04 1.766004e-04 5.150846e-05 1.471670e-05 ## [11] 2.207506e-05 bodywt.hist$counts/(length(nhgh$wt)*binwidth) ## [1] 3.237675e-04 8.535688e-03 1.990434e-02 1.358352e-02 5.305372e-03 ## [6] 1.559971e-03 5.224430e-04 1.766004e-04 5.150846e-05 1.471670e-05 ## [11] 2.207506e-05 8.2.3 Performance of the Histogram Estimate 8.2.3.1 Bias/Variance Decomposition It is common to evaluate the performance of a density estimator through its mean-squared error (MSE). In general, MSE is a function of bias and variance \\[\\begin{equation} MSE = Bias^2 + Variance \\end{equation}\\] We will first look at the mean-squared error of \\(\\hat{f}( x )\\) at a single point \\(x\\) \\[\\begin{eqnarray} \\textrm{MSE}\\{ \\hat{f}(x) \\} &amp;=&amp; E\\Big( \\{ \\hat{f}(x) - f(x) \\}^{2} \\Big) \\nonumber \\\\ &amp;=&amp; \\underbrace{\\Big( E\\{ \\hat{f}(x) \\} - f(x) \\Big)^{2} }_{\\textrm{Bias Squared}} + \\underbrace{\\textrm{Var}\\{ \\hat{f}(x) \\}}_{Variance} \\nonumber \\end{eqnarray}\\] In general, as the bin-width \\(h_{n}\\) increases, the histogram estimate has less variation but becomes more biased. 8.2.3.2 Bias and Variance of the Histogram Estimate Recall that, for a histogram estimate, we have \\(D_{n}\\) bins where the \\(k^{th}\\) bin takes the form \\[\\begin{equation} B_{k} = [x_{0} + (k-1)h_{n}, x_{0} + kh_{n}) \\nonumber \\end{equation}\\] For a point \\(x \\in B_{k}\\), that “belongs” to the \\(k^{th}\\) bin, the histogram density estimate is \\[\\begin{equation} \\hat{f}(x) = \\frac{n_{k}}{nh_{n}}, \\quad \\textrm{ where } n_{k} = \\textrm{ number of observations falling into bin } B_{k} \\end{equation}\\] To better examine what happens as \\(n\\) changes, we will define the function \\(A_{h_{n}}(x)\\) as the function which returns the index of the interval to which \\(x\\) belongs. For example, if we had three bins \\(B_{1} = [0, 1/3)\\), \\(B_{2} = [1/3, 2/3)\\), \\(B_{3} = [2/3, 1)\\) and \\(x = 1/2\\), then \\(A_{h_{n}}( x ) = 2\\). So, we can also write the histogram density estimate as \\[\\begin{equation} \\hat{f}(x) = \\frac{n_{A_{h_{n}}(x)}}{nh_{n}} \\end{equation}\\] Note that \\(n_{A_{h_{n}}(x)}\\) is a binomial random variable with \\(n\\) trials and success probability \\(p_{h_{n}}(x)\\) (why?) \\[\\begin{equation} n_{A_{h_{n}}(x)} \\sim \\textrm{Binomial}\\{ n, p_{h_{n}}(x) \\} \\nonumber \\end{equation}\\] The success probability \\(p_{h_{n}}(x)\\) is defined as \\[\\begin{equation} p_{h_{n}}(x) = P\\Big\\{ X_{i} \\textrm{ falls into bin } A_{h_{n}}(x) \\Big\\} = \\int_{x_{0} + (A_{h_{n}}(x) - 1)h_{n}}^{x_{0} + A_{h_{n}}(x)h_{n} } f(t) dt. \\end{equation}\\] Using what is known about the Binomial distribution (i.e., \\(E( n_{A_{h_{n}}(x)} ) = np_{h_{n}}(x)\\) and \\(\\textrm{Var}( n_{A_{h_{n}}(x)} ) = np_{h_{n}}(x)\\{1 - p_{h_{n}}(x) \\}\\)), we can express the bias and variance of \\(\\hat{f}(x)\\) as \\[\\begin{eqnarray} \\textrm{Bias}\\{ \\hat{f}(x) \\} &amp;=&amp; E\\{ \\hat{f}(x) \\} - f(x) \\nonumber \\\\ &amp;=&amp; \\frac{1}{nh_{n}}E( n_{A_{h_{n}}(x)} ) - f(x) \\nonumber \\\\ &amp;=&amp; \\frac{ p_{h_{n}}(x) }{ h_{n} } - f(x) \\nonumber \\end{eqnarray}\\] and \\[\\begin{eqnarray} \\textrm{Var}\\{ \\hat{f}(x) \\} = \\frac{1}{n^{2}h_{n}^{2}}\\textrm{Var}( n_{A_{h_{n}}(x)} ) = \\frac{ p_{h_{n}}(x)\\{1 - p_{h_{n}}(x) \\} }{ nh_{n}^{2} } \\end{eqnarray}\\] Using the approximation \\(f(t) \\approx f(x) + f&#39;(x)(t - x)\\) for \\(t\\) close to \\(x\\), we have that \\[\\begin{equation} \\frac{ p_{h_{n}}(x) }{ h_{n} } = \\frac{1}{h_{n}}\\int_{x_{0} + (A_{h_{n}}(x) - 1)h_{n}}^{x_{0} + A_{h_{n}}(x)h_{n} } f(t) dt \\approx f(x) + f&#39;(x)\\{ x - x_{0} - (A_{h_{n}}(x) - 1)h_{n} \\} \\end{equation}\\] So, the bias of the histogram density estimate \\(\\hat{f}(x)\\) is \\[\\begin{equation} \\textrm{Bias}\\{ \\hat{f}(x) \\} \\approx f&#39;(x)\\{ x - (x_{0} + (A_{h_{n}}(x) - 1)h_{n}) \\} \\end{equation}\\] [[ Double-check this bias formula and check with Scott ]] Choosing a very small bin width will result in a small bias because the left endpoint of the bin \\(x_{0} + (A_{h_{n}}(x) - 1)h_{n}\\) will always be very close to \\(x\\). Now, turning to the variance of the histogram estimate \\[\\begin{equation} \\textrm{Var}\\{ \\hat{f}(x) \\} = \\frac{p_{h_{n}}(x) }{nh_{n}^{2}}\\{1 - p_{h_{n}}(x)\\} \\approx \\frac{f(x) + f&#39;(x)\\{ x - x_{0} - (A_{h_{n}}(x) - 1)h_{n} \\}}{nh_{n}}\\{1 - p_{h_{n}}(x)\\} \\approx \\frac{f(x)}{n h_{n} } \\end{equation}\\] For a more detailed description of the above approximation see Scott. Note that large bin widths will reduce variance. 8.2.3.3 Point-wise Mean Squared Error Recalling (), the approximate mean-squared error of the histogram density estimate at a particular point \\(x\\) is given by \\[\\begin{eqnarray} \\textrm{MSE}\\{ \\hat{f}(x) \\} &amp;=&amp; E\\Big( \\{ \\hat{f}(x) - f(x) \\}^{2} \\Big) \\nonumber \\\\ &amp;=&amp; \\Big( \\textrm{Bias}\\{ \\hat{f}(x) \\} \\Big)^{2} + \\textrm{Var}\\{ \\hat{f}(x) \\} \\nonumber \\\\ &amp;\\approx&amp; [f&#39;(x)]^{2}\\{ x - (x_{0} + (A_{h_{n}}(x) - 1)h_{n}) \\}^{2} + \\frac{f(x)}{n h_{n} } \\end{eqnarray}\\] For any approach to bin width selection, we should have \\(h_{n} \\longrightarrow 0\\) and \\(nh_{n} \\longrightarrow \\infty\\). This MSE approximation depends on a particular choice of \\(x\\). Difficult to use () as a criterion for selecting the bandwidth because this could vary depending on your choice of \\(x\\). 8.2.3.4 Integrated Mean Squared Error and Optimal Histogram Bin Width Using mean integrated squared error (MISE) allows us to find an optimal bin width that does not depend on a particular choice of \\(x\\). The MISE is defined as \\[\\begin{eqnarray} MISE\\{ \\hat{f}(x) \\} &amp;=&amp; E\\Big\\{ \\int_{-\\infty}^{\\infty} \\{ \\hat{f}(x) - f(x) \\}^{2}dx \\Big\\} \\nonumber \\\\ &amp;=&amp; \\int_{-\\infty}^{\\infty} \\textrm{MSE}\\{ \\hat{f}(x) \\} dx \\end{eqnarray}\\] Using our previously derived approximation for the MSE, we have \\[\\begin{eqnarray} MISE\\{ \\hat{f}(x) \\} &amp;\\approx&amp; \\int x [f&#39;(x)]^{2} - x_{0}\\int [f&#39;(x)]^{2} dx + (A_{h_{n}}(x) - 1)h_{n}) \\}^{2} + \\frac{1}{n h_{n} } \\int f(x) dx \\nonumber \\\\ &amp;=&amp; \\end{eqnarray}\\] To select the optimal bin width, we minimize the MISE as a function of \\(h_{n}\\). Minimizing (), as a function of \\(h_{n}\\) yields the following formula for the optimal bin width \\[\\begin{equation} h_{n}^{opt} = \\Big( \\frac{6}{n \\int_{-\\infty}^{\\infty} [f&#39;(x)]^{2} dx} \\Big)^{1/3} = C n^{-1/3} \\nonumber \\end{equation}\\] Notice that \\(h_{n}^{opt} \\longrightarrow 0\\) and \\(nh_{n}^{opt} \\longrightarrow \\infty\\) as \\(n \\longrightarrow \\infty\\). Notice also that the optimal bin width depends on the unknown quantity \\(\\int_{-\\infty}^{\\infty} [f&#39;(x)]^{2} dx\\). 8.2.4 Choosing the Histogram Bin Width We will mention three rules for selecting the bin width of a histogram. Scott rule: (based on the optimal bin width formula) Friedman and Diaconis rule (also based on the optimal bin width formula) Sturges rule: (based on …) Both Scott and the FD rule are based on the optimal bin width formula (). The main problem with this formula is the presence of \\(\\int_{-\\infty}^{\\infty} [f&#39;(x)]^{2} dx\\). Solution: See what this quantity looks like if we assume that \\(f(x)\\) corresponds to a \\(N(\\mu, \\sigma^{2})\\) density. With this assumption, \\[\\begin{equation} h_{n}^{opt} = 3.5 \\sigma n^{-1/3} \\end{equation}\\] Scott rule: use \\(\\hat{\\sigma} = 2\\) 8.3 Kernel Density Estimation 8.3.1 Histograms and a “Naive” Density Estimate 8.3.2 Kernels, Bandwidth, and Smooth Density Estimation 8.3.3 Bias and Variance of Kernel Density Estimates 8.3.4 Bandwidth Selection 8.4 Kernel Density Estimation in Practice "]
]
