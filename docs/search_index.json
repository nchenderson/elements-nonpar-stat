[
["index.html", "Elements of Nonparametric Statistics Preface", " Elements of Nonparametric Statistics Nicholas Henderson 2020-01-04 Preface This book will serve as the main source of course notes for Biostatistics 685/Statistics 560, Winter 2020. "],
["intro.html", "Chapter 1 Introduction 1.1 What is Nonparametric Statistics? 1.2 Outline of Course 1.3 Example 1: Nonparametric vs. Parametric Two-Sample Testing 1.4 Example 2: Nonparametric Estimation 1.5 Example 3: Confidence Intervals 1.6 Example 4: Nonparametric Regression with a Single Covariate 1.7 Example 5: Classification and Regression Trees (CART)", " Chapter 1 Introduction 1.1 What is Nonparametric Statistics? What is Parametric Statistics? Parametric models refer to probability distributions that can be fully described by a fixed number of parameters that do not change with the sample size. Typical examples include Gaussian Poisson Exponential Beta Could also refer to a regression setting where the mean function is described by a fixed number of parameters. What is Nonparametric Statistics? It is difficult to give a concise, all-encompassing definition, but nonparametric statistics generally refers to statistical methods where there is not a clear parametric component. A more practical definition is that nonparametric statistics refers to flexible statistical procedures where very few assumptions are made regarding the distribution of the data or the form of a regression model. The uses of nonparametric methods in several common statistical contexts are described in Sections 1.3 - 1.7. 1.2 Outline of Course This course is roughly divided into the following 5 categories. Nonparametric Testing Rank-based Tests Permutation Tests Estimation of Basic Nonparametric Quantities The Empirical Distribution Function Density Estimation Nonparametric Confidence Intervals Bootstrap Jacknife Nonparametric Regression Part I (Smoothing Methods) Kernel Methods Splines Local Regression Nonparametric Regression Part II (Machine Learning Methods) Decision Trees/CART Ensemble Methods 1.3 Example 1: Nonparametric vs. Parametric Two-Sample Testing Suppose we have data from two groups. For example, outcomes from two different treatments. Group 1 outcomes: \\(X_{1}, \\ldots, X_{n}\\) an i.i.d (independent and identically distributed) sample from distribution function \\(F_{X}\\). This means that \\[\\begin{equation} F_{X}(t) = P( X_{i} \\leq t) \\quad \\textrm{ for any } 1 \\leq i \\leq n \\nonumber \\end{equation}\\] Group 2 outcomes: \\(Y_{1}, \\ldots, Y_{m}\\) an i.i.d. sample from distribution function \\(F_{Y}\\). \\[\\begin{equation} F_{Y}(t) = P( Y_{i} \\leq t) \\quad \\textrm{ for any } 1 \\leq i \\leq n \\nonumber \\end{equation}\\] To test the impact of a new treatment, we usually want to test whether or not \\(F_{X}\\) differs from \\(F_{Y}\\) in some way. This can be stated in hypothesis testing language as \\[\\begin{eqnarray} H_{0}&amp;:&amp; F_{X} = F_{Y} \\quad \\textrm{( populations are the same)} \\nonumber \\\\ H_{A}&amp;:&amp; F_{X} \\neq F_{Y} \\quad \\textrm{( populations are different)} \\tag{1.1} \\end{eqnarray}\\] Parametric Tests Perhaps the most common parametric test for (1.1) is the t-test. The t-test assumes that \\[\\begin{equation} F_{X} = \\textrm{Normal}(\\mu_{x}, \\sigma^{2}) \\quad \\textrm{ and } \\quad F_{Y} = \\textrm{Normal}(\\mu_{y}, \\sigma^{2}) \\end{equation}\\] Under this parametric assumption, the hypothesis test (1.1) reduces to \\[\\begin{equation} H_{0}: \\mu_{x} = \\mu_{y} \\quad \\textrm{ vs. } \\quad H_{A}: \\mu_{x} \\neq \\mu_{y} \\end{equation}\\] The standard t-statistic (with a pooled estimate of \\(\\sigma^{2}\\)) is the following \\[\\begin{equation} T = \\frac{\\bar{X} - \\bar{Y}}{ s_{p}\\sqrt{\\frac{1}{n} + \\frac{1}{m}} }, \\end{equation}\\] where \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}\\) and \\(\\bar{Y} = \\frac{1}{m}\\sum_{i=1}^{m} Y_{i}\\) are the group-specific sample means and \\(s_{p}^{2}\\) is the pooled estimate of \\(\\sigma^{2}\\) \\[\\begin{equation} s_{p}^{2} = \\frac{1}{m + n - 2}\\Big\\{ \\sum_{i=1}^{n} (X_{i} - \\bar{X})^{2} + \\sum_{i=1}^{m} (Y_{i} - \\bar{Y})^{2} \\Big\\} \\end{equation}\\] The t-test is based on the null distribution of \\(T\\) - the distribution of \\(T\\) under the null hypothesis. Under the assumption of normality, the null distribution of \\(T\\) is a t distribution with \\(n + m - 2\\) degrees of freedom. Notice that the null distribution of \\(T\\) depends on the parametric assumption that both \\(F_{X} = \\textrm{Normal}(\\mu_{x}, \\sigma^{2})\\) and \\(F_{Y} = \\textrm{Normal}(\\mu_{y}, \\sigma^{2})\\). Appealing to the Central Limit Theorem, one could argue that is a quite reasonable assumption. In addition to using the assumption that \\(F_{X} = \\textrm{Normal}(\\mu_{x}, \\sigma^{2})\\) and \\(F_{Y} = \\textrm{Normal}(\\mu_{y}, \\sigma^{2})\\), we used this parametric assumption (at least implicitly) in the formulation of the hypothesis test itself because we assumed that any difference between \\(F_{X}\\) and \\(F_{Y}\\) would be fully described by difference in \\(\\mu_{x}\\) and \\(\\mu_{y}\\). So, in a sense, you are using the assumption of normality twice in the construction of the two-sample t-test. Nonparametric Tests Two-sample nonparametric tests are meant to be “distribution-free”. This means the null distribution of the test statistic does not depend on any parametric assumptions about the two populations \\(F_{X}\\) and \\(F_{Y}\\). Many such tests are based on ranks. The distribution of the ranks under the assumption that \\(F_{X} = F_{Y}\\) do not depend on the form of \\(F_{X}\\) (assuming \\(F_{X}\\) is continuous). Also, the statements of hypotheses tests for nonparametric tests should not rely on any parametric assumptions about \\(F_{X}\\) and \\(F_{Y}\\). For example, \\(H_{A}: F_{X} \\neq F_{Y}\\) or \\(H_{A}: F_{X} \\geq F_{Y}\\). Nonparametric tests usually tradeoff power for greater robustness. In general, if the parametric assumptions are correct, a nonparametric test will have less power than its parametric counterpart. If the parametric assumptions are not correct, parametric tests might have inappropriate type-I error control or lose power. 1.4 Example 2: Nonparametric Estimation Suppose we have \\(n\\) observations \\((X_{1}, \\ldots, X_{n})\\) which are assumed to be i.i.d. (independent and identically distributed). The distribution function of \\(X_{i}\\) is \\(F_{X}\\). Suppose we are interested in estimating the entire distribution function \\(F_{X}\\) rather than specific features of the distribution of \\(X_{i}\\) such as the mean or standard deviation. In a parametric approach to estimating \\(F_{X}\\), we would assume the distribution of \\(X_{i}\\) belongs to some parametric family of distributions. For example, \\(X_{i} \\sim \\textrm{Normal}(\\mu, \\sigma^{2})\\) \\(X_{i} \\sim \\textrm{Exponential}(\\lambda)\\) \\(X_{i} \\sim \\textrm{Beta}(\\alpha, \\beta)\\) If we assume that \\(X_{i} \\sim \\textrm{Normal}( \\mu, \\sigma^{2} )\\), we only need to estimate 2 parameters to fully describe the distribution of \\(X_{i}\\), and the number of parameters will not depend on the sample size. In a nonparametric approach to characterizing the distribution of \\(X_{i}\\), we need to instead estimate the entire distribution function \\(F_{X}\\) or density function \\(f_{X}\\). The distribution function \\(F_{X}\\) is usually estimated by the empirical distribution function \\[\\begin{equation} \\hat{F}_{n}(t) = \\frac{1}{n}\\sum_{i=1}^{n} I( X_{i} \\leq t), \\end{equation}\\] where \\(I()\\) denotes the indicator function. That is, \\(I( X_{i} \\leq t) = 1\\) if \\(X_{i} \\leq t\\), and \\(I(X_{i} \\leq t) = 0\\) if \\(X_{i} &gt; t\\). The empirical distribution function is a discrete distribution function, and it can be thought of as an estimate having \\(n\\) &quot;parameters. The density function of \\(X_{i}\\) is often estimated by a kernel density estimator (KDE). This is defined as \\[\\begin{equation} \\hat{f}_{n}(t) = \\frac{1}{n h_{n}} \\sum_{i=1}^{n} K\\Big( \\frac{t - X_{i}}{ h_{n} } \\Big). \\end{equation}\\] \\(K()\\) - the kernel function \\(h_{n}\\) - the bandwidth The KDE is a type of smoothing procedure. 1.5 Example 3: Confidence Intervals Inference for a wide range of statistical procedures is based on the following argument \\[\\begin{equation} \\hat{\\theta}_{n} \\textrm{ has an approximate Normal}\\Big( \\theta, \\widehat{\\textrm{Var}(\\hat{\\theta}_{n})} \\Big) \\textrm{ distribution } \\tag{1.2} \\end{equation}\\] Above, \\(\\hat{\\theta}_{n}\\) is an estimate of a parameter \\(\\theta\\), and \\(\\widehat{\\textrm{Var}(\\hat{\\theta}_{n})}\\) is an estimate of the variance of \\(\\hat{\\theta}_{n}\\). \\(se_{n} = \\sqrt{\\widehat{\\textrm{Var}(\\hat{\\theta}_{n})}}\\) is usually referred to as the standard error. \\(95\\%\\) confidence intervals are reported using the following formula \\[\\begin{equation} [\\hat{\\theta}_{n} - 1.96 se_{n}, \\hat{\\theta}_{n} + 1.96 se_{n} ] \\end{equation}\\] Common examples of this include: \\(\\hat{\\theta}_{n} = \\bar{X}_{n}\\). In this case, appeals to the Central Limit Theorem would justify approximation (1.2). The variance of \\(\\hat{\\theta}_{n}\\) would be \\(\\sigma^{2}\\), and the standard error would typically be \\(se_{n} = \\hat{\\sigma}/\\sqrt{n}\\). \\(\\hat{\\theta}_{n} = \\textrm{Maximum Likelihood Estimate of } \\theta\\). In this case, asymptotics would justify the approximate distribution \\(\\hat{\\theta}_{n} \\sim \\textrm{Normal}(\\theta, \\frac{1}{nI(\\theta)} )\\), where \\(I(\\theta)\\) denotes the Fisher information. The standard error in this context is often \\(se_{n} = \\{ n I(\\hat{\\theta}_{n}) \\}^{-1/2}\\). Confidence intervals using (1.2) rely on a parametric approximation to the sampling distribution of the statistic \\(\\hat{\\theta}_{n}\\). Moreover, even if one wanted to use something like (1.2), working out standard error formulas can be a great challenge in more complicated situations. The bootstrap is a simulation-based approach for computing standard errors and confidence intervals. The bootstrap does not rely on any particular parametric assumptions and can be applied in almost any context (though bootstrap confidence intervals can fail to work as desired in some situations). Through resampling from the original dataset, the bootstrap uses many possible alternative datasets to assess the variability in \\(\\hat{\\theta}_{n}\\). OriginalDat Dat1 Dat2 Dat3 Dat4 Obs. 1 0.20 0.20 0.80 0.20 0.30 Obs. 2 0.50 0.20 0.80 0.20 0.70 Obs. 3 0.30 0.30 0.50 0.80 0.20 Obs. 4 0.80 0.30 0.70 0.50 0.50 Obs. 5 0.70 0.70 0.20 0.30 0.20 theta.hat 0.50 0.34 0.60 0.40 0.38 In the above example, we have 4 boostrap replications for the statistic \\(\\hat{\\theta}\\): \\[\\begin{eqnarray} \\hat{\\theta}^{(1)} &amp;=&amp; 0.34 \\\\ \\hat{\\theta}^{(2)} &amp;=&amp; 0.60 \\\\ \\hat{\\theta}^{(3)} &amp;=&amp; 0.40 \\\\ \\hat{\\theta}^{(4)} &amp;=&amp; 0.38 \\end{eqnarray}\\] In the above example, the bootstrap standard error for \\(\\hat{\\theta}_{n}\\) would be the standard deviation of the bootstrap replications \\[\\begin{eqnarray} se_{boot} &amp;=&amp; \\Big( \\frac{1}{3} \\sum_{b=1}^{4} \\{ \\hat{\\theta}^{(b)} - \\hat{\\theta}^{(-)} \\}^{2} \\Big)^{1/2} \\nonumber \\\\ &amp;=&amp; \\Big( (0.34 - 0.43)^{2}/3 + (0.60 - 0.43)^{2}/3 + (0.40 - 0.43)^{2}/3 + (0.38 - 0.43)^{2}/3 \\Big)^{1/2} \\nonumber \\\\ &amp;=&amp; 0.116 \\end{eqnarray}\\] where \\(\\hat{\\theta}^{(-)} = 0.43\\) is the average of the bootstrap replications. One would then report the confidence interval \\([\\hat{\\theta} - 1.96 \\times 0.116, \\hat{\\theta} + 1.96 \\times 0.116]\\). In practice, the number of bootstrap replications is typically much larger than \\(4\\). It is often better to construct confidence intervals using the percentiles from the bootstrap distribution of \\(\\hat{\\theta}\\) rather than use a confidence interval of the form: \\(\\hat{\\theta} \\pm 1.96 \\times se_{boot}\\). Figure 1.1: Bootstrap distribution of the sample standard deviation for the age variable from the kidney fitness data. Dasjed vertical lines are placed at the 2.5 and 97.5 percentiles of the bootstrap distribution. 1.6 Example 4: Nonparametric Regression with a Single Covariate Regression is a common way of modeling the relationship between two different variables. Suppose we have \\(n\\) pairs of observations \\((y_{1}, x_{1}), \\ldots, (y_{n}, x_{n})\\) where \\(y_{i}\\) and \\(x_{i}\\) are suspected to have some association. Linear regression would assume that these \\(y_{i}\\) and \\(x_{i}\\) are related by the following \\[\\begin{equation} y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\varepsilon_{i} \\end{equation}\\] with the assumption \\(\\varepsilon_{i} \\sim \\textrm{Normal}(0, \\sigma^{2})\\) often made. In this model, there are only 3 parameters: \\((\\beta_{0}, \\beta_{1}, \\sigma^{2})\\), and the number of parameters stays fixed for all \\(n\\). The nonparametric counterpart to linear regression is usually formulated in the following way \\[\\begin{equation} y_{i} = m( x_{i} ) + \\varepsilon_{i} \\end{equation}\\] Typically, one makes very few assumptions about the form of the mean function \\(m\\), and it is not assumed \\(m\\) can be described by a finite number of parameters. There are a large number of nonparametric methods for estimating \\(m\\). One popular method is the use of smoothing splines. With smoothing splines, one considers mean functions of the form \\[\\begin{equation} m(x) = \\sum_{j=1}^{n} \\beta_{j}g_{j}(x) \\tag{1.3} \\end{equation}\\] where \\(g_{1}, \\ldots, g_{n}(x)\\) are a collection of spline basis functions. Because of the large number of parameters in (1.3), one should estimate the basis function weights \\(\\beta_{j}\\) through penalized regression \\[\\begin{equation} \\textrm{minimize} \\quad \\sum_{i=1}^{n} \\Big( y_{i} - \\sum_{j=1}^{n} \\beta_{j}g_{j}( x_{i} ) \\Big)^{2} + \\lambda \\sum_{i=1}^{n}\\sum_{j=1}^{n} \\Omega_{ij}\\beta_{i}\\beta_{j} \\tag{1.4} \\end{equation}\\] where \\(\\Omega_{ij} = \\int g_{i}&#39;&#39;(t)g_{j}&#39;&#39;(t) dt\\). Using coefficient estimates \\(\\hat{\\beta}_{1}, \\ldots, \\hat{\\beta}_{n}\\) found from solving (1.3), the nonparametric estimate of the mean function is defined as \\[\\begin{equation} \\hat{m}(x) = \\sum_{j=1}^{n} \\hat{\\beta}_{j}g_{j}(x) \\end{equation}\\] While the estimation in (1.4) resembles parametric estimation for linear regression, notice that the number of parameters to be estimated will change with the sample size. Allowing the number of basis functions to grow with \\(n\\) is important. For a sufficiently large number of basis functions, one should be able to approximate the true mean function \\(m(x)\\) arbitrarily closely. 1.7 Example 5: Classification and Regression Trees (CART) Suppose we now have observations \\((y_{1}, \\mathbf{x}_{1}), \\ldots, (y_{n}, \\mathbf{x}_{n})\\) where \\(y_{i}\\) is a continuous response and \\(\\mathbf{x}_{i}\\) is a p-dimensional vector of covariates. Regression trees are a nonparametric approach for predicting \\(y_{i}\\) from \\(\\mathbf{x}_{i}\\). Here, the regression function is a decision tree rather than some fitted curve. With a decision tree, a final prediction from a covariate vector \\(\\mathbf{x}_{i}\\) is obtained by answering a sequence of “yes or no” questions. When the responses \\(y_{i}\\) are binary, such trees are referred to as classification trees. Hence, the name: classification and regression trees (CART). Classification and regression trees are constructed through recursive partitioning. Recursive partitioning is the process of deciding if and how to split a given node into two child nodes. Tree splits are usually chosen to minimize the “within-node” sum of squares. The size of the final is determined by a process of “pruning” the tree with cross-validation determining the best place to stop pruning. Regression trees are an example of a more algorithmic approach to constructing predictions (as opposed to probability modeling in more traditional statistical methods) with a strong emphasis on predictive performance as measured through cross-validation. While single regression trees have the advantage of being directly interpretable, their prediction performance is often not that great. However, using collections of trees can be very effective for prediction and has been used in many popular learning methods. Examples include: random forests, boosting, and Bayesian additive regression trees (BART). Methods such as these can perform well on much larger datasets. We will discuss additional methods if time allows. "],
["getting-started.html", "Chapter 2 Working with R", " Chapter 2 Working with R "],
["rank-tests.html", "Chapter 3 Rank and Sign Statistics 3.1 Ranks 3.2 Two-Sample Tests 3.3 One Sample Tests 3.4 Comparisons with Parametric Tests 3.5 Thinking about Rank statistics more generally", " Chapter 3 Rank and Sign Statistics 3.1 Ranks 3.1.1 Definition Suppose we have \\(n\\) observations \\(X_{1}, \\ldots, X_{n}\\). The rank of the \\(i^{th}\\) observation \\(R_{i}\\) is defined as \\[\\begin{equation} R_{i} = R(X_{i}) = \\sum_{j=1}^{n} I( X_{j} \\leq X_{i}) \\tag{3.1} \\end{equation}\\] where \\[\\begin{equation} I(X_{j} \\leq X_{i}) = \\begin{cases} 1 &amp; \\text{ if } X_{j} \\leq X_{i} \\\\ 0 &amp; \\text{ if } X_{j} &gt; X_{i} \\end{cases} \\end{equation}\\] The largest observation has a rank of \\(n\\). The smallest observation has a rank of \\(1\\) (if there are no ties). x &lt;- c(3, 7, 1, 12, 6) ## 5 observations rank(x) ## [1] 2 4 1 5 3 In the definition of ranks shown in (3.1), tied observations receive their maximum possible rank. For example, suppose that \\((X_{1}, X_{2}, X_{3}, X_{4}) = (0, 1, 1, 2)\\). In this case, one could argue whether both observations 2 and 3 should be ranked \\(2^{nd}\\) or \\(3^{rd}\\) while observations \\(1\\) and \\(4\\) should unambiguously receive ranks of \\(1\\) and \\(4\\) respectively. Under definition @rank(eq:rankdef), both observations \\(2\\) and \\(3\\) receive a rank of \\(3\\). In R, such handling of ties is done using the ties.method = “max” argument x &lt;- c(0, 1, 1, 2) rank(x, ties.method=&quot;max&quot;) ## [1] 1 3 3 4 The default in R is to replace the ranks of tied observations with their “average” rank x &lt;- c(0, 1, 1, 2) rank(x) ## [1] 1.0 2.5 2.5 4.0 3.1.2 Properties of Ranks Suppose \\((X_{1}, \\ldots, X_{n})\\) is random sample from a continuous distribution \\(F\\) (so that the probability of ties is zero). Then, the following properties hold for the associated ranks \\(R_{1}, \\ldots, R_{n}\\). Each \\(R_{i}\\) follows a discrete uniform distribution \\[\\begin{equation} P(R_{i} = j) = 1/n, \\quad \\text{for any } j = 1, \\ldots,n. \\end{equation}\\] The expectation of \\(R_{i}\\) is \\[\\begin{equation} E( R_{i} ) = \\sum_{j=1}^{n} j P(R_{i} = j) = \\frac{1}{n}\\sum_{j=1}^{n} j = \\frac{(n+1)}{2} \\end{equation}\\] The variance of \\(R_{i}\\) is \\[\\begin{equation} \\text{Var}( R_{i} ) = E( R_{i}^{2} ) - E(R_{i})^{2} = \\frac{1}{n}\\sum_{j=1}^{n} j^{2} - \\Big( \\frac{n+1}{2} \\Big)^{2} = \\frac{ n^{2} - 1}{12} \\end{equation}\\] The random variables \\(R_{1}, \\ldots, R_{n}\\) are not independent (why?). However, the vector \\(\\mathbf{R}_{n} = (R_{1}, \\ldots, R_{n})\\) is uniformly distributed on the set of \\(n!\\) permutations of \\((1,2,\\ldots,n)\\). 3.2 Two-Sample Tests 3.2.1 The Wilcoxon Rank Sum (WRS) Test 3.2.1.1 Goal of the Test The Wilcoxon Rank Sum (WRS) test (sometimes referred to as the Wilcoxon-Mann-Whitney test) is a popular, rank-based two-sample test. The WRS test is used to test whether or not observations from one group tend to be larger (or smaller) than observations from the other group. Suppose we have observations from two groups: \\(X_{1}, \\ldots, X_{n} \\sim F_{X}\\) and \\(Y_{1}, \\ldots, Y_{m} \\sim F_{Y}\\). Roughly speaking, the WRS tests the following hypothesis \\[\\begin{eqnarray} H_{0}: &amp; &amp; F_{X} = F_{Y} \\quad \\textrm{ versus } \\nonumber \\\\ H_{A}: &amp; &amp; \\textrm{Observations from } F_{X} \\textrm{ tend to be larger than observations from } F_{Y} \\nonumber \\end{eqnarray}\\] What is meant by “tend to be larger” in the alternative hypothesis? Two common ways of stating the alternative hypothesis for the WRS include The stochastic dominance alternative \\[\\begin{eqnarray} H_{0}: &amp; &amp; F_{X} = F_{Y} \\quad \\textrm{ versus } \\nonumber \\\\ H_{A}: &amp; &amp; F_{X} \\textrm{ is stochastically larger than } F_{Y} \\nonumber \\end{eqnarray}\\] The “shift” alternative \\[\\begin{eqnarray} H_{0}: &amp; &amp; F_{X} = F_{Y} \\quad \\textrm{ versus } \\nonumber \\\\ H_{A}: &amp; &amp; F_{X}(t) = F_{Y}(t - \\Delta), \\Delta &gt; 0. \\end{eqnarray}\\] A distribution function \\(F_{X}\\) is said to be stochastically larger than \\(F_{Y}\\) if \\(F_{X}(t) \\geq F_{Y}(t)\\) for all \\(t\\) with \\(F_{X}(t) &gt; F_{Y}(t)\\) for at least one value of \\(t\\). Note that the “shift alternative” implies stochastic dominance. Why do we need to specify an alternative? 3.2.1.2 Definition of the WRS Test Statistic The WRS test statistic is based on computing the sum of ranks (ranks based on the pooled sample) in one group. If observations from group 1 tend to be larger than those from group 2, the average rank from group 1 should exceed the average rank from group 2. A sufficiently large value of the average rank from group 1 will allow to reject \\(H_{0}\\) in favor of \\(H_{A}\\). Let us define the pooled sample data vector \\(\\mathbf{Z}\\) as \\[\\begin{equation} \\mathbf{Z} = (X_{1}, \\ldots, X_{n}, Y_{1}, \\ldots, Y_{m}) \\end{equation}\\] 3.2.1.3 Computing p-values Give exercise, compute p-values for Wilcoxon test where we have two populations. both are Normally distributed with mean zero but different variances. 3.3 One Sample Tests 3.3.1 The Sign Test Suppose we have observations \\(W_{1}, \\ldots, W_{n}\\) which arise from the following model \\[\\begin{equation} W_{i} = \\theta + \\varepsilon_{i}, \\nonumber \\end{equation}\\] where \\(\\varepsilon_{i}\\) are iid random variables each with distribution function \\(F\\) that is assumed to have a median of zero. 3.3.2 The Signed-Rank Wilcoxon Test 3.4 Comparisons with Parametric Tests 3.5 Thinking about Rank statistics more generally "],
["tidy.html", "Chapter 4 Rank Tests for Multiple Groups", " Chapter 4 Rank Tests for Multiple Groups In Subsection "],
["permutation.html", "Chapter 5 Permutation Tests (Next Chapter Should be U-statistics)", " Chapter 5 Permutation Tests (Next Chapter Should be U-statistics) Permutation tests are … "],
["ustat.html", "Chapter 6 U-Statistics 6.1 Examples 6.2 Mann-Whitney Statistic 6.3 Kendall’s tau 6.4 Distance Covariance", " Chapter 6 U-Statistics 6.1 Examples A wide range of well-known statistics can also be represented as U-statistics. Sample Mean, Variance, Signed-Rank Statistic, Gini Mean Difference 6.2 Mann-Whitney Statistic 6.3 Kendall’s tau 6.4 Distance Covariance "],
["regression.html", "Chapter 7 The Empirical Distribution Function 7.1 Empirical Distribution Functions 7.2 The Empirical Distribution Function in R 7.3 The empirical distribution functino and statistical functionals", " Chapter 7 The Empirical Distribution Function 7.1 Empirical Distribution Functions 7.1.1 Definition and Basic Properties Every random variable has a cumulative distribution function (cdf). The cdf of a random variable \\(X\\) is defined as \\[\\begin{equation} F(t) = P( X \\leq t) \\end{equation}\\] The empirical distribution function or empirical cumulative distribution function (ecdf) estimates \\(F(t)\\) by just finding the proportion of observations which are less than or equal to \\(t\\). For i.i.d. random variables \\(X_{1}, \\ldots, X_{n}\\), the empirical distribution function is defined as \\[\\begin{equation} \\hat{F}_{n}(t) = \\frac{1}{n}\\sum_{i=1}^{n} I( X_{i} \\leq t) \\nonumber \\end{equation}\\] Note that the empirical distribution function can be computed for any type of data without making any assumptions about the distribution from which the data arose. The only assumption we are making is that \\(X_{1}, \\ldots, X_{n}\\) constitute an i.i.d. sample from some common distribution function \\(F\\). 7.1.2 Confidence intervals for Fhat 7.2 The Empirical Distribution Function in R 7.3 The empirical distribution functino and statistical functionals "],
["density-estimation.html", "Chapter 8 Density Estimation 8.1 Introduction 8.2 Histograms 8.3 Kernel Density Estimation 8.4 Kernel Density Estimation in Practice", " Chapter 8 Density Estimation 8.1 Introduction In this section, we focus on methods for estimating a probability density function (pdf) \\(f(x)\\). For a continuous random variable \\(X\\), areas under the probability density function are probabilities \\[\\begin{equation} P(a &lt; X &lt; b) = \\int_{a}^{b} f(x) dx \\nonumber \\end{equation}\\] and \\(f(x)\\) is related to the distribution function via \\(f(x) = F&#39;(x)\\). With parametric approaches to density estimation, you only need to estimate a couple of parameters as these parameters completely determine the form of \\(f(x)\\). For example, with a Gaussian distribution you only need to estimate \\(\\mu\\) and \\(\\sigma^{2}\\) to draw the appropriate bell curve. In a nonparametric approach, you assume that your observations \\(X_{1}, \\ldots, X_{n}\\) are an independent sample from a distribution having pdf \\(f(x)\\), but otherwise you make few assumptions about the particular form of \\(f(x)\\). 8.2 Histograms 8.2.1 Definition Histograms are one of the oldest ways to estimate a pdf. To construct a histogram, you first need to define a series of “bins”: \\(B_{1}, \\ldots, B_{D_{n}}\\). Each bin is a left-closed interval which is often assumed to have the form \\(B_{k} = [x_{0} + (k-1)h_{n}, x_{0} + kh_{n})\\): \\[\\begin{eqnarray} B_{1} &amp;=&amp; [x_{0}, x_{0} + h_{n}) \\nonumber \\\\ B_{2} &amp;=&amp; [x_{0} + h_{n}, x_{0} + 2h_{n}) \\nonumber \\\\ &amp;\\vdots&amp; \\nonumber \\\\ B_{D_{n}} &amp;=&amp; [x_{0} + (D-1)h, x_{0} + D_{n}h_{n}) \\nonumber \\end{eqnarray}\\] \\(x_{0}\\) - the origin \\(h_{n}\\) - bin width \\(D_{n}\\) - number of bins For each bin, you first need to count the number of observations which fall into that bin \\[\\begin{eqnarray} n_{k} &amp;=&amp; \\# \\text{ of observations falling into the $k^{th}$ bin } \\nonumber \\\\ &amp;=&amp; \\sum_{i=1}^{n} I( x_{0} + (k-1)h_{n} \\leq X_{i} &lt; x_{0} + kh_{n} ) \\end{eqnarray}\\] The histogram estimate of the density at a point \\(x\\) in the \\(k^{th}\\) bin is then defined as \\[\\begin{equation} \\hat{f}(x) = \\frac{n_{k}}{nh_{n}} \\end{equation}\\] Note: What is often shown in histogram plots are the actual bin counts \\(n_{k}\\) rather than the values of \\(\\hat{f}(x)\\). To see the motivation for the histogram estimate, notice that if we choose a relatively small value \\(h_{n} &gt; 0\\) \\[\\begin{equation} P(x &lt; X_{i} &lt; x + h_{n}) = \\int_{x}^{x + h_{n}} f(x) dx \\approx h_{n}f(x) \\end{equation}\\] The expected value of \\(\\hat{f}(x)\\) is \\[\\begin{equation} E\\{ \\hat{f}(x) \\} = \\frac{1}{h_{n}} P( x_{0} + (k-1)h_{n} \\leq X_{i} &lt; x_{0} + kh_{n} ) \\approx f(x) \\end{equation}\\] 8.2.2 Histograms in R In R, use the hist function hist(x, breaks, probability, plot, ...) The breaks argument Default is “Sturges”. This is a method for finding the binwidth. Can be a name giving the name of an algorithm for computing binwidth (e.g., “Scott” and “FD”). Can also be a single number. This gives the number of bins used. Could also be a .. The probability argument The plot argument Note: The default for R, is to use right-closed intervals \\((a, b]\\). This can be changed using the right argument of the hist function. ## Use a real dataset here bodywt.hist &lt;- hist(nhgh$wt, main=&quot;&quot;, xlab=&quot;Body Weight from NHANES&quot;) ## Use a real dataset here bodywt.hist2 &lt;- hist(nhgh$wt, main=&quot;Hist of BW on Probability Scale&quot;, xlab=&quot;Body Weight from NHANES&quot;, probability=TRUE) In addition to generating a histogram plot, the histogram function also returns useful stuff. names(bodywt.hist) ## [1] &quot;breaks&quot; &quot;counts&quot; &quot;density&quot; &quot;mids&quot; &quot;xname&quot; &quot;equidist&quot; breaks counts mids density bodywt.hist$breaks ## [1] 20 40 60 80 100 120 140 160 180 200 220 240 bodywt.hist$counts ## [1] 44 1160 2705 1846 721 212 71 24 7 2 3 binwidth &lt;- bodywt.hist$breaks[2] - bodywt.hist$breaks[1] bodywt.hist$density ## [1] 3.237675e-04 8.535688e-03 1.990434e-02 1.358352e-02 5.305372e-03 ## [6] 1.559971e-03 5.224430e-04 1.766004e-04 5.150846e-05 1.471670e-05 ## [11] 2.207506e-05 bodywt.hist$counts/(length(nhgh$wt)*binwidth) ## [1] 3.237675e-04 8.535688e-03 1.990434e-02 1.358352e-02 5.305372e-03 ## [6] 1.559971e-03 5.224430e-04 1.766004e-04 5.150846e-05 1.471670e-05 ## [11] 2.207506e-05 8.2.3 Performance of the Histogram Estimate 8.2.3.1 Bias/Variance Decomposition It is common to evaluate the performance of a density estimator through its mean-squared error (MSE). In general, MSE is a function of bias and variance \\[\\begin{equation} MSE = Bias^2 + Variance \\end{equation}\\] We will first look at the mean-squared error of \\(\\hat{f}( x )\\) at a single point \\(x\\) \\[\\begin{eqnarray} \\textrm{MSE}\\{ \\hat{f}(x) \\} &amp;=&amp; E\\Big( \\{ \\hat{f}(x) - f(x) \\}^{2} \\Big) \\nonumber \\\\ &amp;=&amp; \\underbrace{\\Big( E\\{ \\hat{f}(x) \\} - f(x) \\Big)^{2} }_{\\textrm{Bias Squared}} + \\underbrace{\\textrm{Var}\\{ \\hat{f}(x) \\}}_{\\textrm{Variance}} \\nonumber \\end{eqnarray}\\] In general, as the bin width \\(h_{n}\\) increases, the histogram estimate has less variation but becomes more biased. 8.2.3.2 Bias and Variance of the Histogram Estimate Recall that, for a histogram estimate, we have \\(D_{n}\\) bins where the \\(k^{th}\\) bin takes the form \\[\\begin{equation} B_{k} = [x_{0} + (k-1)h_{n}, x_{0} + kh_{n}) \\nonumber \\end{equation}\\] For a point \\(x \\in B_{k}\\), that “belongs” to the \\(k^{th}\\) bin, the histogram density estimate is \\[\\begin{equation} \\hat{f}(x) = \\frac{n_{k}}{nh_{n}}, \\quad \\textrm{ where } n_{k} = \\textrm{ number of observations falling into bin } B_{k} \\end{equation}\\] To better examine what happens as \\(n\\) changes, we will define the function \\(A_{h_{n}}(x)\\) as the function which returns the index of the interval to which \\(x\\) belongs. For example, if we had three bins \\(B_{1} = [0, 1/3)\\), \\(B_{2} = [1/3, 2/3)\\), \\(B_{3} = [2/3, 1)\\) and \\(x = 1/2\\), then \\(A_{h_{n}}( x ) = 2\\). So, we can also write the histogram density estimate as \\[\\begin{equation} \\hat{f}(x) = \\frac{n_{A_{h_{n}}(x)}}{nh_{n}} \\end{equation}\\] Note that \\(n_{A_{h_{n}}(x)}\\) is a binomial random variable with \\(n\\) trials and success probability \\(p_{h_{n}}(x)\\) (why?) \\[\\begin{equation} n_{A_{h_{n}}(x)} \\sim \\textrm{Binomial}\\{ n, p_{h_{n}}(x) \\} \\nonumber \\end{equation}\\] The success probability \\(p_{h_{n}}(x)\\) is defined as \\[\\begin{equation} p_{h_{n}}(x) = P\\Big\\{ X_{i} \\textrm{ falls into bin } A_{h_{n}}(x) \\Big\\} = \\int_{x_{0} + (A_{h_{n}}(x) - 1)h_{n}}^{x_{0} + A_{h_{n}}(x)h_{n} } f(t) dt. \\end{equation}\\] Using what is known about the Binomial distribution (i.e., \\(E( n_{A_{h_{n}}(x)} ) = np_{h_{n}}(x)\\) and \\(\\textrm{Var}( n_{A_{h_{n}}(x)} ) = np_{h_{n}}(x)\\{1 - p_{h_{n}}(x) \\}\\)), we can express the bias and variance of \\(\\hat{f}(x)\\) as \\[\\begin{eqnarray} \\textrm{Bias}\\{ \\hat{f}(x) \\} &amp;=&amp; E\\{ \\hat{f}(x) \\} - f(x) \\nonumber \\\\ &amp;=&amp; \\frac{1}{nh_{n}}E( n_{A_{h_{n}}(x)} ) - f(x) \\nonumber \\\\ &amp;=&amp; \\frac{ p_{h_{n}}(x) }{ h_{n} } - f(x) \\nonumber \\end{eqnarray}\\] and \\[\\begin{eqnarray} \\textrm{Var}\\{ \\hat{f}(x) \\} = \\frac{1}{n^{2}h_{n}^{2}}\\textrm{Var}( n_{A_{h_{n}}(x)} ) = \\frac{ p_{h_{n}}(x)\\{1 - p_{h_{n}}(x) \\} }{ nh_{n}^{2} } \\end{eqnarray}\\] Using the approximation \\(f(t) \\approx f(x) + f&#39;(x)(t - x)\\) for \\(t\\) close to \\(x\\), we have that \\[\\begin{equation} \\frac{ p_{h_{n}}(x) }{ h_{n} } = \\frac{1}{h_{n}}\\int_{x_{0} + (A_{h_{n}}(x) - 1)h_{n}}^{x_{0} + A_{h_{n}}(x)h_{n} } f(t) dt \\approx f(x) + f&#39;(x)\\{ x - x_{0} - (A_{h_{n}}(x) - 1)h_{n} \\} \\end{equation}\\] So, the bias of the histogram density estimate \\(\\hat{f}(x)\\) is \\[\\begin{equation} \\textrm{Bias}\\{ \\hat{f}(x) \\} \\approx f&#39;(x)\\{ x - (x_{0} + (A_{h_{n}}(x) - 1)h_{n}) \\} \\end{equation}\\] [[ Double-check this bias formula and check with Scott ]] Choosing a very small bin width will result in a small bias because the left endpoint of the bin \\(x_{0} + (A_{h_{n}}(x) - 1)h_{n}\\) will always be very close to \\(x\\). Now, turning to the variance of the histogram estimate \\[\\begin{equation} \\textrm{Var}\\{ \\hat{f}(x) \\} = \\frac{p_{h_{n}}(x) }{nh_{n}^{2}}\\{1 - p_{h_{n}}(x)\\} \\approx \\frac{f(x) + f&#39;(x)\\{ x - x_{0} - (A_{h_{n}}(x) - 1)h_{n} \\}}{nh_{n}}\\{1 - p_{h_{n}}(x)\\} \\approx \\frac{f(x)}{n h_{n} } \\end{equation}\\] For a more detailed description of the above approximation see Scott. Note that large bin widths will reduce variance. 8.2.3.3 Point-wise Mean Squared Error Recalling (), the approximate mean-squared error of the histogram density estimate at a particular point \\(x\\) is given by \\[\\begin{eqnarray} \\textrm{MSE}\\{ \\hat{f}(x) \\} &amp;=&amp; E\\Big( \\{ \\hat{f}(x) - f(x) \\}^{2} \\Big) \\nonumber \\\\ &amp;=&amp; \\Big( \\textrm{Bias}\\{ \\hat{f}(x) \\} \\Big)^{2} + \\textrm{Var}\\{ \\hat{f}(x) \\} \\nonumber \\\\ &amp;\\approx&amp; [f&#39;(x)]^{2}\\{ x - (x_{0} + (A_{h_{n}}(x) - 1)h_{n}) \\}^{2} + \\frac{f(x)}{n h_{n} } \\end{eqnarray}\\] For any approach to bin width selection, we should have \\(h_{n} \\longrightarrow 0\\) and \\(nh_{n} \\longrightarrow \\infty\\). This MSE approximation depends on a particular choice of \\(x\\). Difficult to use () as a criterion for selecting the bandwidth because this could vary depending on your choice of \\(x\\). 8.2.3.4 Integrated Mean Squared Error and Optimal Histogram Bin Width Using mean integrated squared error (MISE) allows us to find an optimal bin width that does not depend on a particular choice of \\(x\\). The MISE is defined as \\[\\begin{eqnarray} MISE\\{ \\hat{f}(x) \\} &amp;=&amp; E\\Big\\{ \\int_{-\\infty}^{\\infty} \\{ \\hat{f}(x) - f(x) \\}^{2}dx \\Big\\} \\nonumber \\\\ &amp;=&amp; \\int_{-\\infty}^{\\infty} \\textrm{MSE}\\{ \\hat{f}(x) \\} dx \\end{eqnarray}\\] Using our previously derived approximation for the MSE, we have \\[\\begin{eqnarray} MISE\\{ \\hat{f}(x) \\} &amp;\\approx&amp; \\int x [f&#39;(x)]^{2} - x_{0}\\int [f&#39;(x)]^{2} dx + (A_{h_{n}}(x) - 1)h_{n}) \\}^{2} + \\frac{1}{n h_{n} } \\int f(x) dx \\nonumber \\\\ &amp;=&amp; \\end{eqnarray}\\] To select the optimal bin width, we minimize the MISE as a function of \\(h_{n}\\). Minimizing (), as a function of \\(h_{n}\\) yields the following formula for the optimal bin width \\[\\begin{equation} h_{n}^{opt} = \\Big( \\frac{6}{n \\int_{-\\infty}^{\\infty} [f&#39;(x)]^{2} dx} \\Big)^{1/3} = C n^{-1/3} \\nonumber \\end{equation}\\] Notice that \\(h_{n}^{opt} \\longrightarrow 0\\) and \\(nh_{n}^{opt} \\longrightarrow \\infty\\) as \\(n \\longrightarrow \\infty\\). Notice also that the optimal bin width depends on the unknown quantity \\(\\int_{-\\infty}^{\\infty} [f&#39;(x)]^{2} dx\\). 8.2.4 Choosing the Histogram Bin Width We will mention three rules for selecting the bin width of a histogram. Scott rule: (based on the optimal bin width formula) Friedman and Diaconis rule (also based on the optimal bin width formula) Sturges rule: (based on …) Both Scott and the FD rule are based on the optimal bin width formula (). The main problem with this formula is the presence of \\(\\int_{-\\infty}^{\\infty} [f&#39;(x)]^{2} dx\\). Solution: See what this quantity looks like if we assume that \\(f(x)\\) corresponds to a \\(N(\\mu, \\sigma^{2})\\) density. With this assumption, \\[\\begin{equation} h_{n}^{opt} = 3.5 \\sigma n^{-1/3} \\end{equation}\\] Scott rule: use \\(\\hat{\\sigma} = 2\\) 8.3 Kernel Density Estimation 8.3.1 Histograms and a “Naive” Density Estimate 8.3.2 Kernels, Bandwidth, and Smooth Density Estimation 8.3.3 Bias and Variance of Kernel Density Estimates 8.3.4 Bandwidth Selection 8.4 Kernel Density Estimation in Practice "],
["sampling.html", "Chapter 9 The Jackknife 9.1 Introduction", " Chapter 9 The Jackknife 9.1 Introduction The jackknife and bootstrap are nonparametric procedures for finding standard errors and constructing confidence intervals. Why use the jackknife or bootstrap? To compute … To find … When you have no idea how to compute reasonable standard errors. "],
["ci.html", "Chapter 10 The Bootstrap and Confidence Intervals 10.1 Bootstrapping", " Chapter 10 The Bootstrap and Confidence Intervals 10.1 Bootstrapping "],
["kernel-regression.html", "Chapter 11 Kernel Regression 11.1 Introduction 11.2 The Nadaraya-Watson estimator 11.3 Local Linear and Polynomial Regression", " Chapter 11 Kernel Regression 11.1 Introduction 11.1.1 An Example 11.1.2 Linear Smoothers and Naive Nonparametric Estimates 11.2 The Nadaraya-Watson estimator 11.3 Local Linear and Polynomial Regression "],
["inference-for-regression.html", "Chapter 12 Splines and Penalized Regression 12.1 Introduction 12.2 Spline Basis Functions 12.3 Smoothing Splines/Penalized Regression", " Chapter 12 Splines and Penalized Regression 12.1 Introduction 12.2 Spline Basis Functions 12.3 Smoothing Splines/Penalized Regression 12.3.1 Selection of Smoothing Parameter "],
["decision-tree.html", "Chapter 13 Decision Trees and CART", " Chapter 13 Decision Trees and CART "],
["ensemble.html", "Chapter 14 Ensemble Methods for Prediction", " Chapter 14 Ensemble Methods for Prediction "]
]
