[
["index.html", "Elements of Nonparametric Statistics Preface", " Elements of Nonparametric Statistics Nicholas Henderson 2020-02-03 Preface This book will serve as the main source of course notes for Biostatistics 685/Statistics 560, Winter 2020. "],
["intro.html", "Chapter 1 Introduction 1.1 What is Nonparametric Statistics? 1.2 Outline of Course 1.3 Example 1: Nonparametric vs. Parametric Two-Sample Testing 1.4 Example 2: Nonparametric Estimation 1.5 Example 3: Confidence Intervals 1.6 Example 4: Nonparametric Regression with a Single Covariate 1.7 Example 5: Classification and Regression Trees (CART)", " Chapter 1 Introduction 1.1 What is Nonparametric Statistics? What is Parametric Statistics? Parametric models refer to probability distributions that can be fully described by a fixed number of parameters that do not change with the sample size. Typical examples include Gaussian Poisson Exponential Beta Could also refer to a regression setting where the mean function is described by a fixed number of parameters. What is Nonparametric Statistics? It is difficult to give a concise, all-encompassing definition, but nonparametric statistics generally refers to statistical methods where there is not a clear parametric component. A more practical definition is that nonparametric statistics refers to flexible statistical procedures where very few assumptions are made regarding the distribution of the data or the form of a regression model. The uses of nonparametric methods in several common statistical contexts are described in Sections 1.3 - 1.7. 1.2 Outline of Course This course is roughly divided into the following 5 categories. Nonparametric Testing Rank-based Tests Permutation Tests Estimation of Basic Nonparametric Quantities The Empirical Distribution Function Density Estimation Nonparametric Confidence Intervals Bootstrap Jacknife Nonparametric Regression Part I (Smoothing Methods) Kernel Methods Splines Local Regression Nonparametric Regression Part II (Machine Learning Methods) Decision Trees/CART Ensemble Methods 1.3 Example 1: Nonparametric vs. Parametric Two-Sample Testing Suppose we have data from two groups. For example, outcomes from two different treatments. Group 1 outcomes: \\(X_{1}, \\ldots, X_{n}\\) an i.i.d (independent and identically distributed) sample from distribution function \\(F_{X}\\). This means that \\[\\begin{equation} F_{X}(t) = P( X_{i} \\leq t) \\quad \\textrm{ for any } 1 \\leq i \\leq n \\nonumber \\end{equation}\\] Group 2 outcomes: \\(Y_{1}, \\ldots, Y_{m}\\) an i.i.d. sample from distribution function \\(F_{Y}\\). \\[\\begin{equation} F_{Y}(t) = P( Y_{i} \\leq t) \\quad \\textrm{ for any } 1 \\leq i \\leq n \\nonumber \\end{equation}\\] To test the impact of a new treatment, we usually want to test whether or not \\(F_{X}\\) differs from \\(F_{Y}\\) in some way. This can be stated in hypothesis testing language as \\[\\begin{eqnarray} H_{0}&amp;:&amp; F_{X} = F_{Y} \\quad \\textrm{( populations are the same)} \\nonumber \\\\ H_{A}&amp;:&amp; F_{X} \\neq F_{Y} \\quad \\textrm{( populations are different)} \\tag{1.1} \\end{eqnarray}\\] Parametric Tests Perhaps the most common parametric test for (1.1) is the t-test. The t-test assumes that \\[\\begin{equation} F_{X} = \\textrm{Normal}(\\mu_{x}, \\sigma^{2}) \\quad \\textrm{ and } \\quad F_{Y} = \\textrm{Normal}(\\mu_{y}, \\sigma^{2}) \\end{equation}\\] Under this parametric assumption, the hypothesis test (1.1) reduces to \\[\\begin{equation} H_{0}: \\mu_{x} = \\mu_{y} \\quad \\textrm{ vs. } \\quad H_{A}: \\mu_{x} \\neq \\mu_{y} \\end{equation}\\] The standard t-statistic (with a pooled estimate of \\(\\sigma^{2}\\)) is the following \\[\\begin{equation} T = \\frac{\\bar{X} - \\bar{Y}}{ s_{p}\\sqrt{\\frac{1}{n} + \\frac{1}{m}} }, \\end{equation}\\] where \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}\\) and \\(\\bar{Y} = \\frac{1}{m}\\sum_{i=1}^{m} Y_{i}\\) are the group-specific sample means and \\(s_{p}^{2}\\) is the pooled estimate of \\(\\sigma^{2}\\) \\[\\begin{equation} s_{p}^{2} = \\frac{1}{m + n - 2}\\Big\\{ \\sum_{i=1}^{n} (X_{i} - \\bar{X})^{2} + \\sum_{i=1}^{m} (Y_{i} - \\bar{Y})^{2} \\Big\\} \\end{equation}\\] The t-test is based on the null distribution of \\(T\\) - the distribution of \\(T\\) under the null hypothesis. Under the assumption of normality, the null distribution of \\(T\\) is a t distribution with \\(n + m - 2\\) degrees of freedom. Notice that the null distribution of \\(T\\) depends on the parametric assumption that both \\(F_{X} = \\textrm{Normal}(\\mu_{x}, \\sigma^{2})\\) and \\(F_{Y} = \\textrm{Normal}(\\mu_{y}, \\sigma^{2})\\). Appealing to the Central Limit Theorem, one could argue that is a quite reasonable assumption. In addition to using the assumption that \\(F_{X} = \\textrm{Normal}(\\mu_{x}, \\sigma^{2})\\) and \\(F_{Y} = \\textrm{Normal}(\\mu_{y}, \\sigma^{2})\\), we used this parametric assumption (at least implicitly) in the formulation of the hypothesis test itself because we assumed that any difference between \\(F_{X}\\) and \\(F_{Y}\\) would be fully described by difference in \\(\\mu_{x}\\) and \\(\\mu_{y}\\). So, in a sense, you are using the assumption of normality twice in the construction of the two-sample t-test. Nonparametric Tests Two-sample nonparametric tests are meant to be “distribution-free”. This means the null distribution of the test statistic does not depend on any parametric assumptions about the two populations \\(F_{X}\\) and \\(F_{Y}\\). Many such tests are based on ranks. The distribution of the ranks under the assumption that \\(F_{X} = F_{Y}\\) do not depend on the form of \\(F_{X}\\) (assuming \\(F_{X}\\) is continuous). Also, the statements of hypotheses tests for nonparametric tests should not rely on any parametric assumptions about \\(F_{X}\\) and \\(F_{Y}\\). For example, \\(H_{A}: F_{X} \\neq F_{Y}\\) or \\(H_{A}: F_{X} \\geq F_{Y}\\). Nonparametric tests usually tradeoff power for greater robustness. In general, if the parametric assumptions are correct, a nonparametric test will have less power than its parametric counterpart. If the parametric assumptions are not correct, parametric tests might have inappropriate type-I error control or lose power. 1.4 Example 2: Nonparametric Estimation Suppose we have \\(n\\) observations \\((X_{1}, \\ldots, X_{n})\\) which are assumed to be i.i.d. (independent and identically distributed). The distribution function of \\(X_{i}\\) is \\(F_{X}\\). Suppose we are interested in estimating the entire distribution function \\(F_{X}\\) rather than specific features of the distribution of \\(X_{i}\\) such as the mean or standard deviation. In a parametric approach to estimating \\(F_{X}\\), we would assume the distribution of \\(X_{i}\\) belongs to some parametric family of distributions. For example, \\(X_{i} \\sim \\textrm{Normal}(\\mu, \\sigma^{2})\\) \\(X_{i} \\sim \\textrm{Exponential}(\\lambda)\\) \\(X_{i} \\sim \\textrm{Beta}(\\alpha, \\beta)\\) If we assume that \\(X_{i} \\sim \\textrm{Normal}( \\mu, \\sigma^{2} )\\), we only need to estimate 2 parameters to fully describe the distribution of \\(X_{i}\\), and the number of parameters will not depend on the sample size. In a nonparametric approach to characterizing the distribution of \\(X_{i}\\), we need to instead estimate the entire distribution function \\(F_{X}\\) or density function \\(f_{X}\\). The distribution function \\(F_{X}\\) is usually estimated by the empirical distribution function \\[\\begin{equation} \\hat{F}_{n}(t) = \\frac{1}{n}\\sum_{i=1}^{n} I( X_{i} \\leq t), \\end{equation}\\] where \\(I()\\) denotes the indicator function. That is, \\(I( X_{i} \\leq t) = 1\\) if \\(X_{i} \\leq t\\), and \\(I(X_{i} \\leq t) = 0\\) if \\(X_{i} &gt; t\\). The empirical distribution function is a discrete distribution function, and it can be thought of as an estimate having \\(n\\) “parameters”. Kernel density estimation is probably the most common nonparametric method for estimating a probability distribution function \\(f_{X}(t) = F_{X}&#39;(t)\\). The density function of \\(X_{i}\\) is often estimated by a kernel density estimator (KDE). This is defined as \\[\\begin{equation} \\hat{f}_{n}(t) = \\frac{1}{n h_{n}} \\sum_{i=1}^{n} K\\Big( \\frac{t - X_{i}}{ h_{n} } \\Big). \\end{equation}\\] \\(K()\\) - the kernel function \\(h_{n}\\) - the bandwidth The KDE is a type of smoothing procedure. 1.5 Example 3: Confidence Intervals Inference for a wide range of statistical procedures is based on the following argument \\[\\begin{equation} \\hat{\\theta}_{n} \\textrm{ has an approximate Normal}\\Big( \\theta, \\widehat{\\textrm{Var}(\\hat{\\theta}_{n})} \\Big) \\textrm{ distribution } \\tag{1.2} \\end{equation}\\] Above, \\(\\hat{\\theta}_{n}\\) is an estimate of a parameter \\(\\theta\\), and \\(\\widehat{\\textrm{Var}(\\hat{\\theta}_{n})}\\) is an estimate of the variance of \\(\\hat{\\theta}_{n}\\). \\(se_{n} = \\sqrt{\\widehat{\\textrm{Var}(\\hat{\\theta}_{n})}}\\) is usually referred to as the standard error. \\(95\\%\\) confidence intervals are reported using the following formula \\[\\begin{equation} [\\hat{\\theta}_{n} - 1.96 se_{n}, \\hat{\\theta}_{n} + 1.96 se_{n} ] \\end{equation}\\] Common examples of this include: \\(\\hat{\\theta}_{n} = \\bar{X}_{n}\\). In this case, appeals to the Central Limit Theorem would justify approximation (1.2). The variance of \\(\\hat{\\theta}_{n}\\) would be \\(\\sigma^{2}/n\\), and the standard error would typically be \\(se_{n} = \\hat{\\sigma}/\\sqrt{n}\\). \\(\\hat{\\theta}_{n} = \\textrm{Maximum Likelihood Estimate of } \\theta\\). In this case, asymptotics would justify the approximate distribution \\(\\hat{\\theta}_{n} \\sim \\textrm{Normal}(\\theta, \\frac{1}{nI(\\theta)} )\\), where \\(I(\\theta)\\) denotes the Fisher information. The standard error in this context is often \\(se_{n} = \\{ n I(\\hat{\\theta}_{n}) \\}^{-1/2}\\). Confidence intervals using (1.2) rely on a parametric approximation to the sampling distribution of the statistic \\(\\hat{\\theta}_{n}\\). Moreover, even if one wanted to use something like (1.2), working out standard error formulas can be a great challenge in more complicated situations. The bootstrap is a simulation-based approach for computing standard errors and confidence intervals. The bootstrap does not rely on any particular parametric assumptions and can be applied in almost any context (though bootstrap confidence intervals can fail to work as desired in some situations). Through resampling from the original dataset, the bootstrap uses many possible alternative datasets to assess the variability in \\(\\hat{\\theta}_{n}\\). OriginalDat Dat1 Dat2 Dat3 Dat4 Obs. 1 0.20 0.20 0.80 0.20 0.30 Obs. 2 0.50 0.20 0.80 0.20 0.70 Obs. 3 0.30 0.30 0.50 0.80 0.20 Obs. 4 0.80 0.30 0.70 0.50 0.50 Obs. 5 0.70 0.70 0.20 0.30 0.20 theta.hat 0.50 0.34 0.60 0.40 0.38 In the above example, we have 4 boostrap replications for the statistic \\(\\hat{\\theta}\\): \\[\\begin{eqnarray} \\hat{\\theta}^{(1)} &amp;=&amp; 0.34 \\\\ \\hat{\\theta}^{(2)} &amp;=&amp; 0.60 \\\\ \\hat{\\theta}^{(3)} &amp;=&amp; 0.40 \\\\ \\hat{\\theta}^{(4)} &amp;=&amp; 0.38 \\end{eqnarray}\\] In the above example, the bootstrap standard error for \\(\\hat{\\theta}_{n}\\) would be the standard deviation of the bootstrap replications \\[\\begin{eqnarray} se_{boot} &amp;=&amp; \\Big( \\frac{1}{3} \\sum_{b=1}^{4} \\{ \\hat{\\theta}^{(b)} - \\hat{\\theta}^{(-)} \\}^{2} \\Big)^{1/2} \\nonumber \\\\ &amp;=&amp; \\Big( (0.34 - 0.43)^{2}/3 + (0.60 - 0.43)^{2}/3 + (0.40 - 0.43)^{2}/3 + (0.38 - 0.43)^{2}/3 \\Big)^{1/2} \\nonumber \\\\ &amp;=&amp; 0.116 \\end{eqnarray}\\] where \\(\\hat{\\theta}^{(-)} = 0.43\\) is the average of the bootstrap replications. One would then report the confidence interval \\([\\hat{\\theta} - 1.96 \\times 0.116, \\hat{\\theta} + 1.96 \\times 0.116]\\). In practice, the number of bootstrap replications is typically much larger than \\(4\\). It is often better to construct confidence intervals using the percentiles from the bootstrap distribution of \\(\\hat{\\theta}\\) rather than use a confidence interval of the form: \\(\\hat{\\theta} \\pm 1.96 \\times se_{boot}\\). Figure 1.1: Bootstrap distribution of the sample standard deviation for the age variable from the kidney fitness data. Dasjed vertical lines are placed at the 2.5 and 97.5 percentiles of the bootstrap distribution. 1.6 Example 4: Nonparametric Regression with a Single Covariate Regression is a common way of modeling the relationship between two different variables. Suppose we have \\(n\\) pairs of observations \\((y_{1}, x_{1}), \\ldots, (y_{n}, x_{n})\\) where \\(y_{i}\\) and \\(x_{i}\\) are suspected to have some association. Linear regression would assume that these \\(y_{i}\\) and \\(x_{i}\\) are related by the following \\[\\begin{equation} y_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\varepsilon_{i} \\end{equation}\\] with the assumption \\(\\varepsilon_{i} \\sim \\textrm{Normal}(0, \\sigma^{2})\\) often made. In this model, there are only 3 parameters: \\((\\beta_{0}, \\beta_{1}, \\sigma^{2})\\), and the number of parameters stays fixed for all \\(n\\). The nonparametric counterpart to linear regression is usually formulated in the following way \\[\\begin{equation} y_{i} = m( x_{i} ) + \\varepsilon_{i} \\end{equation}\\] Typically, one makes very few assumptions about the form of the mean function \\(m\\), and it is not assumed \\(m\\) can be described by a finite number of parameters. There are a large number of nonparametric methods for estimating \\(m\\). One popular method is the use of smoothing splines. With smoothing splines, one considers mean functions of the form \\[\\begin{equation} m(x) = \\sum_{j=1}^{n} \\beta_{j}g_{j}(x) \\tag{1.3} \\end{equation}\\] where \\(g_{1}, \\ldots, g_{n}(x)\\) are a collection of spline basis functions. Because of the large number of parameters in (1.3), one should estimate the basis function weights \\(\\beta_{j}\\) through penalized regression \\[\\begin{equation} \\textrm{minimize} \\quad \\sum_{i=1}^{n} \\Big( y_{i} - \\sum_{j=1}^{n} \\beta_{j}g_{j}( x_{i} ) \\Big)^{2} + \\lambda \\sum_{i=1}^{n}\\sum_{j=1}^{n} \\Omega_{ij}\\beta_{i}\\beta_{j} \\tag{1.4} \\end{equation}\\] where \\(\\Omega_{ij} = \\int g_{i}&#39;&#39;(t)g_{j}&#39;&#39;(t) dt\\). Using coefficient estimates \\(\\hat{\\beta}_{1}, \\ldots, \\hat{\\beta}_{n}\\) found from solving (1.3), the nonparametric estimate of the mean function is defined as \\[\\begin{equation} \\hat{m}(x) = \\sum_{j=1}^{n} \\hat{\\beta}_{j}g_{j}(x) \\end{equation}\\] While the estimation in (1.4) resembles parametric estimation for linear regression, notice that the number of parameters to be estimated will change with the sample size. Allowing the number of basis functions to grow with \\(n\\) is important. For a sufficiently large number of basis functions, one should be able to approximate the true mean function \\(m(x)\\) arbitrarily closely. 1.7 Example 5: Classification and Regression Trees (CART) Suppose we now have observations \\((y_{1}, \\mathbf{x}_{1}), \\ldots, (y_{n}, \\mathbf{x}_{n})\\) where \\(y_{i}\\) is a continuous response and \\(\\mathbf{x}_{i}\\) is a p-dimensional vector of covariates. Regression trees are a nonparametric approach for predicting \\(y_{i}\\) from \\(\\mathbf{x}_{i}\\). Here, the regression function is a decision tree rather than some fitted curve. With a decision tree, a final prediction from a covariate vector \\(\\mathbf{x}_{i}\\) is obtained by answering a sequence of “yes or no” questions. When the responses \\(y_{i}\\) are binary, such trees are referred to as classification trees. Hence, the name: classification and regression trees (CART). Classification and regression trees are constructed through recursive partitioning. Recursive partitioning is the process of deciding if and how to split a given node into two child nodes. Tree splits are usually chosen to minimize the “within-node” sum of squares. The size of the final tree is determined by a process of “pruning” the tree with cross-validation determining the best place to stop pruning. Regression trees are an example of a more algorithmic approach to constructing predictions (as opposed to probability modeling in more traditional statistical methods) with a strong emphasis on predictive performance as measured through cross-validation. While single regression trees have the advantage of being directly interpretable, their prediction performance is often not that great. However, using collections of trees can be very effective for prediction and has been used in many popular learning methods. Examples include: random forests, boosting, and Bayesian additive regression trees (BART). Methods such as these can perform well on much larger datasets. We will discuss additional methods if time allows. "],
["getting-started.html", "Chapter 2 Working with R", " Chapter 2 Working with R You can download R by visiting https://www.r-project.org/ and clicking on the download R link. Follow the instructions to complete installation. THe most recent version is version 3.6.2. It is not necessary to use this, but I find RStudio to be a very useful integrated development environment (IDE) for computing with R. RStudio may be downloaded and installed by visiting https://rstudio.com/ "],
["rank-tests.html", "Chapter 3 Rank and Sign Statistics 3.1 Ranks 3.2 The Wilcoxon Rank Sum (WRS) Test: A Two-Sample Test 3.3 One Sample Tests 3.4 Power and Comparisons with Parametric Tests 3.5 Linear Rank Statistics in General 3.6 Additional Reading", " Chapter 3 Rank and Sign Statistics 3.1 Ranks 3.1.1 Definition Suppose we have \\(n\\) observations \\(\\mathbf{X} = (X_{1}, \\ldots, X_{n})\\). The rank of the \\(i^{th}\\) observation \\(R_{i}\\) is defined as \\[\\begin{equation} R_{i} = R_{i}(\\mathbf{X}) = \\sum_{j=1}^{n} I( X_{i} \\geq X_{j}) \\tag{3.1} \\end{equation}\\] where \\[\\begin{equation} I(X_{i} \\geq X_{j}) = \\begin{cases} 1 &amp; \\text{ if } X_{i} \\geq X_{j} \\\\ 0 &amp; \\text{ if } X_{i} &lt; X_{j} \\end{cases} \\end{equation}\\] The largest observation has a rank of \\(n\\). The smallest observation has a rank of \\(1\\) (if there are no ties). I’m using the notation \\(R_{i}(\\mathbf{X})\\) to emphasize that the rank of the \\(i^{th}\\) observations depends on the entire vector of observations rather than only the value of \\(X_{i}\\). You can compute ranks in R using the rank function: x &lt;- c(3, 7, 1, 12, 6) ## 5 observations rank(x) ## [1] 2 4 1 5 3 3.1.2 Handling Ties In the definition of ranks shown in (3.1), tied observations receive their maximum possible rank. For example, suppose that \\((X_{1}, X_{2}, X_{3}, X_{4}) = (0, 1, 1, 2)\\). In this case, one could argue whether both observations 2 and 3 should be ranked \\(2^{nd}\\) or \\(3^{rd}\\) while observations \\(1\\) and \\(4\\) should unambiguously receive ranks of \\(1\\) and \\(4\\) respectively. Under definition (3.1), both observations \\(2\\) and \\(3\\) receive a rank of \\(3\\). In R, handling ties that is consistent with definition (3.1) is done using the ties.method = “max” argument x &lt;- c(0, 1, 1, 2) rank(x, ties.method=&quot;max&quot;) ## [1] 1 3 3 4 The default in R is to replace the ranks of tied observations with their “average” rank x &lt;- c(0, 1, 1, 2) rank(x) ## [1] 1.0 2.5 2.5 4.0 y &lt;- c(2, 9, 7, 7, 3, 2, 1) rank(y, ties.method=&quot;max&quot;) ## [1] 3 7 6 6 4 3 1 rank(y) ## [1] 2.5 7.0 5.5 5.5 4.0 2.5 1.0 When defining ranks using the “average” or “midrank” approach to handling ties, replaces tied ranks with the average of the two “adjacent” ranks. For example, if we have a vector of ranks \\((R_{1}, R_{2}, R_{3}, R_{4})\\) where \\(R_{2} = R_{3} =3\\) and \\(R_{1} = 4\\) and \\(R_{4} = 1\\), then the vector of modified ranks using the “average” approach to handling ties would be \\[\\begin{equation} (R_{1}&#39;, R_{2}&#39;, R_{3}&#39;, R_{4}&#39;) = \\Big( 4, \\frac{4 + 1}{2}, \\frac{4 + 1}{2}, 1 \\Big) \\end{equation}\\] The “average” approach is the most common way of handling ties when computing the Wilcoxon rank sum statistic. 3.1.3 Properties of Ranks Suppose \\((X_{1}, \\ldots, X_{n})\\) is random sample from a continuous distribution \\(F\\) (so that the probability of ties is zero). Then, the following properties hold for the associated ranks \\(R_{1}, \\ldots, R_{n}\\). Each \\(R_{i}\\) follows a discrete uniform distribution \\[\\begin{equation} P(R_{i} = j) = 1/n, \\quad \\text{for any } j = 1, \\ldots,n. \\end{equation}\\] The expectation of \\(R_{i}\\) is \\[\\begin{equation} E( R_{i} ) = \\sum_{j=1}^{n} j P(R_{i} = j) = \\frac{1}{n}\\sum_{j=1}^{n} j = \\frac{(n+1)}{2} \\tag{3.2} \\end{equation}\\] The variance of \\(R_{i}\\) is \\[\\begin{equation} \\text{Var}( R_{i} ) = E( R_{i}^{2} ) - E(R_{i})^{2} = \\frac{1}{n}\\sum_{j=1}^{n} j^{2} - \\Big( \\frac{n+1}{2} \\Big)^{2} = \\frac{ n^{2} - 1}{12} \\end{equation}\\] The random variables \\(R_{1}, \\ldots, R_{n}\\) are not independent (why?). However, the vector \\(\\mathbf{R}_{n} = (R_{1}, \\ldots, R_{n})\\) is uniformly distributed on the set of \\(n!\\) permutations of \\((1,2,\\ldots,n)\\). Exercise 3.1: Suppose \\(X_{1}, X_{2}, X_{3}\\) are i.i.d. observations from a continuous distribution function \\(F_{X}\\). Compute the covariance matrix of the vector of ranks \\(\\big( R_{1}(\\mathbf{X}), R_{2}(\\mathbf{X}), R_{3}( \\mathbf{X} ) \\big)\\). Exercise 3.2: Again, suppose that \\(X_{1}, X_{2}, X_{3}, X_{4}\\) are i.i.d. observations from a continuous distribution function \\(F_{X}\\). Let \\(T= R_{1}( \\mathbf{X} ) + R_{2}(\\mathbf{X})\\). Compute \\(P( T = j )\\) for \\(j = 3, 4, 5, 6, 7\\). 3.2 The Wilcoxon Rank Sum (WRS) Test: A Two-Sample Test 3.2.1 Goal of the Test The Wilcoxon Rank Sum (WRS) test (sometimes referred to as the Wilcoxon-Mann-Whitney test) is a popular, rank-based two-sample test. The WRS test is used to test whether or not observations from one group tend to be larger (or smaller) than observations from the other group. Suppose we have observations from two groups: \\(X_{1}, \\ldots, X_{n} \\sim F_{X}\\) and \\(Y_{1}, \\ldots, Y_{m} \\sim F_{Y}\\). Roughly speaking, the WRS tests the following hypothesis \\[\\begin{eqnarray} H_{0}: &amp;&amp; F_{X} = F_{Y} \\quad \\textrm{ versus } \\\\ H_{A}: &amp;&amp; \\textrm{Observations from } F_{X} \\textrm{ tend to be larger than observations from } F_{Y} \\nonumber \\tag{3.3} \\end{eqnarray}\\] What is meant by “tend to be larger” in the alternative hypothesis? Two common ways of stating the alternative hypothesis for the WRS include The stochastic dominance alternative \\[\\begin{eqnarray} H_{0}: &amp; &amp; F_{X} = F_{Y} \\quad \\textrm{ versus } \\nonumber \\\\ H_{A}: &amp; &amp; F_{X} \\textrm{ is stochastically larger than } F_{Y} \\tag{3.4} \\end{eqnarray}\\] The “shift” alternative \\[\\begin{eqnarray} H_{0}: &amp; &amp; F_{X} = F_{Y} \\quad \\textrm{ versus } \\nonumber \\\\ H_{A}: &amp; &amp; F_{X}(t) = F_{Y}(t - \\Delta), \\Delta &gt; 0. \\tag{3.5} \\end{eqnarray}\\] A distribution function \\(F_{X}\\) is said to be stochastically larger than \\(F_{Y}\\) if \\(F_{X}(t) \\leq F_{Y}(t)\\) for all \\(t\\) with \\(F_{X}(t) &lt; F_{Y}(t)\\) for at least one value of \\(t\\). Note that the “shift alternative” implies stochastic dominance. Why do we need to specify an alternative? It is often stated that the WRS test is a test of equal medians. This is true under the assumption that the relevant alternative is of the form \\(F_{X}(t) = F_{Y}(t - \\Delta)\\). However, one could have a scenario where the two groups have equal medians, but the WRS test has a very high probability of rejecting \\(H_{0}\\). In addition, in many applications, it is difficult to justify that the “shift alternative” is a reasonable model. An alternative is to view the WRS test as performing the following hypothesis test: \\[\\begin{eqnarray} H_{0}: &amp;&amp; P(X_{i} &gt; Y_{j}) + \\tfrac{1}{2}P(X_{i} = Y_{j}) = 1/2 \\quad \\textrm{ versus } \\\\ H_{A}: &amp;&amp; P(X_{i} &gt; Y_{j}) + \\tfrac{1}{2}P(X_{i} = Y_{j}) &gt; 1/2 \\tag{3.6} \\end{eqnarray}\\] See Divine et al. (2018) for more discussion around this formulation of the WRS test. The hypothesis test (3.6) makes fewer assumptions about how \\(F_{X}\\) and \\(F_{Y}\\) are related and is, in many cases, more interpretable. For example, in medical applications, it is often more natural to answer the question: what is the probability that the outcome under treatment 1 is better than the outcome under treatment 2. The justification of hypothesis test (3.6) comes through the close connection between the WRS test statistic \\(W\\) and the Mann-Whitney statistic \\(U_{MW}\\). Specifically, \\(W = U_{MW} + n(n+1)/2\\). (Although, often \\(U_{MW}\\) is defined as \\(U_{MW} = mn + n(n+1)/2 - W\\)). The Mann-Whitney statistic divided by \\(mn\\) is an estimate of the probability: \\[\\begin{equation} P(X_{i} &gt; Y_{j}) + \\tfrac{1}{2}P(X_{i} = Y_{j}) = 1/2. \\end{equation}\\] The reason for stating \\(H_{0}\\) in (3.6) as \\[\\begin{equation} H_{0}: P(X_{i} &gt; Y_{j}) + \\tfrac{1}{2}P(X_{i} = Y_{j}) = 1/2 \\quad \\textrm{ versus } \\\\ \\end{equation}\\] is to cover the case of either a continuous or discrete distribution. When both \\(X_{i}\\) and \\(Y_{j}\\) are samples from a continuous distribution we will have \\(P(X_{i} = Y_{j}) = 0\\), and we should then think of the null hypothesis as \\(H_{0}: P(X_{i} &gt; Y_{j})\\). For the case when both \\(X_{i}\\) and \\(Y_{j}\\) have a discrete distribution, consider an example where \\(X_{i}\\) and \\(Y_{j}\\) have the same discrete distribution with probabilities \\(P(X_{i} = 0) = p_{0}, P(X_{i} = 1) = p_{1}\\), and \\(P(X_{i} = 2) = 1 - p_{0} - p_{2}\\). With this common discrete distribution on \\(\\{0, 1, 2\\}\\), we can see that \\(P(X_{i} &gt; Y_{j}) + \\tfrac{1}{2}P(X_{i} = Y_{j}) = 1/2\\) because \\[\\begin{eqnarray} P(X_{i} &gt; Y_{j}) + \\frac{1}{2}P(X_{i} = Y_{j}&amp;)&amp; = P(X_{i}=1, Y_{j}=0) + P(X_{i} = 2, Y_{j}=0) + P(X_{i}=2, Y_{j}=1) \\nonumber \\\\ &amp;+&amp; \\frac{1}{2}\\Big[P(X_{i}=0, Y_{j}=0) + P(X_{i} = 1, Y_{j}=1) + P(X_{i}=2, Y_{j}=2) \\Big] \\nonumber \\\\ &amp;=&amp; p_{1}p_{0} + (1 - p_{1} - p_{0})p_{0} + (1 - p_{1} - p_{0})p_{1} \\nonumber \\\\ &amp;+&amp; p_{0}^{2} + p_{1}^{2} + \\frac{1}{2} - p_{0} - p_{1} + p_{0}p_{1} \\nonumber \\\\ &amp;=&amp; 1/2 \\nonumber \\end{eqnarray}\\] 3.2.2 Definition of the WRS Test Statistic The WRS test statistic is based on computing the sum of ranks (ranks based on the pooled sample) in one group. If observations from group 1 tend to be larger than those from group 2, the average rank from group 1 should exceed the average rank from group 2. A sufficiently large value of the average rank from group 1 will allow us to reject \\(H_{0}\\) in favor of \\(H_{A}\\). We will define the pooled data vector \\(\\mathbf{Z}\\) as \\[\\begin{equation} \\mathbf{Z} = (X_{1}, \\ldots, X_{n}, Y_{1}, \\ldots, Y_{m}) \\end{equation}\\] This is a vector with length \\(n + m\\). The Wilcoxon rank-sum test statistic \\(W\\) for testing hypotheses of the form (3.3) is then defined as \\[\\begin{equation} W = \\sum_{i=1}^{n} R_{i}( \\mathbf{Z} ) \\tag{3.7} \\end{equation}\\] In other words, the WRS test statistic is the sum of the ranks for those observations coming from group 1 (i.e., the group with the \\(X_{i}\\) as observations). If the group 1 observations tend to, in fact, be larger than the group 2 observations, then we should expect the sum of the ranks in this group to be larger than the sum of the ranks from group 2. Under \\(H_{0}\\), we can treat both \\(X_{i}\\) and \\(Y_{i}\\) as being observations coming from a common distribution function \\(F\\). Hence, the expectation of \\(R_{i}(\\mathbf{Z})\\) under the null hypothesis is \\[\\begin{equation} E_{H_{0}}\\{ R_{i}(\\mathbf{Z}) \\} = \\frac{n + m + 1}{2} \\end{equation}\\] and thus the expectation of \\(W\\) under \\(H_{0}\\) \\[\\begin{equation} E_{H_{0}}( W ) = \\sum_{i=1}^{n} E_{H_{0}}\\{ R_{i}( \\mathbf{Z} ) \\} = \\frac{ n(n + m + 1) }{ 2 } \\end{equation}\\] It can be shown that the variance of \\(W\\) under the null hypothesis is \\[\\begin{equation} \\textrm{Var}_{H_{0}}( W ) = \\frac{mn(m + n + 1)}{12} \\end{equation}\\] 3.2.3 Computing p-values for the WRS Test Exact Distribution The p-value is found by computing the probability \\[\\begin{equation} \\textrm{p-value} = P_{H_{0}}( W \\geq w_{obs}) \\end{equation}\\] where \\(w_{obs}\\) is the observed WRS test statistic that we get from our data. Computing p-values for the WRS test requires us to work with the null distribution of \\(W\\). That is, the distribution of \\(W\\) under the assumption that \\(F_{X} = F_{Y}\\). The exact null distribution is found by using the fact that each possible ordering of the ranks has the same probability. That is, \\[\\begin{equation} P\\{ R_{1}(\\mathbf{Z}) = r_{1}, \\ldots, R_{n+m}(\\mathbf{Z}) = r_{n+m} \\} = \\frac{1}{(n + m)!}, \\end{equation}\\] where \\((r_{1}, \\ldots, r_{n+m})\\) is any permutation of the set \\(\\{1, 2, \\ldots, n + m\\}\\). Note that the null distribution only depends on \\(n\\) and \\(m\\). Also, there are \\({n + m \\choose n}\\) possible ways to assign distinct ranks to group 1. Consider an example with \\(n = m = 2\\). In this case, there are \\({4 \\choose 2} = 6\\) distinct ways to assign 2 ranks to group 1. What is the null distribution of the WRS test statistic? Try to verify that \\[\\begin{eqnarray} P_{H_{0}}( W = 7) &amp;=&amp; 1/6 \\nonumber \\\\ P_{H_{0}}( W = 6 ) &amp;=&amp; 1/6 \\nonumber \\\\ P_{H_{0}}(W = 5) &amp;=&amp; 1/3 \\nonumber \\\\ P_{H_{0}}( W = 4 ) &amp;=&amp; 1/6 \\nonumber \\\\ P_{H_{0}}(W = 3) &amp;=&amp; 1/6. \\nonumber \\end{eqnarray}\\] Large-Sample Approximate Distribution Looking at (3.7), we can see that the WRS test statistic is a sum of nearly independent random variables (at least nearly independent for large \\(n\\) and \\(m\\)). Thus, we can expect that an appropriately centered and scaled version of \\(W\\) should be approximately Normally distributed (recall the Central Limit Theorem). The standardized version \\(\\tilde{W}\\) of the WRS is defined as \\[\\begin{equation} \\tilde{W} = \\frac{W - E_{H_{0}}(W)}{ \\sqrt{\\textrm{Var}_{H_{0}}(W) } } = \\frac{W - n(n+m+1)/2}{ \\sqrt{ mn(n + m + 1)/12 } } \\end{equation}\\] Under \\(H_{0}\\), \\(\\tilde{W}\\) converges in distribution to a Normal\\((0,1)\\) random variable. A p-value using this large-sample approximation would then be computed in the following way \\[\\begin{eqnarray} \\textrm{p-value} &amp;=&amp; P_{H_{0}}( W \\geq w_{obs}) = P\\Bigg( \\frac{W - n(n+m+1)/2}{ \\sqrt{ mn(n + m + 1)/12 } } \\geq \\frac{w_{obs} - n(n+m+1)/2}{ \\sqrt{ mn(n + m + 1)/12 } }\\Bigg) \\nonumber \\\\ &amp;=&amp; P_{H_{0}}\\Big( \\tilde{W} \\geq \\frac{w_{obs} - n(n+m+1)/2}{ \\sqrt{ mn(n + m + 1)/12 } }\\Big) = 1 - \\Phi\\Bigg( \\frac{w_{obs} - n(n+m+1)/2}{ \\sqrt{ mn(n + m + 1)/12 } } \\Bigg), \\nonumber \\end{eqnarray}\\] where \\(\\Phi(t)\\) denotes the cumulative distribution function of a standard Normal random variable. Often, in practice, a continuity correction is applied when using this large-sample approximation. For example, we would compute the probability \\(P_{H_{0}}(W \\geq w_{obs} - 0.5)\\) with the Normal approximation rather than \\(P_{H_{0}}(W \\geq w_{obs})\\) directly. Many statistical software packages (including R) will not compute p-values using the exact distribution in the presence of ties. The coin package in R does allow you to perform a permutation test in the presence of ties. A “two-sided” Wilcoxon rank sum test can also be performed. The two-sided hypothesis tests could either be stated as \\[\\begin{eqnarray} H_{0}: &amp; &amp; F_{X} = F_{Y} \\quad \\textrm{ versus } \\nonumber \\\\ H_{A}: &amp; &amp; F_{X} \\textrm{ is stochastically larger or smaller than } F_{Y} \\end{eqnarray}\\] or \\[\\begin{eqnarray} H_{0}: &amp; &amp; F_{X} = F_{Y} \\quad \\textrm{ versus } \\nonumber \\\\ H_{A}: &amp; &amp; F_{X}(t) = F_{Y}(t - \\Delta), \\Delta \\neq 0. \\end{eqnarray}\\] or \\[\\begin{eqnarray} H_{0}: &amp;&amp; P(X_{i} &gt; Y_{i}) + \\tfrac{1}{2}P(X_{i} = Y_{i}) = 1/2 \\quad \\textrm{ versus } \\\\ H_{A}: &amp;&amp; P(X_{i} &gt; Y_{i}) + \\tfrac{1}{2}P(X_{i} = Y_{i}) \\neq 1/2 \\end{eqnarray}\\] Exercise 3.3. Using the exact distribution, what is the smallest possible one-sided p-value associated with the WRS test for a fixed value of \\(n\\) and \\(m\\) (assuming the probability of ties is zero)? 3.2.4 Computing the WRS test in R To illustrate performing the WRS test in R, we can use the wine dataset from the rattle.data package. This dataset is also available from the UCI Machine Learning Repository. library(rattle.data) head(wine) ## Type Alcohol Malic Ash Alcalinity Magnesium Phenols Flavanoids Nonflavanoids ## 1 1 14.23 1.71 2.43 15.6 127 2.80 3.06 0.28 ## 2 1 13.20 1.78 2.14 11.2 100 2.65 2.76 0.26 ## 3 1 13.16 2.36 2.67 18.6 101 2.80 3.24 0.30 ## 4 1 14.37 1.95 2.50 16.8 113 3.85 3.49 0.24 ## 5 1 13.24 2.59 2.87 21.0 118 2.80 2.69 0.39 ## 6 1 14.20 1.76 2.45 15.2 112 3.27 3.39 0.34 ## Proanthocyanins Color Hue Dilution Proline ## 1 2.29 5.64 1.04 3.92 1065 ## 2 1.28 4.38 1.05 3.40 1050 ## 3 2.81 5.68 1.03 3.17 1185 ## 4 2.18 7.80 0.86 3.45 1480 ## 5 1.82 4.32 1.04 2.93 735 ## 6 1.97 6.75 1.05 2.85 1450 This dataset contains three types of wine. We will only consider the first two. wine2 &lt;- subset(wine, Type==1 | Type==2) wine2$Type &lt;- factor(wine2$Type) Let’s consider the difference in the level of magnesium across the two types of wine. Suppose we are interested in testing whether or not magnesium levels in Type 1 wine are generally larger than magnesium levels in Type 2 wine. This can be done with the following code wilcox.test(x=wine2$Magnesium[wine2$Type==1], y=wine2$Magnesium[wine2$Type==2], alternative=&quot;greater&quot;) ## ## Wilcoxon rank sum test with continuity correction ## ## data: wine2$Magnesium[wine2$Type == 1] and wine2$Magnesium[wine2$Type == 2] ## W = 3381.5, p-value = 8.71e-10 ## alternative hypothesis: true location shift is greater than 0 You could also use the following code (just be careful about the ordering of the levels of Type) wilcox.test(Magnesium ~ Type, data=wine2, alternative=&quot;greater&quot;) ## ## Wilcoxon rank sum test with continuity correction ## ## data: Magnesium by Type ## W = 3381.5, p-value = 8.71e-10 ## alternative hypothesis: true location shift is greater than 0 What is the value of the WRS test statistic? We can code this directly with the following steps: W &lt;- wilcox.test(x=wine2$Magnesium[wine2$Type==1], y=wine2$Magnesium[wine2$Type==2]) n &lt;- sum(wine2$Type==1) m &lt;- sum(wine2$Type==2) zz &lt;- rank(wine2$Magnesium) ## vector of pooled ranks sum(zz[wine2$Type==1]) ## The WRS test statistic ## [1] 5151.5 The statistic returned by the wilcox.test function is actually equal to \\(W - n(n+1)/2\\) not \\(W\\) sum(zz[wine2$Type==1]) - n*(n + 1)/2 ## [1] 3381.5 W$statistic ## W ## 3381.5 \\(\\{ W - n(n+1)/2 \\}\\) is equal to the Mann-Whitney statistic. Thus, W$statistic/(mn) is an estimate of the probability \\(P(X_{i} &gt; Y_{j}) + P(X_{i} = Y_{j})/2\\). W$statistic/(m*n) ## W ## 0.8072332 Let’s check how the Mann-Whitney statistic matches a simulation-based estimate of this probability ind1 &lt;- which(wine2$Type==1) ind2 &lt;- which(wine2$Type==2) xgreater &lt;- rep(0, 100) for(k in 1:100) { xi &lt;- sample(ind1, size=1) yi &lt;- sample(ind2, size=1) xgreater[k] &lt;- ifelse(wine2$Magnesium[xi] &gt; wine2$Magnesium[yi], 1, 0) + ifelse(wine2$Magnesium[xi] == wine2$Magnesium[yi], 1/2, 0) } mean(xgreater) ## estimate of this probability ## [1] 0.845 3.2.5 Additional Notes for the WRS test 3.2.5.1 Comparing Ordinal Data The WRS test is often suggested when comparing categorical data which are ordinal. For example, we might have 4 categories: Poor Fair Good Excellent In this case, there is a natural ordering of the categories but any numerical values assigned to these categories would be arbitrary. In such cases, we might be interested in testing whether or not outcomes tend to be better in one group than the other rather than simply comparing whether or not the distribution is different between the two groups. A WRS test is useful here since we can still compute ranks without having to choose aribtrary numbers for each category. Thinking of the “probability greater than alternative (3.6)” or “stochastically larger than alternative (3.4)” interpretation of the WRS test is probably more reasonable than the “shift alternative (3.5)” interpretation. Note that there will probably be many ties when comparing ordinal data. The Hodges-Lehmann Estimator \\(\\hat{\\Delta}\\) is an estimator of \\(\\Delta\\) in the location-shift model \\[\\begin{equation} F_{X}(t) = F_{Y}(t - \\Delta) \\nonumber \\end{equation}\\] The Hodges-Lehmann is defined as the median difference among all possible (group 1, group 2) pairs. Specifically, \\[\\begin{equation} \\hat{\\Delta} = \\textrm{median}\\{ (X_{i} - Y_{j}); i=1,\\ldots,n; j=1,\\ldots,m \\} \\nonumber \\end{equation}\\] We won’t discuss the Hodges-Lehmann estimator in detail in this course, but in many statistical software packages, the Hodges-Lehmann is often reported when computing the WRS test. In R, the Hodges-Lehmann estimator can be obtained by using the conf.int=TRUE argument in the wilcox.test function WC &lt;- wilcox.test(x=wine2$Magnesium[wine2$Type==1], y=wine2$Magnesium[wine2$Type==2], conf.int=TRUE) WC$estimate ## The Hodges-Lehmann estimate ## difference in location ## 14.00005 3.3 One Sample Tests 3.3.1 The Sign Test 3.3.1.1 Motivation and Definition The sign test can be thought of as a test of whether or not the median of a distribution is greater than zero (or greater than some other fixed value \\(\\theta_{0}\\)). Frequently, the sign test is explained in the following context: Suppose we have observations \\(D_{1}, \\ldots, D_{n}\\) which arise from the model \\[\\begin{equation} D_{i} = \\theta + \\varepsilon_{i}, \\tag{3.8} \\end{equation}\\] where \\(\\varepsilon_{i}\\) are iid random variables each with distribution function \\(F_{\\epsilon}\\) that is assumed to have a median of zero. Moreover, we will assume the density function \\(f_{\\varepsilon}(t)\\) is symmetric around zero. The distribution function of \\(D_{i}\\) is then \\[\\begin{equation} F_{D}(t) = P(D_{i} \\leq t) = P(\\varepsilon_{i} \\leq t - \\theta) = F_{\\epsilon}(t - \\theta) \\end{equation}\\] Likewise the density function \\(f_{D}(t)\\) of \\(D_{i}\\) is given by \\[\\begin{equation} f_{D}(t) = f_{\\epsilon}(t - \\theta) \\end{equation}\\] In this context, \\(\\theta\\) is usually referred to as a location parameter. The goal here is to test \\(H_{0}: \\theta = \\theta_{0}\\) vs. \\(H_{A}: \\theta &gt; \\theta_{0}\\). (Often, \\(\\theta_{0} = 0\\)). This sort of test usually comes up in the context of paired data. Common examples include patients compared “pre and post treatment” students before and after the introduction of a new teaching method comparison of “matched” individuals who are similar (e.g., same age, sex, education, etc.) comparing consistency of measurements made on the same objects Baseline_Measure Post_Treatment_Measure Patient 1 Y1 X1 Patient 2 Y2 X2 Patient 3 Y3 X3 Patient 4 Y4 X4 In such cases, we have observations \\(X_{i}\\) and \\(Y_{i}\\) for \\(i = 1,\\ldots n\\) where it is not necessarily reasonable to think of \\(X_{i}\\) and \\(Y_{i}\\) as independent. We can define \\(D_{i} = X_{i} - Y_{i}\\) as the difference in the \\(i^{th}\\) pair. With this setup, a natural question is whether or not the differences \\(D_{i}\\) tend to be greater than zero or not. The sign statistic \\(S_{n}\\) is defined as \\[\\begin{equation} S_{n} = \\sum_{i=1}^{n} I( D_{i} &gt; 0) \\tag{3.9} \\end{equation}\\] If the null hypothesis \\(H_{0}: \\theta = 0\\) is true, then we should expect that roughly half of the observations will be positive. This suggests that we will reject \\(H_{0}\\) if \\(S_{n} \\geq c\\) where \\(c\\) is a number that is greater than \\(n/2\\). 3.3.1.2 Null Distribution and p-values Notice that the sign statistic defined in (3.9) is the sum of independent Bernoulli random variable. That is, we can think of \\(Z_{i} = I(D_{i} &gt; 0)\\) as a random variable with success probability \\(p( \\theta )\\) where the formula for \\(p( \\theta )\\) is \\[\\begin{equation} p(\\theta) = P(Z_{i} = 1) = P(D_{i} &gt; 0) = 1 - F_{D}(0) = 1 - F_{\\epsilon}( -\\theta ) \\end{equation}\\] This implies that \\(S_{n}\\) is a binomial random variable with \\(n\\) trials and success probability \\(p(\\theta)\\). That is, \\[\\begin{equation} S_{n} \\sim \\textrm{Binomial}(n, p(\\theta) ) \\tag{3.10} \\end{equation}\\] Because \\(p(0) = 1/2\\), \\(S_{n} \\sim \\textrm{Binomial}(n, 1/2 )\\) under \\(H_{0}\\). Notice that the “null distribution” of the sign statistic is “distribution free” in the sense that the distribution does not depend on the distribution of \\(D_{i}\\). The p-value for the sign test can be computed by \\[\\begin{equation} \\textrm{p-value} = P_{H_{0}}(S_{n} \\geq s_{obs}) = \\sum_{j=s_{obs}}^{n} P_{H_{0}}(S_{n} = j) = \\sum_{j=s_{obs}}^{n} {n \\choose j} \\frac{1}{2^{n}}, \\end{equation}\\] where \\(s_{obs}\\) is the observed value of the sign statistic. ### How to compute the p-value for the sign test using R xx &lt;- rnorm(100) sign.stat &lt;- sum(xx &gt; 0) 1 - pbinom(sign.stat - 1, size=100, prob=1/2) ## p-value for sign test ## [1] 0.6178233 The reason that this is the right expression using R is that for any positive integer \\(w\\) \\[\\begin{equation} P_{H_{0}}(S_{n} \\geq w) = 1 - P_{H_{0}}(S_{n} &lt; w) = 1 - P_{H_{0}}(S_{n} \\leq w - 1) \\end{equation}\\] and the R function pbinom(t, n, prob) computes \\(P(X \\leq t)\\) where \\(X\\) is a binomial random variable with \\(n\\) trials and success probability prob. You can also perform the one-sided sign test by using the binom.test function in R. btest &lt;- binom.test(sign.stat, n=100, p=0.5, alternative=&quot;greater&quot;) btest$p.value ## [1] 0.6178233 3.3.1.3 Two-sided Sign Test Notice that the number of negative values of \\(D_{i}\\) can be expressed as \\[\\begin{equation} \\sum_{i=1}^{n} I(D_{i} &lt; 0) = n - S_{n} \\end{equation}\\] if there are no observations that equal zero exactly. Large value of \\(n - S_{n}\\) would be used in favor of another possible one-sided alternative \\(H_{A}: \\theta &lt; 0\\). If we now want to test the two-sided alternative \\[\\begin{equation} H_{0}: \\theta = 0 \\quad \\textrm{ vs. } \\quad H_{A}: \\theta \\neq 0 \\nonumber \\end{equation}\\] you would need to compute the probability under the null hypothesis of observing a “more extreme” observation than the one that was actually observed. Extreme is defined by thinking about the fact that we would have rejected \\(H_{0}\\) if either \\(S_{n}\\) or \\(n - S_{n}\\) were very large. For example, if \\(n = 12\\), then the expected value of the sign statistic would be \\(6\\). If \\(s_{obs} = 10\\), then the collection of “more extreme” events then this would be \\(\\leq 2\\) and \\(\\geq 10\\). The two-sided p-value is determined by looking at the tail probabilities on both sides \\[\\begin{equation} \\textrm{p-value} = \\begin{cases} P_{H_{0}}(S_{n} \\geq s_{obs}) + P_{H_{0}}(S_{n} \\leq n - s_{obs}) &amp; \\textrm{ if } s_{obs} \\geq n/2 \\\\ P_{H_{0}}(S_{n} \\leq s_{obs}) + P_{H_{0}}(S_{n} \\geq n - s_{obs}) &amp; \\textrm{ if } s_{obs} &lt; n/2 \\end{cases} \\end{equation}\\] It actually works out that \\[\\begin{equation} \\textrm{p-value} = \\begin{cases} 2 P_{H_{0}}(S_{n} \\geq s_{obs}) &amp; \\textrm{ if } s_{obs} \\geq n/2 \\\\ 2 P_{H_{0}}(S_{n} \\leq s_{obs}) &amp; \\textrm{ if } s_{obs} &lt; n/2 \\end{cases} \\end{equation}\\] Also, you can note that this p-value would be the same that you would get from performing the test \\(H_{0}: p = 1/2\\) vs. \\(H_{A}: p \\neq 1/2\\) when it is assumed that \\(S_{n} \\sim \\textrm{Binomial}(n, p)\\). Another note: It is often suggested that one should drop observations which are exactly zero when performing the sign test. 3.3.2 The Wilcoxon Signed Rank Test The Wilcoxon signed rank test can be applied under the same scenario that we used the sign test. One criticism of the sign test is that it ignores the magnitude of the observations. For example, the sign test statistic \\(S\\) treats observations \\(D_{i} = 0.2\\) and \\(D_{i}=3\\) the same. The Wilcoxon signed rank statistic \\(T_{n}\\) weights the signs of \\(D_{i}\\) by the rank of its absolute value. Specifically, the Wilcoxon signed rank statistic is defined as \\[\\begin{equation} T_{n} = \\sum_{i=1}^{n} \\textrm{sign}( D_{i}) R_{i}( |\\mathbf{D}| ) \\end{equation}\\] where the \\(\\textrm{sign}\\) function is defined as \\[\\begin{equation} \\textrm{sign}(x) = \\begin{cases} 1 &amp; \\textrm{if } x &gt; 0 \\\\ 0 &amp; \\textrm{if } x = 0 \\\\ -1 &amp; \\textrm{if } x &lt; 0 \\end{cases} \\end{equation}\\] Here, \\(R_{i}( |\\mathbf{D}| )\\) is the rank of the \\(i^{th}\\) element from the vector \\(|\\mathbf{D}| = (|D_{1}|, |D_{2}|, \\ldots, |D_{n}|)\\). Intuitively, the Wilcoxon signed rank statistic is measuring whether or not large values of \\(|D_{i}|\\) tend to be associated with positive vs. negative values of \\(D_{i}\\). Discuss some of these in class Exercise 3.4. Suppose we had data \\((-2, 1, -1/2, 3/2, 3)\\). What would be the value of the Wilcoxon signed rank statistic? Exercise 3.5. Under the assumptions of model (3.8), what is the density function of \\(|D_{i}|\\) and \\(-|D_{i}|\\)? Exercise 3.6. Under the assumptions of model (3.8) and assuming that \\(\\theta = 0\\), show that the expectation of the Wilcoxon signed-rank statistic is \\(0\\). 3.3.2.1 Asymptotic Distribution As mentioned in the above exercise, the expectation of \\(T_{n}\\) under \\(H_{0}\\) is zero. It can be shown that the variance under the null hypothesis is \\[\\begin{equation} \\textrm{Var}_{H_{0}}( T_{n} ) = \\frac{n(2n + 1)(n + 1)}{6} \\nonumber \\end{equation}\\] Similar, to the large-sample approximation we used for the WRS test, we have the following asymptotic result for the Wilcoxon signed-rank test \\[\\begin{equation} \\frac{T_{n}}{\\sqrt{\\textrm{Var}_{H_{0}}(T_{n}) }} \\longrightarrow \\textrm{Normal}(0,1) \\quad \\textrm{as } n \\longrightarrow \\infty \\end{equation}\\] Because the variance of \\(T\\) is dominated by the term \\(n^{3}/3\\) for very large \\(n\\), we could also say that under \\(H_{0}\\) that \\[\\begin{equation} \\frac{T_{n}}{\\sqrt{n^{3}/3} } \\longrightarrow \\textrm{Normal}(0,1) \\quad \\textrm{as } n \\longrightarrow \\infty \\end{equation}\\] In other words, we can say that \\(T_{n}\\) has an approximately \\(\\textrm{Normal}(0, n^{3}/3)\\) for large \\(n\\). 3.3.2.2 Exact Distribution The exact distribution of the Wilcoxon signed rank statistic \\(T_{n}\\) is somewhat more complicated than the exact distribution of the WRS test statistic. Nevertheless, there exists functions in R for working with this exact distribution. 3.3.3 Using R to Perform the Sign and Wilcoxon Tests Let’s first look at the Meat data from the PairedData R package. This data set contains 20 observations with measures of fat percentage using different measuring techniques. library(PairedData, quietly=TRUE, warn.conflicts=FALSE) ## loading PairedData package data(Meat) ## loading Meat data head(Meat) ## AOAC Babcock MeatType ## 1 22.0 22.3 Wiener ## 2 22.1 21.8 Wiener ## 3 22.1 22.4 Wiener ## 4 22.2 22.5 Wiener ## 5 24.6 24.9 ChoppedHam ## 6 25.3 25.6 ChooppedPork Define the differences \\(D_{i}\\) as the Babcock measurements minus the AOAC measures. We will drop the single observation that equals zero. DD &lt;- Meat[,2] - Meat[,1] DD &lt;- DD[DD!=0] hist(DD, main=&quot;Meat Data&quot;, xlab=&quot;Difference in Measured Fat Percentage&quot;, las=1) summary(DD) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.60000 -0.25000 0.30000 0.04211 0.40000 1.10000 The Sign Test in R Let’s first test the hypothesis \\(H_{0}: \\theta = 0\\) vs. \\(H_{A}: \\theta \\neq 0\\) using the two-sided sign test. This can be done using the binom.test function binom.test(sum(DD &gt; 0), n = length(DD), p=0.5)$p.value ## [1] 0.6476059 Wilcoxon Signed Rank Test in R You can actually use the function wilcox.test to perform the Wilcoxon signed rank test in addition to the Wilcoxon rank sum test. To perform the Wilcoxon signed rank test in R, you just need to enter data for the x argument and leave the y argument empty. wilcox.test(x=DD) ## Warning in wilcox.test.default(x = DD): cannot compute exact p-value with ties ## ## Wilcoxon signed rank test with continuity correction ## ## data: DD ## V = 118.5, p-value = 0.3534 ## alternative hypothesis: true location is not equal to 0 You will note that the p-value for the Wilcoxon signed rank test is lower than that of the sign test. In general, the Wilcoxon signed rank test is somewhat more “sensitive” than the sign test meaning that it will have a greater tendency to reject \\(H_{0}\\) for small deviations from \\(H_{0}\\). We can explore this sensitivity comparison with a small simulation study. We will consider a scenario where \\(D_{i} = 0.4 + \\varepsilon_{i}\\) with \\(\\varepsilon_{i}\\) having a t distribution with \\(3\\) degrees of freedom. set.seed(1327) n.reps &lt;- 500 ## number of simulation replications samp.size &lt;- 50 ## the sample size wilcox.reject &lt;- rep(0, n.reps) sign.reject &lt;- rep(0, n.reps) for(k in 1:n.reps) { dsim &lt;- .4 + rt(samp.size, df=3) wilcox.reject[k] &lt;- ifelse(wilcox.test(x=dsim)$p.value &lt; 0.05, 1, 0) sign.reject[k] &lt;- ifelse(binom.test(sum(dsim &gt; 0), n=samp.size, p=0.5)$p.value &lt; 0.05, 1, 0) } mean(wilcox.reject) ## proportion of times Wilcoxon signed rank rejected H0 ## [1] 0.614 mean(sign.reject) ## proportion of times Wilcoxon signed rank rejected H0 ## [1] 0.488 3.4 Power and Comparisons with Parametric Tests 3.4.1 The Power Function of a Test The power of a test is the probability that a test rejects the null hypothesis when the alternative hypothesis is true. The alternative hypothesis \\(H_{A}\\) is usually characterized by a large range of values of the parameter of interest. For example, \\(H_{A}: \\theta &gt; 0\\) or \\(H_{A}: \\theta \\neq 0\\). For this reason, it is better to think of power as a function that varies across the range of the alternative hypothesis. To be more precise, we will define the power function as a function of some parameter \\(\\theta\\) where the null hypothesis corresponds to \\(\\theta = \\theta_{0}\\) and the alternative hypothesis represents a range of alternative values of \\(\\theta\\). The power function \\(\\gamma_{n}(\\cdot)\\) of a testing procedure is defined as \\[\\begin{equation} \\gamma_{n}(\\delta) = P_{\\theta=\\delta}\\{ \\textrm{reject } H_{0} \\} \\qquad \\textrm{ for } \\delta \\in H_{A}. \\nonumber \\end{equation}\\] The notation \\(P_{\\theta=\\delta}\\{ \\textrm{reject } H_{0} \\}\\) means that we are computing this probability under the assumption that the parameter of interest \\(\\theta\\) equals \\(\\delta\\). The Approximate Power Function of the Sign Test Let us consider the sign test for testing \\(H_{0}: \\theta = 0\\) vs. \\(\\theta &gt; 0\\). The sign test is based on the value of the sign statistic \\(S_{n}\\). Recalling (3.10), we know that \\(S_{n} \\sim \\textrm{Binomial}(n, p(\\theta))\\). Hence, \\[\\begin{equation} \\sqrt{n}(\\tfrac{S_{n}}{n} - p(\\theta)) \\longrightarrow \\textrm{Normal}\\Big( 0, p(\\theta)(1 - p(\\theta)) \\Big) \\quad \\textrm{as } n \\longrightarrow \\infty \\tag{3.11} \\end{equation}\\] The sign test will reject \\(H_{0}\\) when \\(S_{n} \\geq c_{\\alpha,n}\\) where the constant \\(c_{\\alpha,n}\\) is chosen so that \\(P_{H_{0}}( S_{n} \\geq c_{\\alpha,n} ) = \\alpha\\). Using the large-sample approximation (3.11), you can show that \\[\\begin{equation} c_{\\alpha, n} = \\frac{n + \\sqrt{n}z_{1-\\alpha}}{2}, \\tag{3.12} \\end{equation}\\] where \\(z_{1-\\alpha}\\) denotes the upper \\(1 - \\alpha\\) quantile of the standard normal distribution. In other words, \\(\\Phi( z_{1-\\alpha}) = 1-\\alpha\\). Also, when using large-sample approximation (3.11), the power of this test to detect a value of \\(\\theta = \\delta\\) is given by \\[\\begin{eqnarray} \\gamma_{n}(\\delta) &amp;=&amp; P_{\\theta=\\delta}\\{ S_{n} \\geq c_{\\alpha,n} \\} = P_{\\theta=\\delta}\\Bigg\\{ \\frac{\\sqrt{n}(S_{n}/n - p(\\delta))}{\\sqrt{ p(\\delta)(1 - p(\\delta)) } } \\geq \\frac{ \\sqrt{n}(c_{\\alpha, n}/n - p(\\delta)) }{ \\sqrt{p(\\delta)(1 - p(\\delta))} } \\Bigg\\} \\nonumber \\\\ &amp;=&amp; 1 - \\Phi\\Bigg( \\frac{ \\sqrt{n}(c_{\\alpha,n}/n - p(\\delta)) }{ \\sqrt{p(\\delta)(1 - p(\\delta))} } \\Bigg) \\nonumber \\\\ &amp;=&amp; 1 - \\Phi\\Bigg( \\frac{ z_{1-\\alpha} }{ 2\\sqrt{p(\\delta)(1 - p(\\delta))} } - \\frac{ \\sqrt{n}(p(\\delta) - 1/2) }{ \\sqrt{p(\\delta)(1 - p(\\delta))} }\\Bigg) \\tag{3.13} \\end{eqnarray}\\] Notice that the power of the test depends more directly on the term \\(p(\\delta) = P_{\\theta = \\delta}(D_{i} &gt; 0)\\). Recall from Section 3.3.1 that \\(p(\\delta) = 1 - F_{\\epsilon}(\\delta)\\), where \\(F_{\\epsilon}\\) is the distribution function of \\(\\varepsilon_{i}\\) in the model \\(D_{i} = \\theta + \\varepsilon_{i}\\). So, in any power or sample size calculation, it would be more sensible to think about plausible values for \\(p(\\delta)\\) rather than \\(\\delta\\) itself. Plus, \\(p(\\delta)\\) has the direct interpretation \\(p(\\delta) = P_{\\theta=\\delta}( D_{i} &gt; 0)\\). Exercise 3.7: Derive the formula for \\(c_{\\alpha, n}\\) shown in (3.12). 3.4.2 Power Comparisons and Asymptotic Relative Efficiency Notice that for the sign statistic power function shown in (3.13), we have that \\[\\begin{equation} \\lim_{n \\longrightarrow \\infty} \\gamma_{n}(\\delta) = \\begin{cases} \\alpha &amp; \\textrm{ if } \\delta = 0 \\\\ 1 &amp; \\textrm{ if } \\delta &gt; 0 \\end{cases} \\tag{3.14} \\end{equation}\\] The above type of limit for the power function is will be true for most “reasonable” tests. Indeed, a test whose power function satisfies (3.14) is typically called a consistent tests. If nearly all reasonable tests are consistent, then how can we compare tests with respect to their power? One approach is to use simulations to compare power for several plausible alternatives. While this can be useful for a specific application, it limits our ability to make more general statements about power comparisons. Another approach might be to determine for which values of \\((\\delta, n)\\) one test has greater power than another. However, this could be tough to interpret (no test will be uniformly more powerful for all distributions) or even difficult to compute. One way to think about power is to think about the relative efficiency of two testing procedures. The efficiency of a test in this context is the sample size required to achieve a certain level of power. To find the asymptotic relative efficiency, we first need to derive the asymptotic power function. For our hypothesis \\(H_{0}: \\theta = \\theta_{0}\\) vs. \\(H_{A}: \\theta &gt; \\theta_{0}\\), this is defined as \\[\\begin{equation} \\tilde{\\gamma}(\\delta) = \\lim_{n \\longrightarrow \\infty} \\gamma_{n}( \\theta_{0} + \\delta/\\sqrt{n}) \\nonumber \\end{equation}\\] Considering the sequence of “local alternatives” \\(\\theta_{n} = \\theta_{0} + \\delta/\\sqrt{n}\\), we avoid the problem of the power always converging to \\(1\\). It can be shown that \\[\\begin{equation} \\tilde{\\gamma}(\\delta) = 1 - \\Phi\\Bigg( z_{1-\\alpha} - \\delta \\frac{\\mu&#39;(\\theta_{0})}{\\sigma(\\theta_{0})} \\Bigg) \\end{equation}\\] as long as we can find functions \\(\\mu(\\cdot)\\) and \\(\\sigma(\\cdot)\\) such that \\[\\begin{equation} \\frac{\\sqrt{n}(V_{n} - \\mu(\\theta_{n}))}{ \\sigma(\\theta_{n})} \\longrightarrow \\textrm{Normal}(0, 1) \\tag{3.15} \\end{equation}\\] where the test of \\(H_{0}:\\theta = \\theta_{0}\\) vs. \\(H_{A}: \\theta &gt; \\theta_{0}\\) is based on the test statistic \\(V_{n}\\) with rejection of \\(H_{0}\\) occurring whenever \\(V_{n} \\geq c_{\\alpha, n}\\). Statement (3.15) asssumes that the distribution of \\(V_{n}\\) is governed by \\(\\theta_{n}\\) for each \\(n\\). The ratio \\(e(\\theta_{0}) = \\mu&#39;(\\theta_{0})/\\sigma(\\theta_{0})\\) is the asymptotic efficiency of the test. When comparing two tests with efficiency \\(e_{1}(\\theta_{0})\\) and \\(e_{2}(\\theta_{0})\\), the asymptotic relative efficiency of test 1 vs. test 2 is defined as \\[\\begin{equation} ARE_{12}(\\theta_{0}) = \\Big( \\frac{e_{1}(\\theta_{0})}{e_{2}(\\theta_{0})} \\Big)^{2} \\end{equation}\\] Interpretation of Asymptotic Efficiency of Tests Roughly speaking, the asymptotic relative efficiency \\(ARE_{12}( \\theta_{0} )\\) approximately equals \\(n_{2}/n_{1}\\) where \\(n_{1}\\) is the sample size needed for test 1 to achieve power \\(\\beta\\) and \\(n_{2}\\) is the sample size needed for test 2 to achieve power \\(\\beta\\). This is true for an arbitrary \\(\\beta\\). To further justify this interpretation notice that, for large \\(n\\), we should have \\[\\begin{equation} c_{\\alpha, n} \\approx \\mu(\\theta_{0}) + \\frac{ \\sigma(\\theta_{0})z_{1-\\alpha} }{\\sqrt{n}} \\end{equation}\\] (This approximation for \\(c_{\\alpha, n}\\) comes from the asymptotic statement in (3.15)) Now, consider the power for detecting \\(H_{A}: \\theta = \\theta_{A}\\) (where we will assume that \\(\\theta_{A}\\) is “close” to \\(\\theta_{0}\\)). Using (3.15), the approximate power in this setting is \\[\\begin{eqnarray} P_{\\theta_{A}}\\Big( V_{n} \\geq c_{\\alpha, n} \\Big) &amp;=&amp; P_{\\theta_{A} }\\Bigg( \\frac{\\sqrt{n}(V_{n} - \\mu(\\theta_{A} ))}{ \\sigma(\\theta_{A} )} \\geq \\frac{\\sqrt{n}(c_{\\alpha,n} - \\mu(\\theta_{A}))}{ \\sigma(\\theta_{A})} \\Bigg) \\approx 1 - \\Phi\\Bigg( \\frac{\\sqrt{n}(c_{\\alpha,n} - \\mu(\\theta_{A}))}{ \\sigma(\\theta_{A})} \\Bigg) \\nonumber \\\\ &amp;=&amp; 1 - \\Phi\\Bigg( \\frac{\\sqrt{n}(\\mu(\\theta_{0}) - \\mu(\\theta_{A}))}{ \\sigma(\\theta_{A})} + \\frac{z_{1-\\alpha}\\sigma(\\theta_{0})}{ \\sigma(\\theta_{A})}\\Bigg) \\end{eqnarray}\\] Hence, if we want to achieve a power level of \\(\\beta\\) for the alternative \\(H_{A}: \\theta = \\theta_{A}\\), we need the corresponding sample size \\(n_{\\beta}( \\theta_{A} )\\) to satisfy \\[\\begin{equation} \\frac{\\sqrt{n_{\\beta}(\\theta_{A})}(\\mu(\\theta_{0}) - \\mu(\\theta_{A}))}{ \\sigma(\\theta_{A})} + \\frac{z_{1-\\alpha}\\sigma(\\theta_{0})}{ \\sigma(\\theta_{A})} = z_{1-\\beta} \\end{equation}\\] which reduces to \\[\\begin{equation} n_{\\beta}(\\theta_{A}) = \\Bigg( \\frac{ z_{1-\\beta}\\sigma(\\theta_{A}) - z_{1-\\alpha}\\sigma(\\theta_{0}) }{ \\mu(\\theta_{0}) - \\mu(\\theta_{A}) } \\Bigg)^{2} \\approx \\Bigg( \\frac{ [z_{1-\\beta} - z_{1-\\alpha}]\\sigma(\\theta_{0}) }{ (\\theta_{A} - \\theta_{0})\\mu&#39;(\\theta_{0})} \\Bigg)^{2} \\tag{3.16} \\end{equation}\\] So, if we were comparing two testing procedures and we computed the approximate sample sizes \\(n_{\\beta}^{1}(\\theta_{A})\\) and \\(n_{\\beta}^{2}(\\theta_{A})\\) needed to reach \\(\\beta\\) power for the alternative \\(H_{A}: \\theta = \\theta_{A}\\), the sample size ratio (using approximation (3.16)) would be \\[\\begin{equation} \\frac{ n_{\\beta}^{2}(\\theta_{A}) }{n_{\\beta}^{1}(\\theta_{A}) } = \\Bigg( \\frac{ \\mu_{1}&#39;(\\theta_{0})\\sigma_{2}(\\theta_{0}) }{ \\mu_{2}&#39;(\\theta_{0})\\sigma_{1}(\\theta_{0})} \\Bigg)^{2} = \\textrm{ARE}_{12}(\\theta_{0}) \\end{equation}\\] Notice that \\(\\textrm{ARE}_{12}(\\theta_{0}) &gt; 1\\) indicates that the test \\(1\\) is better than test \\(2\\) because the sample size required for test \\(1\\) would be less than the sample size required for test \\(2\\). It is also worth noting that our justification for the interpretation of \\(\\textrm{ARE}_{12}(\\theta_{0})\\) was not very rigorous or precise, but it is possible to make a more rigorous statement. See, for example, Chapter 13 of Lehmann and Romano (2006) for a more rigorous treatment of relative efficiency. In Lehmann and Romano (2006), they have a result that states (under appropriate assumptions) that \\[\\begin{equation} \\lim_{\\theta \\downarrow \\theta_{0}} \\frac{N_{2}(\\theta)}{N_{1}(\\theta)} = ARE_{12}(\\theta_{0}) \\end{equation}\\] where \\(N_{1}(\\theta)\\) and \\(N_{2}(\\theta)\\) are the sample sizes required to have power \\(\\beta\\) against alternative \\(\\theta\\). 3.4.3 Efficiency Examples The Sign Test Let us return to the example of the sign statistic \\(S_{n}\\) and its use in testing the hypothesis \\(H_{0}: \\theta = 0\\) vs. \\(H_{A}: \\theta &gt; 0\\). Notice that the sign test rejects \\(H_{0}:\\theta=0\\) for \\(V_{n} &gt; c_{\\alpha,n}\\) where \\(V_{n} = S_{n}/n\\) and \\(S_{n}\\) is the sign statistic. When \\(V_{n}\\) is defined this way (3.15) is satisfied when \\(\\mu(\\theta) = p(\\theta)\\) and \\(\\sigma(\\theta) = \\sqrt{p(\\theta)(1 - p(\\theta) )}\\) where \\(p(\\theta) = 1 - F_{\\epsilon}( -\\theta )\\). Thus, the efficiency of the sign test for testing \\(H_{0}: \\theta = 0\\) vs. \\(H_{A}: \\theta &gt; 0\\) is \\[\\begin{equation} \\frac{\\mu&#39;(0)}{\\sigma(0)} = \\frac{p&#39;(0)}{\\sqrt{p(0)(1 - p(0))}} = 2f_{\\epsilon}(0) \\end{equation}\\] where \\(f_{\\epsilon}(t) = F_{\\epsilon}&#39;(t)\\). The One-Sample t-test Assume that we have data \\(D_{1}, \\ldots, D_{n}\\) generated under the same assumption as in our discussion of the sign test and the Wilcoxon signed-rank test. That is, \\[\\begin{equation} D_{i} = \\theta + \\varepsilon_{i}, \\end{equation}\\] where \\(\\varepsilon_{i}\\) are assumed to have median \\(0\\) with \\(\\varepsilon_{i}\\) having p.d.f. \\(f_{\\varepsilon}\\) The one-sample t-test will reject \\(H_{0}: \\theta = 0\\) whenever \\(V_{n} &gt; c_{\\alpha, n}\\), where \\(V_{n}\\) is defined to be \\[\\begin{equation} V_{n} = \\frac{\\bar{D}}{ \\hat{\\sigma} } \\end{equation}\\] Note that (3.15) will apply if we choose \\[\\begin{eqnarray} \\mu(\\theta) &amp;=&amp; E_{\\theta}(D_{i}) = \\theta \\nonumber \\\\ \\sigma(\\theta) &amp;=&amp; \\sqrt{\\textrm{Var}_{\\theta}(D_{i})} = \\sqrt{\\textrm{Var}(\\varepsilon_{i})} = \\sigma_{\\epsilon} \\end{eqnarray}\\] These choices of \\(\\mu(\\theta)\\) and \\(\\sigma(\\theta)\\) work because \\[\\begin{eqnarray} \\frac{\\sqrt{n}(V_{n} - \\mu(\\theta_{n}))}{\\sigma(\\theta_{n})} &amp;=&amp; \\frac{\\sqrt{n}(\\bar{D} - \\theta_{n})}{\\sigma_{e}} + \\sqrt{n}\\theta_{n}\\Big( \\frac{1}{\\hat{\\sigma}} - \\frac{1}{\\sigma_{e}} \\Big) \\nonumber \\\\ &amp;=&amp; \\frac{\\sqrt{n}(\\bar{D} - \\theta_{n})}{\\sigma_{e}} + \\delta\\Big( \\frac{1}{\\hat{\\sigma}} - \\frac{1}{\\sigma_{e}} \\Big) \\nonumber \\\\ &amp;\\longrightarrow&amp; \\textrm{Normal}(0, 1) \\end{eqnarray}\\] So, the efficiency of the one-sample t-test is given by \\[\\begin{equation} \\frac{\\mu&#39;(0)}{\\sigma(0)} = \\frac{1}{ \\sigma_{e} } \\nonumber \\end{equation}\\] The Wilcoxon Rank Sum Test Using the close relation between the WRS test statistic and the Mann-Whitney statistic, the WRS test can be represented as rejecting \\(H_{0}\\) when \\(V_{N} \\geq c_{\\alpha, N}\\) where \\(V_{N}\\) is \\[\\begin{equation} V_{N} = \\frac{1}{mn} \\sum_{i=1}^{n}\\sum_{j=1}^{m} I(X_{i} \\geq Y_{j}) \\end{equation}\\] and \\(N = n + m\\). The power of the WRS test is usually analyzed in the context of the “shift alternative”. Namely, we are assuming that \\(F_{X}(t) = F_{Y}(t - \\theta)\\) and test \\(H_{0}: \\theta=0\\) vs. \\(H_{A}: \\theta &gt; 0\\). The natural choice for \\(\\mu(\\theta)\\) is the expectation of \\(V_{N}\\) when \\(\\theta\\) is the true shift parameter. So, let \\(\\mu(\\theta) = P_{\\theta}(X_{i} \\geq Y_{j})\\). This can be written in terms of \\(F_{Y}\\) and \\(f_{Y}\\): \\[\\begin{eqnarray} \\mu(\\theta) &amp;=&amp; \\int_{-\\infty}^{\\infty} P_{\\theta}( X_{i} \\geq Y_{j} | Y_{j}=t) f_{Y}(t) dt = \\int_{-\\infty}^{\\infty} P_{\\theta}( X_{i} \\geq t) f_{Y}(t) dt \\nonumber \\\\ &amp;=&amp; \\int_{-\\infty}^{\\infty} \\{1 - F_{X}(t) \\} f_{Y}(t) dt = 1 - \\int_{-\\infty}^{\\infty} F_{Y}(t - \\theta) f_{Y}(t) dt \\end{eqnarray}\\] You can show that (3.15) holds (see e.g, Chapter 14 of Van der Vaart (2000)) if you choose \\(\\sigma^{2}(\\theta)\\) to be \\[\\begin{eqnarray} \\sigma^{2}(\\theta) &amp;=&amp; \\frac{1}{1 - \\lambda}\\textrm{Var}\\{ F_{Y}(X_{i}) \\} + \\frac{1}{\\lambda} \\textrm{Var}\\{ F_{Y}(Y_{i} - \\theta) \\} \\end{eqnarray}\\] Here, \\(n/(m + n) \\longrightarrow \\lambda\\). Thus, the efficiency of testing \\(H_{0}: \\theta = 0\\) for the WRS test is \\[\\begin{equation} e(0) = \\frac{\\mu&#39;(0)}{\\sigma(0)} = \\frac{\\int_{-\\infty}^{\\infty} f^{2}(t) dt}{\\sigma(0)} \\end{equation}\\] 3.4.4 Efficiency Comparisons for Several Distributions Sign Test vs. One-Sample t-test Comparisons of the Efficiency of the sign and one-sample t-test only require us to find \\(f_{\\epsilon}(0)\\) and \\(\\sigma_{e}^{2}\\) for different assumptions about the residual density \\(f_{\\epsilon}\\). For the Logistic(0,1) distribution, \\(f_{\\epsilon}(0) = 1/4\\) and the standard deviation is \\(\\pi/\\sqrt{3}\\). Hence, the asymptotic relative efficiency of the sign test vs. the one-sample t-test would be \\((\\pi/2\\sqrt{3})^{2}\\). The relative efficiencies for the sign vs. t-test for other distributions are shown below \\[\\begin{eqnarray} \\textrm{Distribution} &amp; &amp; \\quad \\textrm{Efficiency} \\\\ \\textrm{Normal}(0,1) &amp; &amp; \\qquad 2/\\pi \\\\ \\textrm{Logistic}(0,1) &amp; &amp; \\qquad \\pi^{2}/12 \\\\ \\textrm{Laplace}(0,1) &amp; &amp; \\qquad 2 \\\\ \\textrm{Uniform}(-1, 1) &amp; &amp; \\qquad 1/3 \\\\ \\textrm{t-dist}_{\\nu} &amp; &amp; \\qquad [4(\\nu/(\\nu-2))\\Gamma^{2}\\{ (\\nu + 1)/2\\}]/[ \\Gamma^{2}(\\nu/2)\\nu \\pi ] \\end{eqnarray}\\] WRS Test vs. Two-Sample t-test The relative efficiencies for the WRS test vs. the two-sample t-test for several distributions are shown below. \\[\\begin{eqnarray} \\textrm{Distribution} &amp; &amp; \\quad \\textrm{Efficiency} \\\\ \\textrm{Normal}(0,1) &amp; &amp; \\qquad 3/\\pi \\\\ \\textrm{Logistic}(0,1) &amp; &amp; \\qquad \\pi^{2}/9 \\\\ \\textrm{Laplace}(0,1) &amp; &amp; \\qquad 3/2 \\\\ \\textrm{Uniform}(-1, 1) &amp; &amp; \\qquad 1 \\\\ \\textrm{t-dist}_{3} &amp; &amp; \\qquad 1.24 \\\\ \\textrm{t-dist}_{5} &amp; &amp; \\qquad 1.90 \\\\ \\end{eqnarray}\\] 3.4.5 A Power “Contest” To compare power across for specific sample sizes, effect sizes, and distributional assumptions, a simulation study can be more helpful than statements about asymptotic relative efficiency. Below shows the results of a simulation study in R which compares power for the one-sample testing problem. This simulation study compares the sign test, Wilcoxon signed rank test, and the one-sample t-test. It is assumed that \\(n = 200\\) and that responses \\(D_{i}\\) are generated from the following model: \\[\\begin{equation} D_{i} = 0.2 + \\varepsilon_{i} \\end{equation}\\] Three choices for the distribution of \\(\\varepsilon_{i}\\) were considered: \\(\\varepsilon_{i} \\sim \\textrm{Logistic}(0, 1)\\) \\(\\varepsilon_{i} \\sim \\textrm{Normal}(0, 1)\\) \\(\\varepsilon_{i} \\sim \\textrm{Uniform}(-3/2, 3/2)\\) The R code and simulation results are shown below. set.seed(148930) theta &lt;- 0.2 n &lt;- 200 nreps &lt;- 500 RejectSign &lt;- RejectWilcoxonSign &lt;- RejectT &lt;- matrix(NA, nrow=nreps, ncol=4) for(k in 1:nreps) { xx &lt;- theta + rlogis(n) yy &lt;- theta + rnorm(n) zz &lt;- theta + runif(n, min=-3/2, max=3/2) ww &lt;- theta + (rexp(n, rate=1) - rexp(n, rate=1))/sqrt(2) RejectSign[k,1] &lt;- ifelse(binom.test(x=sum(xx &gt; 0), n=n, p=0.5)$p.value &lt; 0.05, 1, 0) RejectWilcoxonSign[k,1] &lt;- ifelse(wilcox.test(xx)$p.value &lt; 0.05, 1, 0) RejectT[k,1] &lt;- ifelse(t.test(xx)$p.value &lt; 0.05, 1, 0) RejectSign[k,2] &lt;- ifelse(binom.test(x=sum(yy &gt; 0), n=n, p=0.5)$p.value &lt; 0.05, 1, 0) RejectWilcoxonSign[k,2] &lt;- ifelse(wilcox.test(yy)$p.value &lt; 0.05, 1, 0) RejectT[k,2] &lt;- ifelse(t.test(yy)$p.value &lt; 0.05, 1,0) RejectSign[k,3] &lt;- ifelse(binom.test(x=sum(zz &gt; 0), n=n, p=0.5)$p.value &lt; 0.05, 1, 0) RejectWilcoxonSign[k,3] &lt;- ifelse(wilcox.test(zz)$p.value &lt; 0.05, 1, 0) RejectT[k,3] &lt;- ifelse(t.test(zz)$p.value &lt; 0.05,1,0) RejectSign[k,4] &lt;- ifelse(binom.test(x=sum(ww &gt; 0), n=n, p=0.5)$p.value &lt; 0.05, 1, 0) RejectWilcoxonSign[k,4] &lt;- ifelse(wilcox.test(ww)$p.value &lt; 0.05, 1, 0) RejectT[k,4] &lt;- ifelse(t.test(ww)$p.value &lt; 0.05, 1, 0) } power.results &lt;- data.frame(Distribution=c(&quot;Logistic&quot;, &quot;Normal&quot;, &quot;Uniform&quot;, &quot;Laplace&quot;), SignTest=colMeans(RejectSign), WilcoxonSign=colMeans(RejectWilcoxonSign), tTest=colMeans(RejectT)) Estimated power for three one-sample tests and three distributions. 500 simulation replications were used. Distribution SignTest WilcoxonSign tTest Logistic 0.25 0.37 0.34 Normal 0.59 0.77 0.81 Uniform 0.44 0.87 0.90 Laplace 0.93 0.92 0.81 3.5 Linear Rank Statistics in General 3.5.1 Definition The Wilcoxon rank sum statistic is an example of a statistic from a more general class of rank statistics. This is the class of linear rank statistics. Suppose we have observations \\(\\mathbf{Z} = (Z_{1}, \\ldots, Z_{N})\\). A linear rank statistic is a statistic \\(T_{N}\\) that can be expressed as \\[\\begin{equation} T_{N} = \\sum_{i=1}^{N} c_{iN} a_{N}\\big( R_{i}( \\mathbf{Z} ) \\big) \\tag{3.17} \\end{equation}\\] The terms \\(c_{1N}, \\ldots, c_{NN}\\) are usually called coefficients. These are fixed numbers and are not random variables. The terms \\(a_{N}(R_{i}( \\mathbf{Z} ) )\\) are commonly referred to as scores. Typically, the scores are generated from a given function \\(\\psi\\) in the following way \\[\\begin{equation} a_{N}(i) = \\psi\\Big( \\frac{i}{N+1} \\Big) \\end{equation}\\] Example: WRS statistic For the Wilcoxon rank sum test, we separated the data \\(\\mathbf{Z} = (Z_{1}, \\ldots, Z_{N})\\) into two groups. The first \\(n\\) observations were from group 1 while the last \\(m\\) observations were from group 2. The WRS statistic was then defined as \\[\\begin{equation} W = \\sum_{i=1}^{n} R_{i}(\\mathbf{Z}) \\end{equation}\\] In this case, the WRS statistic can be expressed in the form (3.17) if we choose the coefficients to be the following \\[\\begin{equation} c_{iN} = \\begin{cases} 1 &amp; \\textrm{ if } i \\leq n \\\\ 0 &amp; \\textrm{ if } i &gt; n \\end{cases} \\end{equation}\\] and we choose the scores to be \\[\\begin{equation} a_{N}(i) = i \\end{equation}\\] 3.5.2 Properties of Linear Rank Statistics The expected value of the linear rank statistic (if the distribution of the \\(Z_{i}\\) is continuous) is \\[\\begin{equation} E(T_{N}) = N\\bar{c}_{N}\\bar{a}_{N}, \\tag{3.18} \\end{equation}\\] where \\(\\bar{c}_{N} = \\frac{1}{N} \\sum_{j=1}^{N} c_{jN}\\) and \\(\\bar{a}_{N} = \\frac{1}{N}\\sum_{j=1}^{N} a_{N}(j)\\) The formula (3.18) for the expectation only uses the fact that \\(R_{i}(\\mathbf{Z})\\) has a discrete uniform distribution. So, \\[\\begin{equation} E\\{ a_{N}( R_{i}(\\mathbf{Z} ) \\} = \\sum_{j=1}^{N} a_{N}(j)P\\{ R_{i}( \\mathbf{Z}) = j \\} = \\sum_{j=1}^{N} \\frac{ a_{N}(j) }{N} = \\bar{a}_{N} \\end{equation}\\] Using this, we can then see that \\[\\begin{equation} E( T_{N} ) = \\sum_{j=1}^{N} c_{jN} E\\{ a_{N}(R_{i}(\\mathbf{Z})) \\} = \\sum_{j=1}^{N} c_{jN}\\bar{a}_{N} = N\\bar{c}_{N}\\bar{a}_{N} \\end{equation}\\] A similar argument can show that the variance of \\(T_{N}\\) is \\[\\begin{equation} \\textrm{Var}( T_{N} ) = \\frac{N^{2}}{n-1} \\sigma_{a}^{2}\\sigma_{c}^{2}, \\end{equation}\\] where \\(\\sigma_{c}^{2} = \\frac{1}{N}\\sum_{j=1}^{N} (c_{jN} - \\bar{c}_{N})^{2}\\) and \\(\\sigma_{a}^{2} = \\frac{1}{N}\\sum_{j=1}^{N} (a_{N}(j) - \\bar{a}_{N})^{2}\\) To perform hypothesis testing when using a general linear rank statistics, working with the exact distribution or performing permutation tests can often be computationally demanding. Using a large-sample approximation is often easier. As long as a few conditions for the coefficients and scores are satisfied, one can state the following \\[\\begin{equation} \\frac{T_{N} - E( T_{N})}{\\sqrt{\\textrm{Var}(T_{N})}} \\longrightarrow \\textrm{Normal}(0, 1), \\end{equation}\\] where, as we showed, both \\(E(T_{N})\\) and \\(\\textrm{Var}(T_{N})\\) both have closed-form expressions for an arbitrary linear rank statistic. 3.5.3 Other Examples of Linear Rank Statistics 3.5.3.1 The van der Waerden statistic and the normal scores test Van der Waerden’s rank statistic is used for two-sample problems where the first \\(n\\) observations come from group 1 while the last \\(m\\) observations come from group 2. Van der Waerden’s rank statistic \\(VW_{N}\\) is defined as \\[\\begin{equation} VW_{N} = \\sum_{j=1}^{n} \\Phi^{-1}\\Bigg( \\frac{\\mathbf{R}_{i}( \\mathbf{Z})}{N+1} \\Bigg) \\end{equation}\\] The function \\(\\Phi^{-1}\\) denotes the inverse of the cumulative distribution function of a standard Normal random variable. The statistic \\(VW_{N}\\) is a linear rank statistic with coefficients \\[\\begin{equation} c_{iN} = \\begin{cases} 1 &amp; \\textrm{ if } i \\leq n \\\\ 0 &amp; \\textrm{ if } i &gt; n \\end{cases} \\end{equation}\\] and scores determined by \\[\\begin{equation} a_{N}(i) = \\Phi^{-1}\\Big( \\frac{i}{N+1} \\Big) \\end{equation}\\] A test based on van der Waerden’s statistic is often referred to as the normal scores test. The normal scores test is often suggested as an attractive test when the underlying data has an approximately normal distribution. If you plot a histogram of the van der Waerden scores \\(a_{N}(i)\\) it should look roughly like a Gaussian distribution (if there are not too many ties). 3.5.3.2 The median test The median test is also a two-sample rank test. While the Wilcoxon rank sum test looks at the average rank within group \\(1\\), the median test instead looks at how many of the ranks from group \\(1\\) are less than the median rank (which should equal \\((N+1)/2\\)). The test statistic \\(M_{N}\\) for the median test is defined as \\[\\begin{equation} M_{N} = \\sum_{i=1}^{n} I\\Big( R_{i}(\\mathbf{Z}) \\leq \\frac{N+1}{2} \\Big) \\end{equation}\\] because \\((N+1)/2\\) will be the median rank. This is a linear rank statistic with coefficients \\[\\begin{equation} c_{iN} = \\begin{cases} 1 &amp; \\textrm{ if } i \\leq n \\\\ 0 &amp; \\textrm{ if } i &gt; n \\end{cases} \\end{equation}\\] and scores \\[\\begin{equation} a_{N}(i) = \\begin{cases} 1 &amp; \\textrm{ if } i \\leq (N+1)/2 \\\\ 0 &amp; \\textrm{ if } i &gt; (N+1)/2 \\end{cases} \\end{equation}\\] The median test could be used to test whether or not observations from group 1 tend to be smaller than those from group 2. 3.5.4 Choosing the scores \\(a_{N}(i)\\) The rank tests we have discussed so far are nonparametric in the sense that their null distribution does not depend on any particular parametric assumptions about the distributions from which the observations arise. For power calculations, we often think of some parameter or “effect size” modifying the base distribution in some way. For example, we often think of the shift alternative \\(F_{X}(t) = F_{Y}(t - \\theta)\\) in the two-sample problem. In parametric statistics, when testing \\(H_{0}:\\theta = 0\\) the most powerful test of \\(H_{0}: \\theta = \\theta_{0}\\) vs. \\(H_{A}:\\theta = \\theta_{A}\\) is based on rejecting \\(H_{0}\\) whenever the likelihood ratio is large enough: \\[\\begin{equation} \\textrm{Reject } H_{0} \\textrm{ if: } \\quad \\frac{p_{\\theta_{A}}(\\mathbf{z})}{p_{\\theta_{0}}(\\mathbf{z})} \\geq c_{\\alpha, n} \\tag{3.19} \\end{equation}\\] This is the Neyman-Pearson Lemma. The same property is true if we are considering tests based on ranks. The most powerful test for testing \\(H_{0}: \\theta = \\theta_{0}\\) vs. \\(H_{A}:\\theta = \\theta_{A}\\) is based on \\[\\begin{equation} \\textrm{Reject } H_{0} \\textrm{ if: } \\quad \\frac{P_{\\theta_{A}}\\Big( R_{1}(\\mathbf{Z}), \\ldots, R_{N}(\\mathbf{Z}) \\Big)}{ P_{\\theta_{0}}\\Big( R_{1}(\\mathbf{Z}), \\ldots, R_{N}(\\mathbf{Z}) \\Big) } \\geq c_{\\alpha, n} \\tag{3.20} \\end{equation}\\] The main difference between (3.19) and (3.19) is that the distribution \\(P_{\\theta_{A}}\\Big( R_{1}(\\mathbf{Z}), \\ldots, R_{N}(\\mathbf{Z}) \\Big)\\) is unknown unless we are willing to make certain distributional assumptions. Nevertheless, we can approximate this probability if \\(\\theta_{A}\\) is a location parameter “close” to \\(\\theta_{0}\\) \\[\\begin{equation} P_{\\theta_{A}}\\Big( R_{1}(\\mathbf{Z}), \\ldots, R_{N}(\\mathbf{Z}) \\Big) \\approx P_{\\theta_{0}}\\Big( R_{1}(\\mathbf{Z}), \\ldots, R_{N}(\\mathbf{Z}) \\Big) + \\frac{\\theta_{A}}{N!}\\sum_{i=1}^{N} c_{iN} E\\Bigg\\{ \\frac{\\partial \\log f(Z_{(i)})}{ \\partial Z} \\Bigg\\} \\end{equation}\\] where \\(Z_{(i)}\\) denotes the \\(i^{th}\\) order statistic. See, for example, Chapter 13 of Van der Vaart (2000) for more details on the derivation of this approximation. So, large values of the linear rank statistic \\(T_{N} = \\sum_{i=1}^{N} c_{iN} a_{N}(i)\\) will approximately correspond to large values of \\(P_{\\theta_{A}}\\Big( R_{1}(\\mathbf{Z}), \\ldots, R_{N}(\\mathbf{Z}) \\Big)\\) if we choose the scores to be \\[\\begin{equation} a_{N}(i) = E\\Bigg\\{ \\frac{\\partial \\log f(Z_{(i)})}{ \\partial Z} \\Bigg\\} \\end{equation}\\] Linear rank statistics with scores generated this way are usually called locally most powerful rank test. The best choice of the scores will depend on what we assume about the density \\(f\\). For example, if we assume that \\(f(z)\\) is \\(\\textrm{Normal}(0,1)\\), then \\[\\begin{equation} \\frac{\\partial \\log f(z)}{\\partial z} = -z \\end{equation}\\] The approximate expectation of the order statistics from a Normal\\((0,1)\\) distribution are \\[\\begin{equation} E\\{ Z_{(i)} \\} \\approx \\Phi^{-1}\\Bigg( \\frac{i}{N+1} \\Bigg) \\end{equation}\\] This implies that the van der Waerden’s scores are approximately optimal if we assume the distribution of the \\(Z_{i}\\) is Normal. This can also be worked out for other choices of \\(f(z)\\). If \\(f(z)\\) is a Logistic distribution, the optimal scores correspond to the Wilcoxon rank sum test statistic. If \\(f(z)\\) is Laplace (meaning that \\(f(z) = \\frac{1}{2}e^{-|z|}\\)), then the optimal scores correspond to the median test. 3.6 Additional Reading Additional reading which covers the material discussed in this chapter includes: Chapters 3-4 from Hollander, Wolfe, and Chicken (2013) References "],
["krusk-wallis.html", "Chapter 4 Rank Tests for Multiple Groups 4.1 The Kruskal-Wallis Test 4.2 Performing the Kruskal-Wallis Test in R 4.3 Comparison of Specific Groups 4.4 An Additional Example 4.5 Additional Reading", " Chapter 4 Rank Tests for Multiple Groups We can roughly think of the tests discussed in Chapter 3 as being related to the parametric tests shown in the table below. \\[\\begin{eqnarray} \\textbf{Parametric Test} &amp; &amp; \\qquad \\textbf{ Nonparametric Tests } \\nonumber \\\\ &amp; &amp; \\nonumber \\\\ \\textrm{One-Sample t-test} &amp; &amp; \\qquad \\textrm{Wilcoxon Signed Rank/Sign Test} \\nonumber \\\\ \\textrm{Two-Sample t-test} &amp; &amp; \\qquad \\textrm{Wilcoxon Rank Sum/Normal Scores/Median Test} \\nonumber \\end{eqnarray}\\] The Kruskal-Wallis test can be though of as the nonparametric analogue of one-way analysis of variance (ANOVA). For \\(K \\geq 3\\) groups, one-way ANOVA considers the analysis of data arising from the following model \\[\\begin{equation} Y_{kj} = \\mu_{k} + \\varepsilon_{kj}, \\qquad j=1,\\ldots, n_{k}; k=1,\\ldots,K \\tag{4.1} \\end{equation}\\] where it is often assumed that \\(\\varepsilon_{kj} \\sim \\textrm{Normal}(0, \\sigma^{2})\\). Usually, the one-way ANOVA hypothesis of interest is something like \\[\\begin{equation} H_{0}: \\mu_{1} = \\mu_{2} = \\ldots = \\mu_{K} \\tag{4.2} \\end{equation}\\] which is sometimes referred to as the homogeneity hypothesis. A test of the hypothesis (4.2) is based on decomposing the observed variation in the responses \\(Y_{kj}\\): \\[\\begin{eqnarray} \\underbrace{ \\sum_{k=1}^{K}\\sum_{j=1}^{n_{k}} (Y_{kj} - \\bar{Y}_{..})^{2}}_{SST} &amp;=&amp; \\sum_{k=1}^{K}\\sum_{j=1}^{n_{k}} (\\bar{Y}_{k.} - \\bar{Y}_{..})^{2} + \\sum_{k=1}^{K}\\sum_{j=1}^{n_{k}} (Y_{kj} - \\bar{Y}_{k.})^{2} \\nonumber \\\\ &amp;=&amp; \\underbrace{\\sum_{k=1}^{K} n_{k} (\\bar{Y}_{k.} - \\bar{Y}_{..})^{2}}_{SSA} + \\underbrace{\\sum_{k=1}^{K}\\sum_{j=1}^{n_{k}} (Y_{kj} - \\bar{Y}_{k.})^{2}}_{SSE} \\tag{4.3} \\end{eqnarray}\\] where \\(\\bar{Y}_{k.} = \\frac{1}{n_{k}}\\sum_{j=1}^{n_{k}} Y_{kj}\\) and \\(\\bar{Y}_{..} = \\frac{1}{K}\\sum_{k=1}^{K} \\bar{Y}_{k.}\\). Large values of \\(SSA = \\sum_{k=1}^{K} n_{k} (\\bar{Y}_{k.} - \\bar{Y}_{..})^{2}\\) provide evidence against the null hypothesis (4.2). The alternative hypothesis here is that there is at least one pair of means \\(\\mu_{h}, \\mu_{l}\\) such that \\(\\mu_{h} \\neq \\mu_{l}\\). 4.1 The Kruskal-Wallis Test 4.1.1 Definition Instead of assuming (4.1) for the responses \\(Y_{kj}\\), nonparametric way of thinking about this problem is to instead only assume that \\[\\begin{equation} Y_{kj} \\sim F_{k} \\end{equation}\\] That is, \\(Y_{k1}, Y_{k2}, \\ldots, Y_{kn_{k}}\\) is an i.i.d. sample from \\(F_{k}\\) for each \\(k\\). A nonparametric version of the one-way ANOVA homogeneity hypothesis is \\[\\begin{equation} H_{0}: F_{1} = F_{2} = \\ldots = F_{K} \\tag{4.4} \\end{equation}\\] The “shift alternative” in this case can be stated as \\[\\begin{equation} H_{A}: F_{k}(t) = F(t - \\Delta_{k}), \\quad \\textrm{ for } k = 1, \\ldots K \\quad \\textrm{ and not all $\\Delta_{k}$ equal} \\end{equation}\\] The Kruskal-Wallis test statistic is similar to the SSA term (defined in (4.3)) in the one-way ANOVA setting. Rather than comparing the group-specific means \\(\\bar{Y}_{k.}\\) with the overall mean \\(\\bar{Y}_{..}\\), the Kruskal-Wallis test statistic will be comparing the group-specific rank means \\(\\bar{R}_{k.}\\) with their overall expectation under the null hypothesis. The Kruskal-Wallis test statistic is defined as \\[\\begin{equation} KW_{N} = \\frac{12}{N(N-1)}\\sum_{k=1}^{K} n_{k}\\Big( \\bar{R}_{k.} - \\frac{N + 1}{2} \\Big)^{2}, \\quad \\textrm{ where } N = \\sum_{k=1}^{K} n_{k} \\tag{4.5} \\end{equation}\\] In (4.5), \\(\\bar{R}_{k.}\\) is the average rank of those \\(k^{th}\\) group \\[\\begin{equation} \\bar{R}_{k.} = \\frac{1}{n_{k}} \\sum_{j=1}^{n_{k}} R_{kj}(\\mathbf{Z}), \\end{equation}\\] where \\(\\mathbf{Z}\\) denotes the pooled-data vector and \\(R_{kj}(\\mathbf{Z})\\) denotes the rank of \\(Y_{kj}\\) in the “pooled-data ranking”. What is the expectation of \\(\\bar{R}_{k.}\\) under the null hypothesis (4.4)? Again, if the null hypothesis is true, we can treat all of our responses \\(Y_{kj}\\) as just an i.i.d. sample of size \\(N\\) from a common distribution function \\(F\\). Hence, as we showed in (3.2) from Chapter 3, \\(E\\{ R_{kj}(\\mathbf{Z}) \\} = (N+1)/2\\) under the assumption that the data are an i.i.d. sample from a common distribution function. So, the intuition behind the definition of \\(KW_{N}\\) is that the differences \\(\\bar{R}_{k.} - \\frac{N + 1}{2}\\) should be small whenever the homogeneity hypothesis (4.4) is true. When \\(K=2\\), the following relationship between the Kruskal-Wallis statistic \\(KW_{N}\\) and the Wilcoxon rank sum test statistic \\(W\\) from Chapter 3 holds. \\[\\begin{equation} KW_{N} = \\frac{12}{mn(N+1)}\\Big( W - \\frac{n(N+1)}{2} \\Big)^{2}. \\tag{4.6} \\end{equation}\\] Hence, the p-value from a Kruskal-Wallis test and a (two-sided) WRS test should be the same when \\(K = 2\\). However, you cannot directly perform a one-sided test using the Kruskal-Wallis test. Exercise 4.1 If \\(K=2\\), show that equation (4.6) holds. An Example Group Y Rank Group 1 1.00 8 Group 1 -1.20 2 Group 1 -1.50 1 Group 2 0.00 5 Group 2 -0.10 4 Group 2 1.10 9 Group 3 0.90 7 Group 3 -0.40 3 Group 3 0.60 6 In this case, \\(N = 9\\), \\(\\bar{R}_{1.} = 11/3\\), \\(\\bar{R}_{2.} = 6\\), and \\(\\bar{R}_{3.} = 16/3\\). The Kruskall-Wallis statistic is \\[\\begin{equation} KW_{N} = \\frac{1}{2}\\Big\\{ 3(11/3 - 5)^{2} + 3(6 - 5)^{2} + 3(16/3 - 5)^{2} \\Big\\} = 13/9 \\end{equation}\\] 4.1.2 Asymptotic Distribution and Connection to One-Way ANOVA The Kruskal-Wallis statistic \\(KW_{N}\\) has an asymptotic chi-square distribution with \\(K-1\\) degrees of freedom under the null hypothesis (4.4). This follows from the fact that \\((\\bar{R}_{k.} - (N+1)/2)\\) is approximately normally distributed for large \\(n_{k}\\). R uses the large-sample approximation when computing the p-value for the Kruskal-Wallis test. The Kruskal-Wallis test can also be thought of as the test you would obtain if you applied the one-way ANOVA setup to the ranks of \\(Y_{kj}\\). The one-way ANOVA test is based on the value of SSA where, as in (4.3), SSA is defined as \\[\\begin{equation} SSA = \\sum_{k=1}^{K} n_{k} (\\bar{Y}_{k.} - \\bar{Y}_{..})^{2} \\end{equation}\\] You then reject \\(H_{0}\\), when \\(SSA/SSE = SSA/(SST - SSA)\\) is sufficiently large. Notice that if we computed SSA using the ranks \\(R_{kj}( \\mathbf{Z} )\\) rather than the observations \\(Y_{kj}\\), we would get: \\[\\begin{eqnarray} SSA_{r} &amp;=&amp; \\sum_{k=1}^{K} n_{k} (\\bar{R}_{k.} - \\bar{R}_{..})^{2} \\nonumber \\\\ &amp;=&amp; \\sum_{k=1}^{K} n_{k} (\\bar{R}_{k.} - \\frac{N+1}{2})^{2} \\nonumber \\\\ &amp;=&amp; \\frac{N(N-1)}{12} KW_{N} \\tag{4.7} \\end{eqnarray}\\] If you were applying ANOVA to the ranks of \\(Y_{kj}\\), \\(SST_{r}\\) would be the following fixed constant: \\[\\begin{equation} \\textrm{SST}_{r} = \\frac{N(N + 1)(N-1)}{12} \\end{equation}\\] So, any test of the homogeneity hypothesis would be based on just the value of \\(SSA_{r}\\) which as we showed in (4.7) is just a constant times the Kruskal-Wallis statistic. 4.2 Performing the Kruskal-Wallis Test in R We will look at performing Kruskal-Wallis tests in R by using the “InsectSprays” dataset. head(InsectSprays) ## count spray ## 1 10 A ## 2 7 A ## 3 20 A ## 4 14 A ## 5 14 A ## 6 12 A This dataset has 72 observations. The variable count is the number of insects measured in some agricultural unit. The variable spray was the type of spray used on that unit. You could certainly argue that a standard ANOVA is not appropriate in this situation because the responses are counts, and for count data, the variance is usually a function of the mean. A generalized linear model with a log link function might be more appropriate. Applying a square-root transformation to count data is also a commonly suggested approach. (The square-root transformation is the “variance-stabilizing transformation” for Poisson-distributed data). boxplot(sqrt(count) ~ spray, data=InsectSprays,las=1, ylab=&quot;square root of insect counts&quot;) Let us perform a test of homogeneity using both the one-way ANOVA approach and a Kruskal-Wallis test anova(lm(sqrt(count) ~ spray, data=InsectSprays)) ## Analysis of Variance Table ## ## Response: sqrt(count) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## spray 5 88.438 17.6876 44.799 &lt; 2.2e-16 *** ## Residuals 66 26.058 0.3948 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 a &lt;- kruskal.test(sqrt(count) ~ spray, data=InsectSprays) a$p.value ## [1] 1.510844e-10 Notice that it applying the square root transformation or not does not affect the value of the Kruskal-Wallis statistic or the Kruskal-Wallis p-value. kruskal.test(count ~ spray, data=InsectSprays) ## ## Kruskal-Wallis rank sum test ## ## data: count by spray ## Kruskal-Wallis chi-squared = 54.691, df = 5, p-value = 1.511e-10 This invariance to data transformation is not true for the standard one-way ANOVA. anova(lm(count ~ spray, data=InsectSprays)) ## Analysis of Variance Table ## ## Response: count ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## spray 5 2668.8 533.77 34.702 &lt; 2.2e-16 *** ## Residuals 66 1015.2 15.38 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 4.3 Comparison of Specific Groups A Kruskal-Wallis test performs a test of the overall homogeneity hypothesis \\[\\begin{equation} H_{0}: F_{1} = F_{2} = \\ldots = F_{K} \\end{equation}\\] However, a rejection of the homogeneity hypothesis does not indicate which group differences are primarily the source of this rejection nor does it provide any measure of the “magnitude” of the differences between each of the groups. Dunn’s test is the suggested way to compute pairwise tests of stochatic dominance. Performing a series of pairwise Wilcoxon rank sum test can lead to violations of transitivity. For example, group A is “better” than B which is better than C, but group C is better than A. In R, Dunn’s test can be performed using the dunn.test package. In traditional one-way ANOVA one often reports pairwise differences in the means and their associated confidence intervals. In the context of a Kruskal-Wallis test, one could report pairwise differences in the Hodges-Lehmann estimate though other comparisons may also be of interest. One nice approach is to use the proportional odds model interpretation of the Kruskal-Wallis test and then report the difference in the estimated proportional odds coefficients. See Section 7.6 of http://hbiostat.org/doc/bbr.pdf for more details on the proportional odds model. 4.4 An Additional Example We will use the “cane” dataset from the boot package. library(boot) data(cane) head(cane) ## n r x var block ## 1 87 76 19 1 A ## 2 119 8 14 2 A ## 3 94 74 9 3 A ## 4 95 11 12 4 A ## 5 134 0 12 5 A ## 6 92 0 3 6 A These data come from a study trying to determine the susceptibility of different types of sugar cane to a particular type of disease. The variable n contains the total number of shoots in each plot. The variable r containes the total number of diseased shoots. We can create a new variable prop that measures the proportion of shoots that are diseased. cane$prop &lt;- cane$r/cane$n You could certainly argue that a logistic regression model is a better approach here, but we will analyze the transformed proportions using the arcsine square root transformation. cane$prop.trans &lt;- asin(sqrt(cane$prop)) boxplot(prop.trans ~ block, data=cane, las=1, ylab=&quot;number of shoots&quot;) kruskal.test(prop ~ block, data=cane) ## ## Kruskal-Wallis rank sum test ## ## data: prop by block ## Kruskal-Wallis chi-squared = 1.1355, df = 3, p-value = 0.7685 4.5 Additional Reading Additional reading which covers the material discussed in this chapter includes: Chapters 6 from Hollander, Wolfe, and Chicken (2013) References "],
["permutation.html", "Chapter 5 Permutation Tests 5.1 Notation 5.2 Permutation Tests for the Two-Sample Problem 5.3 The Permutation Test as a Conditional Test 5.4 A Permutation Test for Correlation 5.5 A Permutation Test for Variable Importance in Regression and Machine Learning", " Chapter 5 Permutation Tests Permutation tests are a useful tool to avoid having to depend on specific parametric assumptions. Permutation tests are also useful in more complex modern applications where it can be difficult to work out the theoretical null distribution of a certain test statistic. 5.1 Notation A permutation \\(\\pi\\) of a set \\(S\\) is a function \\(\\pi: S \\longrightarrow S\\) is a function that is both one-to-one and onto. We will usually think of \\(S\\) as the set of observation indices in which case \\(S = \\{1, \\ldots, N\\}\\) for sample size \\(N\\). Each permutation \\(\\pi\\) of \\(S = \\{1, \\ldots, N\\}\\) defines a particular ordering of the elements of \\(S\\). For this reason, a permutation is often expressed as the following ordered list \\[\\begin{equation} \\pi = \\big( \\pi(1), \\pi(2), \\ldots, \\pi(N) \\big) \\end{equation}\\] In other words, we can think of a permutation of \\(S\\) as a particular ordering of the elements of \\(S\\). For example, if \\(S = \\{1,2,3\\}\\), and \\(\\pi_{1}\\) is a permutation of \\(S\\) defined as \\(\\pi_{1}(1) = 3\\), \\(\\pi_{1}(2) = 1\\), \\(\\pi_{1}(3) = 2\\), then this permutation expressed as an ordered list would be \\[\\begin{equation} \\pi_{1} = (3, 1, 2) \\end{equation}\\] There are \\(5\\) other possible permutations of \\(S\\): \\[\\begin{eqnarray} \\pi_{2} &amp;=&amp; (1,2,3) \\nonumber \\\\ \\pi_{3} &amp;=&amp; (2,1,3) \\nonumber \\\\ \\pi_{4} &amp;=&amp; (1,3,2) \\nonumber \\\\ \\pi_{5} &amp;=&amp; (3,2,1) \\nonumber \\\\ \\pi_{6} &amp;=&amp; (2, 3, 1) \\nonumber \\end{eqnarray}\\] If \\(S\\) has \\(N\\) distinct elements, there are \\(N!\\) possible permutations of \\(S\\). We will let \\(\\mathcal{S}_{N}\\) denote the set of all permutations of the set \\(\\{1, \\ldots, N\\}\\). 5.2 Permutation Tests for the Two-Sample Problem A permutation test is motivated by the following reasoning. If there is no real difference between the two groups, there is nothing “special” about the difference in means between the two groups. The observed difference in the mean between the two groups should not be notably different than mean differences from randomly formed groups. Forming “random” groups can be done by using many permutations of the original data. 5.2.1 Example 1 Suppose we have observations from two groups \\(X_{1}, \\ldots, X_{n} \\sim F_{X}\\) and \\(Y_{1}, \\ldots, Y_{m} \\sim F_{Y}\\). Let \\(\\mathbf{Z} = (Z_{1}, \\ldots, Z_{N})\\) denote pooled data \\[\\begin{equation} (Z_{1}, \\ldots, Z_{N}) = (X_{1}, \\ldots, X_{n}, Y_{1}, \\ldots, Y_{m}) \\end{equation}\\] For a permutation \\(\\pi\\) of \\(\\{1, \\ldots, N\\}\\), we will let \\(\\mathbf{Z}_{\\pi}\\) denote the corresponding permuted dataset \\[\\begin{equation} \\mathbf{Z}_{\\pi} = (Z_{\\pi(1)}, Z_{\\pi(2)}, \\ldots, Z_{\\pi(N)}) \\end{equation}\\] Example of Permuting a Vector of Responses. This example assumes n=m=5. OriginalData Perm1 Perm2 Perm3 Perm4 Perm5 z1 0.60 -0.60 0.60 -0.90 0.70 0.60 z2 -0.80 -1.40 -0.60 0.70 -0.40 -0.60 z3 -0.60 0.70 0.20 0.60 -1.40 -0.80 z4 -0.90 0.20 -0.40 0.20 0.20 0.30 z5 0.30 -0.40 -1.30 -0.40 -0.90 -0.40 z6 -1.30 -1.30 -1.40 -0.60 -0.80 0.70 z7 0.20 0.30 0.70 -1.40 0.30 -0.90 z8 0.70 0.60 0.30 -1.30 0.60 0.20 z9 -1.40 -0.90 -0.80 0.30 -0.60 -1.40 z10 -0.40 -0.80 -0.90 -0.80 -1.30 -1.30 mean difference 0.16 0.12 0.12 0.80 0.00 0.36 The columns in the above table are just permutations of the original data \\(\\mathbf{Z}\\). Suppose we want to base a test on the difference in the means between the two groups \\[\\begin{equation} T_{N}(\\mathbf{Z}) = \\bar{X} - \\bar{Y} = \\frac{1}{n}\\sum_{i=1}^{n} Z_{i} - \\frac{1}{m}\\sum_{i=n+1}^{N} Z_{i} \\end{equation}\\] We will let \\(t_{obs}\\) denote the observed value of the mean difference. That is, \\(t_{obs} = T_{N}(\\mathbf{Z}_{obs})\\), where \\(\\mathbf{Z}_{obs}\\) is the vector of the observed data. Under the null hypothesis that \\(F_{X} = F_{Y}\\), the observed mean difference should not be “abnormal” when compared with the mean differences from many other permutations of the data. z &lt;- c(0.6, -0.8, -0.6, -0.9, 0.3, -1.3, 0.2, 0.7, -1.4, -0.4) ## data observed.diff &lt;- mean(z[1:5]) - mean(z[6:10]) ## observed mean difference nreps &lt;- 1000 mean.diff &lt;- rep(NA, nreps) for(k in 1:nreps) { ss &lt;- sample(1:10, size=10) ## draw a random permutation z.perm &lt;- z[ss] ## form the permuted dataset mean.diff[k] &lt;- mean(z.perm[1:5]) - mean(z.perm[6:10]) ## compute mean difference ## for permuted dataset } hist(mean.diff, las=1, col=&quot;grey&quot;, main=&quot;Permutation Distribution of Mean Difference&quot;) abline(v=observed.diff, lwd=3) 5.2.2 Permutation Test p-values The one-sided p-value for the permutation test is \\[\\begin{eqnarray} \\textrm{p-value} &amp;=&amp; \\frac{\\textrm{number of permutations such that } T_{N} \\geq t_{obs}}{ N! } \\nonumber \\\\ &amp;=&amp; \\frac{1}{N!} \\sum_{\\pi \\in \\mathcal{S}_{N}} I\\Big( T_{N}(\\mathbf{Z}_{\\pi}) \\geq t_{obs} \\Big) \\nonumber \\end{eqnarray}\\] The two-sided p-value for the two-sample problem would be \\[\\begin{equation} \\textrm{p-value} = \\frac{1}{N!} \\sum_{\\pi \\in \\mathcal{S}_{N}} I\\Big( \\Big| T_{N}(\\mathbf{Z}_{\\pi}) \\Big| \\geq |t_{obs}| \\Big) \\nonumber \\end{equation}\\] As we did when producing the above histogram, the permutation-test p-value is often computed by using a large number of random permutations rather than computing the test statistic for every possible permutation. The Monte Carlo permutation-test p-value is defined as \\[\\begin{equation} \\textrm{p-value}_{mc} = \\frac{1}{S+1}\\Bigg[ 1 + \\sum_{s = 1}^{S} I\\Big( T_{N}(\\mathbf{Z}_{\\pi_{s}}) \\geq t_{obs} \\Big) \\Bigg] \\end{equation}\\] where \\(\\pi_{1}, \\ldots, \\pi_{S}\\) are randomly drawn permutations The two-sided (Monte Carlo) p-value for the example shown in the above Table is pval.mc &lt;- (1 + sum(abs(mean.diff) &gt;= abs(observed.diff)))/(nreps + 1) round(pval.mc, 2) ## [1] 0.78 5.2.3 Example 2: Ratios of Means With permutation tests, you are not limited to difference in means. You can choose the statistic \\(T_{N}(\\mathbf{Z})\\) to measure other contrasts of interest. For example, with nonnegative data you might be interested in the ratio of means between the two groups \\[\\begin{equation} T_{N}( \\mathbf{Z} ) = \\max\\Big\\{ \\bar{X}/\\bar{Y} , \\bar{Y}/\\bar{X} \\Big\\} \\end{equation}\\] Let us see how this works for a simulated example where we assume that \\(X_{1}, \\ldots, X_{n} \\sim \\textrm{Exponential}(1)\\) and \\(Y_{1}, \\ldots, Y_{m} \\sim \\textrm{Exponential}(1/2)\\) set.seed(5127) xx &lt;- rexp(20, rate=1) yy &lt;- rexp(20, rate=0.5) zz &lt;- c(xx, yy) ## this is the original data t.obs &lt;- max(mean(zz[1:20])/mean(zz[21:40]), mean(zz[21:40])/mean(zz[1:20])) nperms &lt;- 500 mean.ratio &lt;- rep(0, nperms) for(k in 1:nperms) { ss &lt;- sample(1:40, size=40) zz.perm &lt;- zz[ss] mean.ratio[k] &lt;- max(mean(zz.perm[1:20])/mean(zz.perm[21:40]), mean(zz.perm[21:40])/mean(zz.perm[1:20])) } hist(mean.ratio, las=1, col=&quot;grey&quot;, main=&quot;Permutation Distribution of Maximum Mean Ratio&quot;, xlab=&quot;maximum mean ratio&quot;) abline(v=t.obs, lwd=3) The two-side (Monte Carlo) permutation test p-value is: pval.mc &lt;- (1 + sum(mean.ratio &gt;= t.obs))/(nperms + 1) round(pval.mc, 2) ## [1] 0.04 5.2.4 Example 3: Differences in Quantiles Permutation tests are especially useful in problems where working out the null distribution is difficult, or when certain approximations of the null distributions are hard to justify. An example of this occurrs if you want to compare medians, or more generally, compare quantiles. The difference-in-quantiles statistic would be defined as \\[\\begin{equation} T_{N}( \\mathbf{Z} ) = Q_{p}(Z_{1}, \\ldots, Z_{n}) - Q_{p}(Z_{n+1}, \\ldots, Z_{N}) \\end{equation}\\] where \\(Q_{p}(X_{1}, \\ldots, X_{H})\\) denotes the \\(p^{th}\\) quantile from the data \\(X_{1}, \\ldots, X_{H}\\). The difference in quantiles could be computed with the following R code: z &lt;- rnorm(10) quantile(z[1:5], probs=.3) - quantile(z[6:10], probs=.3) ## 30% ## 0.2671133 Note that setting probs=.5 in the quantile function will return the median. Exercise 5.1 Suppose we have the following data from two groups \\((X_{1}, X_{2}, X_{3}) = (-1, 0, 3)\\) and \\((Y_{1}, Y_{2}, Y_{3}) = (2, 0, 1)\\). Compute the (two-sided) permutation p-value for the following two statistics: \\(T_{N}( \\mathbf{Z} ) = \\textrm{median}(X_{1}, X_{2}, X_{3}) - \\textrm{median}(Y_{1}, Y_{2}, Y_{3})\\). \\(T_{N}( \\mathbf{Z} ) = \\bar{X} - \\bar{Y}\\). Exercise 5.2. Suppose we have data from two groups such that \\(X_{1}, \\ldots, X_{n} \\sim \\textrm{Normal}(0, 1)\\) and \\(Y_{1}, \\ldots, Y_{m} \\sim \\textrm{Normal}(1, 1)\\). Using \\(n=m=50\\) and 500 simulation replications, compute \\(500\\) significance thresholds from the one-sided permutation test which uses the statistic \\(T_{N}( \\mathbf{Z} ) = \\bar{X} - \\bar{Y}\\). How, does this compare with the t-statistic threshold of \\(1.66\\)? Exercise 5.3. Suppose we have data from two groups such that \\(X_{1}, \\ldots, X_{n} \\sim \\textrm{Normal}(0, 1)\\) and \\(Y_{1}, \\ldots, Y_{m} \\sim \\textrm{Normal}(1, 1)\\). Using \\(n=m=50\\) and 500 simulation replications, compute the power of the permutation test which uses the statistic \\(T_{N}( \\mathbf{Z} ) = \\bar{X} - \\bar{Y}\\) to detect this true alternative. How, does the power compare with the (two-sided) two-sample t-statistic and the (two-sided) Wilcoxon rank sum test? 5.3 The Permutation Test as a Conditional Test A permutation test is an example of a conditional test. Typically, the p-value is defined as \\[\\begin{equation} \\textrm{p-value} = P(T \\geq t_{obs}|H_{0}) \\end{equation}\\] for some test statistic \\(T\\). In other words, the p-value is the probability that a random variable following the null distribution exceeds \\(t_{obs}\\). In many problems however, the null hypothesis \\(H_{0}\\) is not determined by a single parameter but contains many parameters. For example, the null hypothesis in a t-test is really \\(H_{0}: \\mu_{x} = \\mu_{y}\\) and \\(\\sigma &gt; 0\\). That is, the null hypothesis is true for many different values of \\(\\sigma\\). When \\(H_{0}\\) contains many parameter values, one approach for computing a p-value is to choose the test statistic \\(T\\) so that its distribution is the same for every point in \\(H_{0}\\). A more general approach is to instead compute a conditional p-value The conditional p-value is defined as \\[\\begin{equation} \\textrm{p-value} = P(T \\geq t_{obs}| S=s, H_{0}) \\end{equation}\\] where \\(S\\) is a sufficient statistic for the unknown terms in \\(H_{0}\\). A classic example of this is Fisher’s exact test. A permutation test computes a conditional p-value where the sufficient statistic is the vector of order statistics \\((Z_{(1)}, Z_{(2)}, \\ldots,Z_{(N)})\\). Recall that the order statistics are defined as \\[\\begin{eqnarray} Z_{(1)} &amp;=&amp; \\textrm{ smallest observation } \\\\ Z_{(2)} &amp;=&amp; \\textrm{ second smallest observation} \\\\ &amp; \\vdots &amp; \\\\ Z_{(N)} &amp;=&amp; \\textrm{ largest observation} \\end{eqnarray}\\] What is the conditional distribution of the observed data conditional on the observed order statistics? It is: \\[\\begin{eqnarray} f_{Z_{1}, \\ldots, Z_{N}|Z_{(1)}, \\ldots, Z_{(N)}}( z_{\\pi(1)}, \\ldots, z_{\\pi(N)} | z_{1}, \\ldots, z_{N}) &amp;=&amp; \\frac{f_{Z_{1}, \\ldots, Z_{N}}( z_{\\pi(1)}, \\ldots, z_{\\pi(N)} ) }{ f_{Z_{(1)},\\ldots,Z_{(N)}}(z_{1}, \\ldots, z_{N}) } \\nonumber \\\\ &amp;=&amp; \\frac{f_{Z_{i}}(z_{\\pi(1)}) \\cdots f_{Z_{i}}(z_{\\pi(N)})}{ N!f_{Z_{i}}(z_{1}) \\cdots f_{Z_{i}}(z_{N}) } \\nonumber \\\\ &amp;=&amp; \\frac{1}{N!} \\end{eqnarray}\\] (See Chapter 5 of Casella and Berger (2002) for a detailed description of the distribution of order statistics) In other words, if we know the value of: \\(Z_{(1)}=z_{1}, \\ldots, Z_{(N)} = z_{N}\\), then any event of the form \\(\\{ Z_{1} = z_{\\pi(1)}, \\ldots, Z_{N} = z_{\\pi(N)} \\}\\) has an equal probability of occurring for any permutation chosen. This equal probability of \\(1/N!\\) is only true under \\(H_{0}\\) where we can regard the data as coming from a common distribution. If we are conditioning on \\(Z_{(1)}=z_{1}, \\ldots, Z_{(N)} = z_{N}\\), then the probability that \\(T_{N}(Z_{1}, \\ldots, Z_{N}) \\geq t\\) is just the number of permutations of \\((z_{1}, \\ldots, z_{N})\\) such that the test statistic is greater than \\(t\\) divided by \\(N!\\). In other words \\[\\begin{eqnarray} &amp; &amp; P\\Big\\{ T_{N}(Z_{1}, \\ldots, Z_{N}) \\geq t| Z_{(1)} = z_{1}, \\ldots, Z_{(N)} = z_{N} \\Big\\} \\\\ &amp;=&amp; \\frac{1}{N!} \\sum_{\\pi \\in \\mathcal{S}_{N}} I\\Big( T_{N}(z_{\\pi(1)}, \\ldots, z_{\\pi(N)}) \\geq t \\Big) \\end{eqnarray}\\] Let us consider a concrete example. Suppose we have a two-sample problem with four observations. The first two observations come from the first group while the last two observations come from the second group. The order statistics that we will condition on are: \\[\\begin{eqnarray} Z_{(1)} &amp;=&amp; z_{1} = -3 \\\\ Z_{(2)} &amp;=&amp; z_{2} = -1 \\\\ Z_{(3)} &amp;=&amp; z_{3} = 2 \\\\ Z_{(4)} &amp;=&amp; z_{4} = 5 \\end{eqnarray}\\] If \\(T_{4}\\) is the mean difference \\[\\begin{equation} T_{4}(Z_{1}, Z_{2},Z_{3},Z_{4}) = \\frac{Z_{1} + Z_{2} - Z_{3} - Z_{4}}{2} \\end{equation}\\] what is the probability \\[\\begin{equation} P\\Big\\{ T_{4}(Z_{1}, Z_{2}, Z_{3}, Z_{4}) \\geq 2.5 | Z_{(1)}=z_{1}, Z_{(2)}=z_{2}, Z_{(3)}=z_{3}, Z_{(4)} = z_{4} \\Big \\} \\end{equation}\\] From the below table, we see that the number of times \\(T_{4} \\geq 2.5\\) occurs is \\(8\\). Hence, \\[\\begin{eqnarray} &amp; &amp; P\\Big\\{ T_{4}(Z_{1}, Z_{2}, Z_{3}, Z_{4}) \\geq 2.5 | Z_{(1)}=z_{1}, Z_{(2)}=z_{2}, Z_{(3)}=z_{3}, Z_{(4)} = z_{4} \\Big \\} \\nonumber \\\\ &amp;=&amp; 8/24 = 1/3. \\nonumber \\end{eqnarray}\\] a1 a2 a3 a4 P(Z1 = a1, Z2=a2, Z3=a3, Z4=a4|order stat) T(a1, a2, a3, a4) T(a1, a2, a3, a4) &gt;= 2.5 -3 -1 2 5 1/24 -5.50 0 -3 -1 5 2 1/24 -5.50 0 -3 2 -1 5 1/24 -2.50 0 -3 2 5 -1 1/24 -2.50 0 -3 5 -1 2 1/24 0.50 0 -3 5 2 -1 1/24 0.50 0 -1 -3 2 5 1/24 -5.50 0 -1 -3 5 2 1/24 -5.50 0 -1 2 -3 5 1/24 -0.50 0 -1 2 5 -3 1/24 -0.50 0 -1 5 -3 2 1/24 2.50 1 -1 5 2 -3 1/24 2.50 1 2 -3 -1 5 1/24 -2.50 0 2 -3 5 -1 1/24 -2.50 0 2 -1 -3 5 1/24 -0.50 0 2 -1 5 -3 1/24 -0.50 0 2 5 -3 -1 1/24 5.50 1 2 5 -1 -3 1/24 5.50 1 5 -3 -1 2 1/24 0.50 0 5 -3 2 -1 1/24 0.50 0 5 -1 -3 2 1/24 2.50 1 5 -1 2 -3 1/24 2.50 1 5 2 -3 -1 1/24 5.50 1 5 2 -1 -3 1/24 5.50 1 5.4 A Permutation Test for Correlation Suppose we have \\(N\\) pairs of observations \\((U_{1}, V_{1}), \\ldots, (U_{N}, V_{N})\\) The correlation between these pairs is defined as \\[\\begin{equation} \\rho_{UV} = \\frac{\\textrm{Cov}(U_{i}, V_{i})}{\\sigma_{U}\\sigma_{V}} \\end{equation}\\] The test statistic of interest here is the sample correlation \\[\\begin{equation} T_{N}(\\mathbf{U}, \\mathbf{V}) = \\frac{\\sum_{i=1}^{N}(U_{i} - \\bar{U})(V_{i} - \\bar{V})}{\\sqrt{ \\sum_{i=1}^{N}(U_{i} - \\bar{U})^{2}\\sum_{i=1}^{N}(V_{i} - \\bar{V})^{2}} } \\end{equation}\\] To find the the permutation distribution, we only need to look at \\(T_{N}(\\mathbf{U}_{\\pi}, \\mathbf{V})\\) for different permutations \\(\\pi\\). In other words, we are computing correlation among pairs of the form \\((U_{\\pi(1)}, V_{1}), \\ldots, (U_{\\pi(N)}, V_{N})\\). We only need to look at \\(\\mathbf{U}_{\\pi}\\) because this achieves the objective of randomly “switching observation pairs”. The two-sided p-value for the permutation test of \\(H_{0}: \\rho_{UV} = 0\\) vs. \\(H_{A}: \\rho_{UV} \\neq 0\\) is \\[\\begin{equation} \\textrm{p-value} = \\frac{1}{N!} \\sum_{\\pi \\in \\mathcal{S}_{N}} I\\Big( \\Big| T_{N}(\\mathbf{U}_{\\pi}, \\mathbf{V}) \\Big| \\geq |t_{obs}| \\Big) \\nonumber \\end{equation}\\] library(rattle.data) ## Computing the permutation distribution for correlation ## between flavanoids and phenols n.obs &lt;- nrow(wine) ## number of observations t.obs.pf &lt;- cor(wine$Phenols, wine$Flavanoids) ## observed correlation nperms &lt;- 2000 cor.perm.pf &lt;- rep(0, nperms) for(k in 1:nperms) { ss &lt;- sample(1:n.obs, size=n.obs) uu.perm &lt;- wine$Phenols[ss] cor.perm.pf[k] &lt;- cor(uu.perm, wine$Flavanoids) } hist(cor.perm.pf, xlim=c(-1, 1), las=1, col=&quot;grey&quot;, main=&quot;Permutation Distribution of Correlation between Phenols and Flavanoids&quot;, xlab=&quot;correlation&quot;) abline(v=t.obs.pf, lwd=3) Now let us compute the p-values for both the Phenols/Flavanoids and Phenols/Color association tests. pval.mc &lt;- (1 + sum(abs(cor.perm.pf) &gt;= abs(t.obs.pf)))/(nperms + 1) round(pval.mc, 4) ## [1] 5e-04 pval.mc &lt;- (1 + sum(abs(cor.perm.pc) &gt;= abs(t.obs.pc)))/(nperms + 1) round(pval.mc, 4) ## [1] 0.4648 5.5 A Permutation Test for Variable Importance in Regression and Machine Learning The idea of permutation testing can also be applied in the context of regression. In regression, we have a series of responses \\(y_{1}, \\ldots, y_{N}\\), and we have a series of associated covariates vectors \\(\\mathbf{x}_{i}\\). For regression, we are going to perform permutations on the vector of responses \\(\\mathbf{y} = (y_{1}, \\ldots, y_{N})\\) and compute some measure for each permutation. For example, we might compute some measure of variable importance for each permutation. The idea here is that when permuting \\(\\mathbf{y}\\), the association between \\(\\mathbf{y}\\) and any “important covariates” should be lost. We want to see what the typical values of our variable importance measures will be when we break any association between \\(\\mathbf{y}\\) and a covariate. This idea can be useful in the context of difficult-to-interpret variable importance measures or variable importance measures which are known to have certain biases. This idea has been suggested as an alternative way of measuring variable importance for random forests (see e.g., Altmann et al. (2010) or Nembrini (2019)) With these approaches, we permute the response vector \\(\\mathbf{y}\\) many times. Our permutation variable importance p-value for a particular variable will be the proportion of permutations where that variable’s importance score exceeded the importance score from the original data. (In this case, a smaller p-value would mean the variable was more important). Let us see an example of that if we look at a random forest model for predicting wine type from the wine data. First, we will load the data and fit a random forest model. library(rattle.data) library(randomForest) ## randomForest 4.6-14 ## Type rfNews() to see new features/changes/bug fixes. wine2 &lt;- subset(wine, Type==1 | Type==2) wine2$Type &lt;- factor(wine2$Type) X &lt;- model.matrix(Type ~ . -1, data=wine2) yy &lt;- wine2$Type n &lt;- length(yy) nvars &lt;- ncol(X) ## Variable importance scores using original data originalRF &lt;- randomForest(X, y=yy) var.imp &lt;- originalRF$importance var.imp ## MeanDecreaseGini ## Alcohol 16.4477773 ## Malic 1.6547628 ## Ash 0.9512346 ## Alcalinity 1.8280983 ## Magnesium 4.2799222 ## Phenols 2.4436762 ## Flavanoids 6.2584880 ## Nonflavanoids 0.5184224 ## Proanthocyanins 0.6201685 ## Color 10.0968269 ## Hue 0.5552606 ## Dilution 1.0398252 ## Proline 17.3324599 Now, let us compare these original variable importance scores with the importance scores obtained across 10,000 permuted datasets. nperm &lt;- 10000 VarImpMat &lt;- matrix(0, nrow=nperm, ncol=ncol(X)) for(k in 1:nperm) { ytmp &lt;- yy[sample(1:n,size=n)] rf.fit &lt;- randomForest(X, y=ytmp) VarImpMat[k,] &lt;- rf.fit$importance ## VarImpMat[k,h] contains the importance score of ## variable h in permutation k } perm.pval &lt;- rep(0, nvars) for(h in 1:nvars) { #apply(VarImpMat, 1, function(x) mean(x &gt;= var.imp)) perm.pval[h] &lt;- mean(VarImpMat[,h] &gt;= var.imp[h]) } Permutation p-val Alcohol 0.000 Malic 1.000 Ash 1.000 Alcalinity 1.000 Magnesium 0.923 Phenols 1.000 Flavanoids 0.080 Nonflavanoids 1.000 Proanthocyanins 1.000 Color 0.000 Hue 1.000 Dilution 1.000 Proline 0.000 References "],
["ustat.html", "Chapter 6 U-Statistics 6.1 Definition 6.2 Examples 6.3 U-statistics for Two-Sample Problems 6.4 Measures of Association", " Chapter 6 U-Statistics 6.1 Definition Suppose we have observations \\(X_{1}, \\ldots, X_{n}\\). U-statistics are a family of statistics used to estimate quantities that can be written as \\[\\begin{equation} \\theta = E\\Big\\{ h(X_{1}, \\ldots, X_{r}) \\Big\\} \\tag{6.1} \\end{equation}\\] The U-statistic \\(U\\) which estimates (6.1) is given by the following formula: \\[\\begin{equation} U = \\frac{1}{{n \\choose r}} \\sum_{c \\in \\mathcal{C}_{n,r}} h(X_{c_{1}}, \\ldots, X_{c_{r}}) \\tag{6.2} \\end{equation}\\] The function \\(h\\) is usually called the kernel of the U-statistic. We will assume the kernel is symmetric. The integer \\(r\\) is called the order of the U-statistic. Typically, \\(r = 2\\), or \\(r = 3\\) at most. In (6.2), \\(\\mathcal{C}_{n,r}\\) denotes the set of all \\({n \\choose r}\\) combinations of size \\(r\\) from the set \\(\\{1, \\ldots, n\\}\\). For example, if \\(n = 3\\) and \\(r = 2\\) then \\[\\begin{equation} \\mathcal{C}_{n, r} = \\{ (1,2), (1,3), (2, 3) \\} \\nonumber \\end{equation}\\] For many common U-statistics \\(r=2\\) in which case (6.2) can be written as \\[\\begin{equation} U = \\frac{2}{n(n-1)} \\sum_{i=1}^{n}\\sum_{j=i+1}^{n} h(X_{i}, X_{j}) \\end{equation}\\] 6.2 Examples A wide range of well-known statistics can be represented as U-statistics. 6.2.1 Example 1: The Sample Mean The sample mean is actually an example of a U-statistic with \\(r = 1\\). Choosing \\(h(x) = x\\) means that the corresponding U-statistic is \\[\\begin{equation} U = \\frac{1}{n} \\sum_{i=1}^{n} X_{i} \\end{equation}\\] 6.2.2 Example 2: The Sample Variance The sample variance is actually another example of a U-statistic. In this case, \\(r = 2\\). To show why this is the case, choose the kernel \\(h(x_{1}, x_{2})\\) to be \\[\\begin{equation} h(x_{1}, x_{2}) = \\frac{1}{2}(x_{1} - x_{2})^{2} \\end{equation}\\] The expectation of this kernel is \\(\\sigma^{2} = E\\{ h(X_{1}, X_{2}) \\}\\) because \\[\\begin{eqnarray} E\\{ h(X_{1}, X_{2}) \\} &amp;=&amp; \\frac{1}{2}\\Big[ E(X_{1}^{2}) - 2E(X_{1})E(X_{2}) + E(X_{2}^{2}) \\Big] \\nonumber \\\\ &amp;=&amp; \\frac{1}{2}\\Big[ \\sigma^{2} + \\mu^{2} - 2\\mu^{2} + \\sigma^{2} + \\mu^{2} \\Big] \\nonumber \\\\ &amp;=&amp; \\sigma^{2} \\tag{6.3} \\end{eqnarray}\\] Also, by using formula (6.2), this choice of kernel generates the sample variance at its U-statistic: \\[\\begin{eqnarray} U_{var} &amp;=&amp; \\frac{1}{{n \\choose 2}} \\sum_{c \\in \\mathcal{C}_{n,2}} h(X_{c_{1}}, X_{c_{2}}) = \\frac{2}{n(n-1)}\\sum_{i=1}^{n} \\sum_{j=i+1}^{n} \\frac{1}{2} (X_{i} - X_{j})^{2} \\nonumber \\\\ &amp;=&amp; \\frac{2}{n(n-1)}\\sum_{i=1}^{n} \\sum_{j=1}^{n} (X_{i} - X_{j})^{2} \\nonumber \\\\ &amp;=&amp; \\frac{2}{n(n-1)}\\sum_{i=1}^{n} \\sum_{j=1}^{n} \\{ (X_{i} - \\bar{X})^{2} - 2(X_{i} - \\bar{X})(X_{j} - \\bar{X}) + (X_{j} - \\bar{X})^{2} \\} \\nonumber \\\\ &amp;=&amp; \\frac{2}{n(n-1)}\\sum_{i=1}^{n} n(X_{i} - \\bar{X})^{2} + \\frac{2}{n(n-1)}\\sum_{j=1}^{n} n(X_{j} - \\bar{X})^{2} \\} \\nonumber \\\\ &amp;=&amp; \\frac{1}{n-1}\\sum_{i=1}^{n} (X_{i} - \\bar{X})^{2} \\end{eqnarray}\\] Typically, the variance has the interpretation of \\(\\sigma^{2} = E\\{ (X_{i} - \\mu)^{2} \\}\\). That is, \\(\\sigma^{2}\\) measures the expected squared deviation of \\(X_{i}\\) from its mean. Given the form of the U-statistic (6.3), we can also interpret the variance in the following way: if we select two observations \\(X_{i}\\) and \\(X_{j}\\) at random, the expected squared distance between \\(X_{i}\\) and \\(X_{j}\\) will be \\(2\\sigma^{2}\\). You can see this through the following computer experiment. n &lt;- 50000 xx &lt;- rlogis(n, location=2, scale=0.75) diff.sq &lt;- rep(0, 5000) for(k in 1:5000) { idx &lt;- sample(1:n, size=2) diff.sq[k] &lt;- (xx[idx[1]] - xx[idx[2]])^2 } round(var(xx), 3) ## [1] 1.837 round(mean(diff.sq)/2, 3) ## [1] 1.868 6.2.3 Example 3: Gini’s Mean Difference Gini’s mean difference statistic is defined as \\[\\begin{equation} U_{G} = \\frac{1}{{n \\choose 2}} \\sum_{i=1}^{n}\\sum_{j=i+1} | X_{i} - X_{j} | \\nonumber \\end{equation}\\] This is a U-statistic with kernel \\[\\begin{equation} h(X_{1},X_{2}) = | X_{1} - X_{2} | \\end{equation}\\] The parameter that we are estimating with Gini’s mean difference statistic is: \\[\\begin{equation} \\theta_{G} = E\\Big\\{ \\Big| X_{1} - X_{2} \\Big| \\Big\\} \\end{equation}\\] Gini’s mean difference parameter \\(\\theta_{G}\\) can be interpreted in the following way: If we draw two observations at random from our population, \\(\\theta_{G}\\) represents the expected absolute difference between these two observations. The Gini coefficient \\(\\theta_{Gc}\\) is a popular measure of inequality. It is related to the mean difference parameter via \\[\\begin{equation} \\theta_{Gc} = \\frac{ \\theta_{G}}{ 2\\mu }, \\end{equation}\\] where \\(\\mu = E( X_{i} )\\). Exercise 6.1. Compute the Gini coefficient \\(\\theta_{Gc}\\) when it is assumed that \\(X_{i} \\sim \\textrm{Normal}( \\mu, \\sigma^{2})\\), for \\(\\mu &gt; 0\\). \\(X_{i} \\sim \\textrm{Exponential}(\\lambda)\\), (Hint: The difference between two independent Exponential random variables has a Laplace distribution). 6.2.4 Example 4: Wilcoxon Signed Rank Statistic The Wilcoxon signed rank test statistic is related to the following U statistic \\[\\begin{equation} U_{WS} = \\frac{2}{n(n-1)}\\sum_{i=1}^{n}\\sum_{j=i+1}^{n} I\\Big( X_{i} + X_{j} &gt; 0 \\Big) \\end{equation}\\] \\(U_{WS}\\) is a U-statistic of order \\(2\\) with kernel \\[\\begin{equation} h(x, y) = I\\Big( x + y &gt; 0\\Big) \\end{equation}\\] Hence, \\(U_{WS}\\) can be interpreted as an estimate of the following parameter \\[\\begin{equation} \\theta_{WS} = P\\Big( X_{i} + X_{j} &gt; 0 \\Big) = P\\Big( X_{i} &gt; -X_{j} \\Big) \\end{equation}\\] If the distribution of \\(X_{i}\\) is symmetric around \\(0\\), \\(\\theta_{WS}\\) will be equal to \\(1/2\\). Recall that the Wilcoxon signed rank test is designed to detect distributions which are not symmetric around \\(0\\). The Wilcoxon signed rank statistic \\(T_{n}\\) that we defined in Chapter 3 had the following formula \\[\\begin{equation} T_{n} = \\sum_{i=1}^{n} \\textrm{sign}( X_{i}) R_{i}( |\\mathbf{X}| ) \\end{equation}\\] A considerable amount of algebra can show that \\[\\begin{eqnarray} T_{n} &amp;=&amp; n(n-1) U_{WS} + 2\\sum_{i=1}^{n} I(X_{i} &gt; 0) - \\frac{n(n+1)}{2} \\nonumber \\\\ &amp;=&amp; n(n-1) U_{WS} + 2 S_{n} - \\frac{n(n+1)}{2} \\nonumber \\end{eqnarray}\\] where \\(S_{n}\\) is the sign test statistic defined in Chapter 3. For large \\(n\\), \\(T_{n}\\) is largely determined by \\(U_{WS} - 1/2\\). Hence, a “large” value of \\(U_{WS} - 1/2\\) will lead to rejection of the one-sample null hypothesis discussed in Section 3.3. 6.3 U-statistics for Two-Sample Problems In two-sample problems, we have data from two groups which we label \\(X_{1}, \\ldots, X_{n}\\) and \\(Y_{1}, \\ldots, Y_{m}\\) A U-statistic with order \\((r,s)\\) for a two-sample problem is \\[\\begin{equation} U = \\frac{1}{{n \\choose r}}\\frac{1}{{m \\choose s}} \\sum_{c \\in \\mathcal{C}_{n,r}} \\sum_{q \\in \\mathcal{C}_{m,s}} h(X_{c_{1}}, \\ldots, X_{c_{r}}, Y_{q_{1}}, \\ldots, Y_{q_{s}}) \\end{equation}\\] 6.3.1 The Mann-Whitney Statistic Consider the following U-statistic \\[\\begin{equation} U_{MW} = \\frac{1}{mn}\\sum_{i=1}^{n}\\sum_{j=1}^{m} I( X_{i} \\geq Y_{j}) \\end{equation}\\] This is a U-statistic of order \\((1,1)\\) with kernel \\(h(x, y) = I(x \\geq y)\\). Hence, the U-statistic \\(U_{MW}\\) can be thought of as an estimate of the following parameter \\[\\begin{equation} \\theta_{MW} = P\\Big( X_{i} \\geq Y_{j} \\Big) \\tag{6.4} \\end{equation}\\] If both \\(X_{i}\\) and \\(Y_{j}\\) have the same distribution, then \\(\\theta_{MW}\\) should equal \\(1/2\\). The statistic \\(mn U_{MW}\\) is known as the Mann-Whitney statistic. The Mann-Whitney statistic has a close relation to the Wilcoxon rank sum statistic \\(W\\) that we defined in Section 3.2: \\[\\begin{eqnarray} mn U_{MW} &amp;=&amp; \\sum_{i=1}^{n}\\sum_{j=1}^{m} I( X_{i} \\geq Y_{j}) \\nonumber \\\\ &amp;=&amp; \\sum_{i=1}^{n}\\Big[ \\sum_{j=1}^{m} I( X_{i} \\geq Y_{j}) + \\sum_{k=1}^{n} I( X_{i} \\geq X_{k}) \\Big] - \\sum_{i=1}^{n}\\sum_{k=1}^{n} I( X_{i} \\geq X_{k}) \\nonumber \\\\ &amp;=&amp; \\sum_{i=1}^{n} R_{i}(\\mathbf{Z}) - \\sum_{i=1}^{n} R_{i}( \\mathbf{X} ) \\tag{6.5} \\\\ &amp;=&amp; W - \\frac{n(n+1)}{2} \\nonumber \\end{eqnarray}\\] In other words, the Mann-Whitney statistic is equal to the WRS statistic minus a constant term. In (6.5), we are defining \\(\\mathbf{Z}\\) as the pooled-data vector \\(\\mathbf{Z} = (X_{1}, \\ldots, X_{n}, Y_{1}, \\ldots, Y_{m})\\). Also, the above derivation assumes no ties so that \\(\\sum_{i=1}^{n} R_{i}( \\mathbf{X} ) = n(n+1)/2\\). Because \\(W = n(n+1)/2 + mn U_{MW}\\) is a linear function of \\(U_{MW}\\), inference from the Wilcoxon rank sum test (when using large sample p-values) should match inferences made from using \\(U_{MW}\\) to test the hypothesis \\(H_{0}: \\theta_{MW} = 0\\). In other words, the two-sided Wilcoxon rank sum test can be thought of as a test of \\(H_{0}: \\theta_{MW} = 0\\) vs. \\(H_{A}:\\theta_{MW} \\neq 0\\), where \\(\\theta_{MW}\\) is the parameter defined in (6.4). 6.4 Measures of Association Many important measures of association are also examples of U-statistics. For measures of association, we have observations on \\(n\\) pairs of variables \\[\\begin{equation} (X_{1}, Y_{1}), \\ldots, (X_{n}, Y_{n}), \\nonumber \\end{equation}\\] and our goal is to report some measure which quantifies the relationship between these two variables. In this context, we will think about U-statistics which have the form \\[\\begin{equation} U = \\frac{1}{{n \\choose r}} \\sum_{c \\in \\mathcal{C}_{n,r} } h\\Bigg( \\begin{bmatrix} X_{c_{1}} \\\\ Y_{c_{1}} \\end{bmatrix}, \\ldots \\begin{bmatrix} X_{c_{r}} \\\\ Y_{c_{r}} \\end{bmatrix} \\Bigg) \\end{equation}\\] 6.4.1 Spearman’s Rank Correlation Spearman’s rank correlation coefficient is defined as \\[\\begin{eqnarray} \\hat{\\rho}_{R} &amp;=&amp; \\frac{\\sum_{i=1}^{n} \\{R_{i}(\\mathbf{X}) - \\bar{R}(\\mathbf{X}) \\}\\{ R_{i}(\\mathbf{Y}) - \\bar{R}(\\mathbf{Y}) \\}}{ \\big[ \\sum_{i=1}^{n} \\{R_{i}(\\mathbf{X}) - \\bar{R}(\\mathbf{X}) \\}\\sum_{i=1}^{n}\\{ R_{i}(\\mathbf{Y}) - \\bar{R}(\\mathbf{Y}) \\}^{2} \\big]^{1/2} } \\nonumber \\\\ &amp;=&amp; \\frac{12}{n(n-1)(n+1)}\\sum_{i=1}^{n} R_{i}( \\mathbf{X} )R_{i}(\\mathbf{Y}) - \\frac{3(n+1)}{n-1}, \\end{eqnarray}\\] where \\(\\bar{R}(X) = \\frac{1}{n}\\sum_{i=1}^{n} R_{i}( \\mathbf{X} )\\) and \\(\\bar{R}( \\mathbf{Y} ) = \\frac{1}{n} \\sum_{i=1}^{n} R_{i}( \\mathbf{Y} )\\). Remember that \\(R_{i}(\\mathbf{X})\\) denotes the rank of \\(X_{i}\\) when only using the vector \\(\\mathbf{X}\\) to compute the rankings. Likewise, \\(R_{i}(\\mathbf{Y})\\) denotes the rank of \\(Y_{i}\\) when only using the vector \\(\\mathbf{X}\\) to compute the rankings. Notice that \\(\\hat{\\rho}\\) comes from applying the usual Pearson’s estimate of correlation to the ranks \\((R_{i}( \\mathbf{X} )\\), \\(R_{i}(\\mathbf{Y}) )\\) rather than the original data \\((X_{i}, Y_{i})\\). As with the usual estimate of correlation, \\(\\hat{\\rho}\\) is large (i.e., closer to 1) whenever large values of \\(X_{i}\\) tend to be associated with large values of \\(Y_{i}\\). Similarly, \\(\\hat{\\rho}\\) is small wheneve large values of \\(X_{i}\\) tend to be associated with small values of \\(Y_{i}\\). Values of \\(\\hat{\\rho}_{R}\\) near zero indicate that there is little association between these two variables. Due to its use of ranks, \\(\\hat{\\rho}_{R}\\) is less sensitive to outliers. Another important feature of \\(\\hat{\\rho}_{R}\\) is that it is invariant to monotone transformations of the data. While Pearson’s correlation is very effective for detecting linear associations between two variables, the rank correlation is very effective at detecting any monotone associations between two variables. \\(\\hat{\\rho}_{R}\\) will equal 1 if \\(Y_{i}\\) is a monotone increasing function of \\(X_{i}\\), and \\(\\hat{\\rho}_{R}\\) will equal -1 if \\(Y_{i}\\) is a monotone decreasing function \\(X_{i}\\). xx &lt;- pmax(rnorm(100, mean=10), 0.01) yy &lt;- pmax(xx + rnorm(100, sd=.5), 0.01) ## Compare the usual Pearson&#39;s correlation between ## (xx, yy) and (xx, yy^2) round( c( cor(xx, yy), cor(xx, yy^2)), 3) ## [1] 0.895 0.895 ## Now do the same for Spearman&#39;s rank correlation round(c( cor(xx, yy, method=&quot;spearman&quot;), cor(xx, yy^2, method=&quot;spearman&quot;)), 3) ## [1] 0.883 0.883 \\(\\hat{\\rho}_{R}\\) can be thought of as an estimate of the following population quantity: 6.4.2 Kendall’s tau Kendall’s \\(\\tau\\)-statistic \\(U_{\\tau}\\) is given by \\[\\begin{eqnarray} U_{\\tau} &amp;=&amp; \\frac{2}{n(n-1)}\\sum_{i=1}^{n}\\sum_{j=1}^{n} I\\Big\\{ (X_{j} - X_{i})(Y_{j} - Y_{i}) &gt; 0 \\Big\\} - 1 \\nonumber \\\\ &amp;=&amp; \\frac{4}{n(n-1)}\\sum_{i=1}^{n}\\sum_{j=i+1}^{n} I\\Big\\{ (X_{j} - X_{i})(Y_{j} - Y_{i}) &gt; 0 \\Big\\} - 1 \\end{eqnarray}\\] Note that \\(U_{\\tau}\\) is a U-statistic of order \\(2\\) with kernel \\[\\begin{equation} h\\Bigg( \\begin{bmatrix} X_{1} \\\\ Y_{1} \\end{bmatrix}, \\begin{bmatrix} X_{2} \\\\ Y_{2} \\end{bmatrix} \\Bigg) = 2 \\times I\\Big\\{ (X_{2} - X_{1})(Y_{2} - Y_{1}) &gt; 0 \\Big\\} - I\\Big\\{ X_{2} \\neq X_{1} \\Big\\}I\\Big\\{ Y_{2} \\neq Y_{1} \\Big\\} \\end{equation}\\] Assuming the probability of ties is zero, Kendall’s \\(\\tau\\) can be thought of as an estimate of the following quantity \\[\\begin{equation} 2 P\\Big\\{ (X_{j} - X_{i})(Y_{j} - Y_{i}) &gt; 0 \\Big\\} - 1 \\end{equation}\\] Kendall’s \\(\\tau\\) must be in between \\(-1\\) and \\(1\\). If \\(X_{i}\\) and \\(Y_{i}\\) are idependent, Kendall’s \\(\\tau\\) will be equal to zero (why?). In the context of computing \\(U_{\\tau}\\), pairs of observations \\((X_{i}, Y_{i})\\) and \\((X_{j}, Y_{j})\\) are said to be concordant if the sign of \\(X_{j} - X_{i}\\) agrees with the sign of \\(Y_{j} - Y_{i}\\). If the sign of \\(X_{j} - X_{i}\\) and \\(Y_{j} - Y_{i}\\) do not agree, then the pairs \\((X_{i}, Y_{i})\\) and \\((X_{j}, Y_{j})\\) are said to be discordant. If either \\(X_{j}=X_{i}\\) or \\(Y_{j}=Y_{i}\\), then the pairs \\((X_{i}, Y_{i})\\) and \\((X_{j}, Y_{j})\\) are neither concordant or discordant. Let us define the following \\[\\begin{eqnarray} n_{c} &amp;=&amp; \\textrm{ the number of concordant pairs} \\nonumber \\\\ n_{d} &amp;=&amp; \\textrm{ the number of discordant pairs} \\nonumber \\\\ n_{n} &amp;=&amp; \\textrm{ number of pairs which are neither} \\nonumber \\end{eqnarray}\\] Here, we are counting \\(n_{c}\\) and \\(n_{d}\\) from the number of unique possible pairings. There are \\(n(n-1)/2\\) unique pairings, and hence \\[\\begin{equation} n_{c} + n_{d} + n_{n} = {n \\choose 2} = \\frac{n(n-1)}{2} \\end{equation}\\] If we assume that there are no ties (i.e., \\(n_{n} = 0\\)), then \\(U_{\\tau}\\) can be written as \\[\\begin{equation} U_{\\tau} = \\frac{4n_{c}}{n(n-1)} - 1 = \\frac{2n_{c} + 2n_{c} - n(n - 1)}{n(n-1)} = \\frac{2n_{c} - 2n_{d} }{n(n-1)} = \\frac{2(n_{c} - n_{d})}{n(n-1)} \\nonumber \\end{equation}\\] Under independence, the number of concordant and discordant pairs should be roughly equal. We just need ordinal data to use Kendall’s \\(\\tau\\). Kendall’s \\(\\tau\\) can be computed as long you can tell if one observation is “larger” than another. Kendall’s \\(\\tau\\) is often used in the context of assessing the agreement between different ratings. Here, we might have \\(K\\) different judges which are rating \\(J\\) different objects. If \\(r_{jk}\\) denotes the object-j rating given by judge \\(k\\), Kendall’s \\(\\tau\\) from the \\(J\\) pairs \\((r_{11}, r_{12}), \\ldots, (r_{J1}, r_{J2})\\) would give a measure of the agreement between judges 1 and 2. 6.4.3 Distance Covariance Define \\(a_{ij}\\) and \\(b_{ij}\\) as \\[\\begin{eqnarray} a_{ij} &amp;=&amp; | X_{i} - X_{j}| \\nonumber \\\\ b_{ij} &amp;=&amp; | Y_{i} - Y_{j}| \\nonumber \\end{eqnarray}\\] "],
["regression.html", "Chapter 7 The Empirical Distribution Function 7.1 Empirical Distribution Functions 7.2 The Empirical Distribution Function in R 7.3 The empirical distribution function and statistical functionals", " Chapter 7 The Empirical Distribution Function 7.1 Empirical Distribution Functions 7.1.1 Definition and Basic Properties Every random variable has a cumulative distribution function (cdf). The cdf of a random variable \\(X\\) is defined as \\[\\begin{equation} F(t) = P( X \\leq t) \\end{equation}\\] The empirical distribution function or empirical cumulative distribution function (ecdf) estimates \\(F(t)\\) by computing the proportion of observations which are less than or equal to \\(t\\). For i.i.d. random variables \\(X_{1}, \\ldots, X_{n}\\) with cdf \\(F\\), the empirical distribution function is defined as \\[\\begin{equation} \\hat{F}_{n}(t) = \\frac{1}{n}\\sum_{i=1}^{n} I( X_{i} \\leq t) \\nonumber \\end{equation}\\] Note that the empirical distribution function can be computed for any type of data without making any assumptions about the distribution from which the data arose. The only assumption we are making is that \\(X_{1}, \\ldots, X_{n}\\) constitute an i.i.d. sample from some common distribution function \\(F\\). 7.1.2 Confidence intervals for F(t) For a fixed value of \\(t\\), the distribution of \\(\\hat{F}_{n}(t)\\) is \\[\\begin{equation} \\hat{F}_{n}(t) \\sim \\textrm{Binomial}( n, F(t)) \\end{equation}\\] (why?) 7.2 The Empirical Distribution Function in R 7.3 The empirical distribution function and statistical functionals "],
["density-estimation.html", "Chapter 8 Density Estimation 8.1 Introduction 8.2 Histograms 8.3 Kernel Density Estimation 8.4 Kernel Density Estimation in Practice", " Chapter 8 Density Estimation 8.1 Introduction In this section, we focus on methods for estimating a probability density function (pdf) \\(f(x)\\). For a continuous random variable \\(X\\), areas under the probability density function are probabilities \\[\\begin{equation} P(a &lt; X &lt; b) = \\int_{a}^{b} f(x) dx \\nonumber \\end{equation}\\] and \\(f(x)\\) is related to the distribution function via \\(f(x) = F&#39;(x)\\). With parametric approaches to density estimation, you only need to estimate a couple of parameters as these parameters completely determine the form of \\(f(x)\\). For example, with a Gaussian distribution you only need to estimate \\(\\mu\\) and \\(\\sigma^{2}\\) to draw the appropriate bell curve. In a nonparametric approach, you assume that your observations \\(X_{1}, \\ldots, X_{n}\\) are an independent sample from a distribution having pdf \\(f(x)\\), but otherwise you make few assumptions about the particular form of \\(f(x)\\). 8.2 Histograms 8.2.1 Definition Histograms are one of the oldest ways to estimate a pdf. To construct a histogram, you first need to define a series of “bins”: \\(B_{1}, \\ldots, B_{D_{n}}\\). Each bin is a left-closed interval which is often assumed to have the form \\(B_{k} = [x_{0} + (k-1)h_{n}, x_{0} + kh_{n})\\): \\[\\begin{eqnarray} B_{1} &amp;=&amp; [x_{0}, x_{0} + h_{n}) \\nonumber \\\\ B_{2} &amp;=&amp; [x_{0} + h_{n}, x_{0} + 2h_{n}) \\nonumber \\\\ &amp;\\vdots&amp; \\nonumber \\\\ B_{D_{n}} &amp;=&amp; [x_{0} + (D-1)h, x_{0} + D_{n}h_{n}) \\nonumber \\end{eqnarray}\\] \\(x_{0}\\) - the origin \\(h_{n}\\) - bin width \\(D_{n}\\) - number of bins For each bin, you first need to count the number of observations which fall into that bin \\[\\begin{eqnarray} n_{k} &amp;=&amp; \\# \\text{ of observations falling into the $k^{th}$ bin } \\nonumber \\\\ &amp;=&amp; \\sum_{i=1}^{n} I( x_{0} + (k-1)h_{n} \\leq X_{i} &lt; x_{0} + kh_{n} ) \\end{eqnarray}\\] The histogram estimate of the density at a point \\(x\\) in the \\(k^{th}\\) bin is then defined as \\[\\begin{equation} \\hat{f}(x) = \\frac{n_{k}}{nh_{n}} \\end{equation}\\] Note: What is often shown in histogram plots are the actual bin counts \\(n_{k}\\) rather than the values of \\(\\hat{f}(x)\\). To see the motivation for the histogram estimate, notice that if we choose a relatively small value \\(h_{n} &gt; 0\\) \\[\\begin{equation} P(x &lt; X_{i} &lt; x + h_{n}) = \\int_{x}^{x + h_{n}} f(x) dx \\approx h_{n}f(x) \\end{equation}\\] The expected value of \\(\\hat{f}(x)\\) is \\[\\begin{equation} E\\{ \\hat{f}(x) \\} = \\frac{1}{h_{n}} P( x_{0} + (k-1)h_{n} \\leq X_{i} &lt; x_{0} + kh_{n} ) \\approx f(x) \\end{equation}\\] 8.2.2 Histograms in R In R, use the hist function hist(x, breaks, probability, plot, ...) The breaks argument Default is “Sturges”. This is a method for finding the binwidth. Can be a name giving the name of an algorithm for computing binwidth (e.g., “Scott” and “FD”). Can also be a single number. This gives the number of bins used. Could also be a .. The probability argument The plot argument Note: The default for R, is to use right-closed intervals \\((a, b]\\). This can be changed using the right argument of the hist function. ## Use a real dataset here bodywt.hist &lt;- hist(nhgh$wt, main=&quot;&quot;, xlab=&quot;Body Weight from NHANES&quot;) ## Use a real dataset here bodywt.hist2 &lt;- hist(nhgh$wt, main=&quot;Hist of BW on Probability Scale&quot;, xlab=&quot;Body Weight from NHANES&quot;, probability=TRUE) In addition to generating a histogram plot, the histogram function also returns useful stuff. names(bodywt.hist) ## [1] &quot;breaks&quot; &quot;counts&quot; &quot;density&quot; &quot;mids&quot; &quot;xname&quot; &quot;equidist&quot; breaks counts mids density bodywt.hist$breaks ## [1] 20 40 60 80 100 120 140 160 180 200 220 240 bodywt.hist$counts ## [1] 44 1160 2705 1846 721 212 71 24 7 2 3 binwidth &lt;- bodywt.hist$breaks[2] - bodywt.hist$breaks[1] bodywt.hist$density ## [1] 3.237675e-04 8.535688e-03 1.990434e-02 1.358352e-02 5.305372e-03 ## [6] 1.559971e-03 5.224430e-04 1.766004e-04 5.150846e-05 1.471670e-05 ## [11] 2.207506e-05 bodywt.hist$counts/(length(nhgh$wt)*binwidth) ## [1] 3.237675e-04 8.535688e-03 1.990434e-02 1.358352e-02 5.305372e-03 ## [6] 1.559971e-03 5.224430e-04 1.766004e-04 5.150846e-05 1.471670e-05 ## [11] 2.207506e-05 8.2.3 Performance of the Histogram Estimate 8.2.3.1 Bias/Variance Decomposition It is common to evaluate the performance of a density estimator through its mean-squared error (MSE). In general, MSE is a function of bias and variance \\[\\begin{equation} MSE = Bias^2 + Variance \\end{equation}\\] We will first look at the mean-squared error of \\(\\hat{f}( x )\\) at a single point \\(x\\) \\[\\begin{eqnarray} \\textrm{MSE}\\{ \\hat{f}(x) \\} &amp;=&amp; E\\Big( \\{ \\hat{f}(x) - f(x) \\}^{2} \\Big) \\nonumber \\\\ &amp;=&amp; \\underbrace{\\Big( E\\{ \\hat{f}(x) \\} - f(x) \\Big)^{2} }_{\\textrm{Bias Squared}} + \\underbrace{\\textrm{Var}\\{ \\hat{f}(x) \\}}_{\\textrm{Variance}} \\nonumber \\end{eqnarray}\\] In general, as the bin width \\(h_{n}\\) increases, the histogram estimate has less variation but becomes more biased. 8.2.3.2 Bias and Variance of the Histogram Estimate Recall that, for a histogram estimate, we have \\(D_{n}\\) bins where the \\(k^{th}\\) bin takes the form \\[\\begin{equation} B_{k} = [x_{0} + (k-1)h_{n}, x_{0} + kh_{n}) \\nonumber \\end{equation}\\] For a point \\(x \\in B_{k}\\), that “belongs” to the \\(k^{th}\\) bin, the histogram density estimate is \\[\\begin{equation} \\hat{f}(x) = \\frac{n_{k}}{nh_{n}}, \\quad \\textrm{ where } n_{k} = \\textrm{ number of observations falling into bin } B_{k} \\end{equation}\\] To better examine what happens as \\(n\\) changes, we will define the function \\(A_{h_{n}}(x)\\) as the function which returns the index of the interval to which \\(x\\) belongs. For example, if we had three bins \\(B_{1} = [0, 1/3)\\), \\(B_{2} = [1/3, 2/3)\\), \\(B_{3} = [2/3, 1)\\) and \\(x = 1/2\\), then \\(A_{h_{n}}( x ) = 2\\). So, we can also write the histogram density estimate as \\[\\begin{equation} \\hat{f}(x) = \\frac{n_{A_{h_{n}}(x)}}{nh_{n}} \\end{equation}\\] Note that \\(n_{A_{h_{n}}(x)}\\) is a binomial random variable with \\(n\\) trials and success probability \\(p_{h_{n}}(x)\\) (why?) \\[\\begin{equation} n_{A_{h_{n}}(x)} \\sim \\textrm{Binomial}\\{ n, p_{h_{n}}(x) \\} \\nonumber \\end{equation}\\] The success probability \\(p_{h_{n}}(x)\\) is defined as \\[\\begin{equation} p_{h_{n}}(x) = P\\Big\\{ X_{i} \\textrm{ falls into bin } A_{h_{n}}(x) \\Big\\} = \\int_{x_{0} + (A_{h_{n}}(x) - 1)h_{n}}^{x_{0} + A_{h_{n}}(x)h_{n} } f(t) dt. \\end{equation}\\] Using what is known about the Binomial distribution (i.e., \\(E( n_{A_{h_{n}}(x)} ) = np_{h_{n}}(x)\\) and \\(\\textrm{Var}( n_{A_{h_{n}}(x)} ) = np_{h_{n}}(x)\\{1 - p_{h_{n}}(x) \\}\\)), we can express the bias and variance of \\(\\hat{f}(x)\\) as \\[\\begin{eqnarray} \\textrm{Bias}\\{ \\hat{f}(x) \\} &amp;=&amp; E\\{ \\hat{f}(x) \\} - f(x) \\nonumber \\\\ &amp;=&amp; \\frac{1}{nh_{n}}E( n_{A_{h_{n}}(x)} ) - f(x) \\nonumber \\\\ &amp;=&amp; \\frac{ p_{h_{n}}(x) }{ h_{n} } - f(x) \\nonumber \\end{eqnarray}\\] and \\[\\begin{eqnarray} \\textrm{Var}\\{ \\hat{f}(x) \\} = \\frac{1}{n^{2}h_{n}^{2}}\\textrm{Var}( n_{A_{h_{n}}(x)} ) = \\frac{ p_{h_{n}}(x)\\{1 - p_{h_{n}}(x) \\} }{ nh_{n}^{2} } \\end{eqnarray}\\] Using the approximation \\(f(t) \\approx f(x) + f&#39;(x)(t - x)\\) for \\(t\\) close to \\(x\\), we have that \\[\\begin{equation} \\frac{ p_{h_{n}}(x) }{ h_{n} } = \\frac{1}{h_{n}}\\int_{x_{0} + (A_{h_{n}}(x) - 1)h_{n}}^{x_{0} + A_{h_{n}}(x)h_{n} } f(t) dt \\approx f(x) + f&#39;(x)\\{ x - x_{0} - (A_{h_{n}}(x) - 1)h_{n} \\} \\end{equation}\\] So, the bias of the histogram density estimate \\(\\hat{f}(x)\\) is \\[\\begin{equation} \\textrm{Bias}\\{ \\hat{f}(x) \\} \\approx f&#39;(x)\\{ x - (x_{0} + (A_{h_{n}}(x) - 1)h_{n}) \\} \\end{equation}\\] [[ Double-check this bias formula and check with Scott ]] Choosing a very small bin width will result in a small bias because the left endpoint of the bin \\(x_{0} + (A_{h_{n}}(x) - 1)h_{n}\\) will always be very close to \\(x\\). Now, turning to the variance of the histogram estimate \\[\\begin{equation} \\textrm{Var}\\{ \\hat{f}(x) \\} = \\frac{p_{h_{n}}(x) }{nh_{n}^{2}}\\{1 - p_{h_{n}}(x)\\} \\approx \\frac{f(x) + f&#39;(x)\\{ x - x_{0} - (A_{h_{n}}(x) - 1)h_{n} \\}}{nh_{n}}\\{1 - p_{h_{n}}(x)\\} \\approx \\frac{f(x)}{n h_{n} } \\end{equation}\\] For a more detailed description of the above approximation see Scott (1979). Note that large bin widths will reduce variance. 8.2.3.3 Point-wise Mean Squared Error Recalling (), the approximate mean-squared error of the histogram density estimate at a particular point \\(x\\) is given by \\[\\begin{eqnarray} \\textrm{MSE}\\{ \\hat{f}(x) \\} &amp;=&amp; E\\Big( \\{ \\hat{f}(x) - f(x) \\}^{2} \\Big) \\nonumber \\\\ &amp;=&amp; \\Big( \\textrm{Bias}\\{ \\hat{f}(x) \\} \\Big)^{2} + \\textrm{Var}\\{ \\hat{f}(x) \\} \\nonumber \\\\ &amp;\\approx&amp; [f&#39;(x)]^{2}\\{ x - (x_{0} + (A_{h_{n}}(x) - 1)h_{n}) \\}^{2} + \\frac{f(x)}{n h_{n} } \\end{eqnarray}\\] For any approach to bin width selection, we should have \\(h_{n} \\longrightarrow 0\\) and \\(nh_{n} \\longrightarrow \\infty\\). This MSE approximation depends on a particular choice of \\(x\\). Difficult to use () as a criterion for selecting the bandwidth because this could vary depending on your choice of \\(x\\). 8.2.3.4 Integrated Mean Squared Error and Optimal Histogram Bin Width Using mean integrated squared error (MISE) allows us to find an optimal bin width that does not depend on a particular choice of \\(x\\). The MISE is defined as \\[\\begin{eqnarray} MISE\\{ \\hat{f}(x) \\} &amp;=&amp; E\\Big\\{ \\int_{-\\infty}^{\\infty} \\{ \\hat{f}(x) - f(x) \\}^{2}dx \\Big\\} \\nonumber \\\\ &amp;=&amp; \\int_{-\\infty}^{\\infty} \\textrm{MSE}\\{ \\hat{f}(x) \\} dx \\end{eqnarray}\\] Using our previously derived approximation for the MSE, we have \\[\\begin{eqnarray} MISE\\{ \\hat{f}(x) \\} &amp;\\approx&amp; \\int x [f&#39;(x)]^{2} - x_{0}\\int [f&#39;(x)]^{2} dx + (A_{h_{n}}(x) - 1)h_{n}) \\}^{2} + \\frac{1}{n h_{n} } \\int f(x) dx \\nonumber \\\\ &amp;=&amp; \\end{eqnarray}\\] To select the optimal bin width, we minimize the MISE as a function of \\(h_{n}\\). Minimizing (), as a function of \\(h_{n}\\) yields the following formula for the optimal bin width \\[\\begin{equation} h_{n}^{opt} = \\Big( \\frac{6}{n \\int_{-\\infty}^{\\infty} [f&#39;(x)]^{2} dx} \\Big)^{1/3} = C n^{-1/3} \\nonumber \\end{equation}\\] Notice that \\(h_{n}^{opt} \\longrightarrow 0\\) and \\(nh_{n}^{opt} \\longrightarrow \\infty\\) as \\(n \\longrightarrow \\infty\\). Notice also that the optimal bin width depends on the unknown quantity \\(\\int_{-\\infty}^{\\infty} [f&#39;(x)]^{2} dx\\). 8.2.4 Choosing the Histogram Bin Width We will mention three rules for selecting the bin width of a histogram. Scott rule: (based on the optimal bin width formula) Friedman and Diaconis rule (also based on the optimal bin width formula) Sturges rule: (based on …) Both Scott and the FD rule are based on the optimal bin width formula (). The main problem with this formula is the presence of \\(\\int_{-\\infty}^{\\infty} [f&#39;(x)]^{2} dx\\). Solution: See what this quantity looks like if we assume that \\(f(x)\\) corresponds to a \\(N(\\mu, \\sigma^{2})\\) density. With this assumption, \\[\\begin{equation} h_{n}^{opt} = 3.5 \\sigma n^{-1/3} \\end{equation}\\] Scott rule: use \\(\\hat{\\sigma} = 2\\) 8.3 Kernel Density Estimation 8.3.1 Histograms and a “Naive” Density Estimate 8.3.2 Kernels, Bandwidth, and Smooth Density Estimation 8.3.3 Bias and Variance of Kernel Density Estimates 8.3.4 Bandwidth Selection 8.4 Kernel Density Estimation in Practice References "],
["sampling.html", "Chapter 9 The Jackknife 9.1 Introduction", " Chapter 9 The Jackknife 9.1 Introduction The jackknife and bootstrap are nonparametric procedures for finding standard errors and constructing confidence intervals. Why use the jackknife or bootstrap? To compute … To find … When you have no idea how to compute reasonable standard errors. "],
["ci.html", "Chapter 10 The Bootstrap and Confidence Intervals 10.1 Bootstrapping", " Chapter 10 The Bootstrap and Confidence Intervals 10.1 Bootstrapping "],
["kernel-regression.html", "Chapter 11 Kernel Regression 11.1 Introduction 11.2 The Nadaraya-Watson estimator 11.3 Local Linear and Polynomial Regression", " Chapter 11 Kernel Regression 11.1 Introduction 11.1.1 An Example 11.1.2 Linear Smoothers and Naive Nonparametric Estimates 11.2 The Nadaraya-Watson estimator 11.3 Local Linear and Polynomial Regression "],
["inference-for-regression.html", "Chapter 12 Splines and Penalized Regression 12.1 Introduction 12.2 Spline Basis Functions 12.3 Smoothing Splines/Penalized Regression", " Chapter 12 Splines and Penalized Regression 12.1 Introduction 12.2 Spline Basis Functions 12.3 Smoothing Splines/Penalized Regression 12.3.1 Selection of Smoothing Parameter "],
["decision-tree.html", "Chapter 13 Decision Trees and CART", " Chapter 13 Decision Trees and CART "],
["ensemble.html", "Chapter 14 Ensemble Methods for Prediction", " Chapter 14 Ensemble Methods for Prediction "]
]
