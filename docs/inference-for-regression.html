<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 12 Splines and Penalized Regression | Elements of Nonparametric Statistics</title>
  <meta name="description" content="Course notes for Biostatistics 685/Statistics 560 (Winter 2020)." />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 12 Splines and Penalized Regression | Elements of Nonparametric Statistics" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://nchenderson.github.io/elements-nonpar-stat/" />
  
  <meta property="og:description" content="Course notes for Biostatistics 685/Statistics 560 (Winter 2020)." />
  <meta name="github-repo" content="nchenderson/elements-nonpar-stat" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 12 Splines and Penalized Regression | Elements of Nonparametric Statistics" />
  
  <meta name="twitter:description" content="Course notes for Biostatistics 685/Statistics 560 (Winter 2020)." />
  

<meta name="author" content="Nicholas Henderson" />


<meta name="date" content="2020-04-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="kernel-regression-and-local-regression.html"/>
<link rel="next" href="decision-tree.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Biostat 685/Stat 560</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#sec:whatisnonpar"><i class="fa fa-check"></i><b>1.1</b> What is Nonparametric Statistics?</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#sec:course-outline"><i class="fa fa-check"></i><b>1.2</b> Outline of Course</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#sec:example-nonpar-tests"><i class="fa fa-check"></i><b>1.3</b> Example 1: Nonparametric vs.Â Parametric Two-Sample Testing</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#sec:example-nonpar-estimation"><i class="fa fa-check"></i><b>1.4</b> Example 2: Nonparametric Estimation</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#sec:example-nonpar-confint"><i class="fa fa-check"></i><b>1.5</b> Example 3: Confidence Intervals</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#sec:example-nonpar-regress1"><i class="fa fa-check"></i><b>1.6</b> Example 4: Nonparametric Regression with a Single Covariate</a></li>
<li class="chapter" data-level="1.7" data-path="intro.html"><a href="intro.html#sec:example-nonpar-regress2"><i class="fa fa-check"></i><b>1.7</b> Example 5: Classification and Regression Trees (CART)</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>2</b> Working with R</a></li>
<li class="part"><span><b>I Nonparametric Testing</b></span></li>
<li class="chapter" data-level="3" data-path="rank-tests.html"><a href="rank-tests.html"><i class="fa fa-check"></i><b>3</b> Rank and Sign Statistics</a><ul>
<li class="chapter" data-level="3.1" data-path="rank-tests.html"><a href="rank-tests.html#ranks"><i class="fa fa-check"></i><b>3.1</b> Ranks</a><ul>
<li class="chapter" data-level="3.1.1" data-path="rank-tests.html"><a href="rank-tests.html#definition"><i class="fa fa-check"></i><b>3.1.1</b> Definition</a></li>
<li class="chapter" data-level="3.1.2" data-path="rank-tests.html"><a href="rank-tests.html#handling-ties"><i class="fa fa-check"></i><b>3.1.2</b> Handling Ties</a></li>
<li class="chapter" data-level="3.1.3" data-path="rank-tests.html"><a href="rank-tests.html#properties-of-ranks"><i class="fa fa-check"></i><b>3.1.3</b> Properties of Ranks</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="rank-tests.html"><a href="rank-tests.html#the-wilcoxon-rank-sum-wrs-test-a-two-sample-test"><i class="fa fa-check"></i><b>3.2</b> The Wilcoxon Rank Sum (WRS) Test: A Two-Sample Test</a><ul>
<li class="chapter" data-level="3.2.1" data-path="rank-tests.html"><a href="rank-tests.html#goal-of-the-test"><i class="fa fa-check"></i><b>3.2.1</b> Goal of the Test</a></li>
<li class="chapter" data-level="3.2.2" data-path="rank-tests.html"><a href="rank-tests.html#definition-of-the-wrs-test-statistic"><i class="fa fa-check"></i><b>3.2.2</b> Definition of the WRS Test Statistic</a></li>
<li class="chapter" data-level="3.2.3" data-path="rank-tests.html"><a href="rank-tests.html#computing-p-values-for-the-wrs-test"><i class="fa fa-check"></i><b>3.2.3</b> Computing p-values for the WRS Test</a></li>
<li class="chapter" data-level="3.2.4" data-path="rank-tests.html"><a href="rank-tests.html#computing-the-wrs-test-in-r"><i class="fa fa-check"></i><b>3.2.4</b> Computing the WRS test in R</a></li>
<li class="chapter" data-level="3.2.5" data-path="rank-tests.html"><a href="rank-tests.html#additional-notes-for-the-wrs-test"><i class="fa fa-check"></i><b>3.2.5</b> Additional Notes for the WRS test</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="rank-tests.html"><a href="rank-tests.html#one-sample-tests"><i class="fa fa-check"></i><b>3.3</b> One Sample Tests</a><ul>
<li class="chapter" data-level="3.3.1" data-path="rank-tests.html"><a href="rank-tests.html#sign-test"><i class="fa fa-check"></i><b>3.3.1</b> The Sign Test</a></li>
<li class="chapter" data-level="3.3.2" data-path="rank-tests.html"><a href="rank-tests.html#the-wilcoxon-signed-rank-test"><i class="fa fa-check"></i><b>3.3.2</b> The Wilcoxon Signed Rank Test</a></li>
<li class="chapter" data-level="3.3.3" data-path="rank-tests.html"><a href="rank-tests.html#using-r-to-perform-the-sign-and-wilcoxon-tests"><i class="fa fa-check"></i><b>3.3.3</b> Using R to Perform the Sign and Wilcoxon Tests</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="rank-tests.html"><a href="rank-tests.html#power-and-comparisons-with-parametric-tests"><i class="fa fa-check"></i><b>3.4</b> Power and Comparisons with Parametric Tests</a><ul>
<li class="chapter" data-level="3.4.1" data-path="rank-tests.html"><a href="rank-tests.html#the-power-function-of-a-test"><i class="fa fa-check"></i><b>3.4.1</b> The Power Function of a Test</a></li>
<li class="chapter" data-level="3.4.2" data-path="rank-tests.html"><a href="rank-tests.html#power-comparisons-and-asymptotic-relative-efficiency"><i class="fa fa-check"></i><b>3.4.2</b> Power Comparisons and Asymptotic Relative Efficiency</a></li>
<li class="chapter" data-level="3.4.3" data-path="rank-tests.html"><a href="rank-tests.html#efficiency-examples"><i class="fa fa-check"></i><b>3.4.3</b> Efficiency Examples</a></li>
<li class="chapter" data-level="3.4.4" data-path="rank-tests.html"><a href="rank-tests.html#efficiency-comparisons-for-several-distributions"><i class="fa fa-check"></i><b>3.4.4</b> Efficiency Comparisons for Several Distributions</a></li>
<li class="chapter" data-level="3.4.5" data-path="rank-tests.html"><a href="rank-tests.html#a-power-contest"><i class="fa fa-check"></i><b>3.4.5</b> A Power âContestâ</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="rank-tests.html"><a href="rank-tests.html#linear-rank-statistics-in-general"><i class="fa fa-check"></i><b>3.5</b> Linear Rank Statistics in General</a><ul>
<li class="chapter" data-level="3.5.1" data-path="rank-tests.html"><a href="rank-tests.html#definition-1"><i class="fa fa-check"></i><b>3.5.1</b> Definition</a></li>
<li class="chapter" data-level="3.5.2" data-path="rank-tests.html"><a href="rank-tests.html#properties-of-linear-rank-statistics"><i class="fa fa-check"></i><b>3.5.2</b> Properties of Linear Rank Statistics</a></li>
<li class="chapter" data-level="3.5.3" data-path="rank-tests.html"><a href="rank-tests.html#other-examples-of-linear-rank-statistics"><i class="fa fa-check"></i><b>3.5.3</b> Other Examples of Linear Rank Statistics</a></li>
<li class="chapter" data-level="3.5.4" data-path="rank-tests.html"><a href="rank-tests.html#choosing-the-scores-a_ni"><i class="fa fa-check"></i><b>3.5.4</b> Choosing the scores <span class="math inline">\(a_{N}(i)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="rank-tests.html"><a href="rank-tests.html#additional-reading"><i class="fa fa-check"></i><b>3.6</b> Additional Reading</a></li>
<li class="chapter" data-level="3.7" data-path="rank-tests.html"><a href="rank-tests.html#exercises"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="krusk-wallis.html"><a href="krusk-wallis.html"><i class="fa fa-check"></i><b>4</b> Rank Tests for Multiple Groups</a><ul>
<li class="chapter" data-level="4.1" data-path="krusk-wallis.html"><a href="krusk-wallis.html#the-kruskal-wallis-test"><i class="fa fa-check"></i><b>4.1</b> The Kruskal-Wallis Test</a><ul>
<li class="chapter" data-level="4.1.1" data-path="krusk-wallis.html"><a href="krusk-wallis.html#definition-2"><i class="fa fa-check"></i><b>4.1.1</b> Definition</a></li>
<li class="chapter" data-level="4.1.2" data-path="krusk-wallis.html"><a href="krusk-wallis.html#asymptotic-distribution-and-connection-to-one-way-anova"><i class="fa fa-check"></i><b>4.1.2</b> Asymptotic Distribution and Connection to One-Way ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="krusk-wallis.html"><a href="krusk-wallis.html#performing-the-kruskal-wallis-test-in-r"><i class="fa fa-check"></i><b>4.2</b> Performing the Kruskal-Wallis Test in R</a></li>
<li class="chapter" data-level="4.3" data-path="krusk-wallis.html"><a href="krusk-wallis.html#comparison-of-specific-groups"><i class="fa fa-check"></i><b>4.3</b> Comparison of Specific Groups</a></li>
<li class="chapter" data-level="4.4" data-path="krusk-wallis.html"><a href="krusk-wallis.html#an-additional-example"><i class="fa fa-check"></i><b>4.4</b> An Additional Example</a></li>
<li class="chapter" data-level="4.5" data-path="krusk-wallis.html"><a href="krusk-wallis.html#additional-reading-1"><i class="fa fa-check"></i><b>4.5</b> Additional Reading</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="permutation.html"><a href="permutation.html"><i class="fa fa-check"></i><b>5</b> Permutation Tests</a><ul>
<li class="chapter" data-level="5.1" data-path="permutation.html"><a href="permutation.html#notation"><i class="fa fa-check"></i><b>5.1</b> Notation</a></li>
<li class="chapter" data-level="5.2" data-path="permutation.html"><a href="permutation.html#permutation-tests-for-the-two-sample-problem"><i class="fa fa-check"></i><b>5.2</b> Permutation Tests for the Two-Sample Problem</a><ul>
<li class="chapter" data-level="5.2.1" data-path="permutation.html"><a href="permutation.html#example-1"><i class="fa fa-check"></i><b>5.2.1</b> Example 1</a></li>
<li class="chapter" data-level="5.2.2" data-path="permutation.html"><a href="permutation.html#permutation-test-p-values"><i class="fa fa-check"></i><b>5.2.2</b> Permutation Test p-values</a></li>
<li class="chapter" data-level="5.2.3" data-path="permutation.html"><a href="permutation.html#example-2-ratios-of-means"><i class="fa fa-check"></i><b>5.2.3</b> Example 2: Ratios of Means</a></li>
<li class="chapter" data-level="5.2.4" data-path="permutation.html"><a href="permutation.html#example-3-differences-in-quantiles"><i class="fa fa-check"></i><b>5.2.4</b> Example 3: Differences in Quantiles</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="permutation.html"><a href="permutation.html#the-permutation-test-as-a-conditional-test"><i class="fa fa-check"></i><b>5.3</b> The Permutation Test as a Conditional Test</a></li>
<li class="chapter" data-level="5.4" data-path="permutation.html"><a href="permutation.html#a-permutation-test-for-correlation"><i class="fa fa-check"></i><b>5.4</b> A Permutation Test for Correlation</a></li>
<li class="chapter" data-level="5.5" data-path="permutation.html"><a href="permutation.html#a-permutation-test-for-variable-importance-in-regression-and-machine-learning"><i class="fa fa-check"></i><b>5.5</b> A Permutation Test for Variable Importance in Regression and Machine Learning</a></li>
</ul></li>
<li class="part"><span><b>II Nonparametric Estimation</b></span></li>
<li class="chapter" data-level="6" data-path="ustat.html"><a href="ustat.html"><i class="fa fa-check"></i><b>6</b> U-Statistics</a><ul>
<li class="chapter" data-level="6.1" data-path="ustat.html"><a href="ustat.html#definition-3"><i class="fa fa-check"></i><b>6.1</b> Definition</a></li>
<li class="chapter" data-level="6.2" data-path="ustat.html"><a href="ustat.html#examples"><i class="fa fa-check"></i><b>6.2</b> Examples</a><ul>
<li class="chapter" data-level="6.2.1" data-path="ustat.html"><a href="ustat.html#example-1-the-sample-mean"><i class="fa fa-check"></i><b>6.2.1</b> Example 1: The Sample Mean</a></li>
<li class="chapter" data-level="6.2.2" data-path="ustat.html"><a href="ustat.html#example-2-the-sample-variance"><i class="fa fa-check"></i><b>6.2.2</b> Example 2: The Sample Variance</a></li>
<li class="chapter" data-level="6.2.3" data-path="ustat.html"><a href="ustat.html#example-3-ginis-mean-difference"><i class="fa fa-check"></i><b>6.2.3</b> Example 3: Giniâs Mean Difference</a></li>
<li class="chapter" data-level="6.2.4" data-path="ustat.html"><a href="ustat.html#example-4-wilcoxon-signed-rank-statistic"><i class="fa fa-check"></i><b>6.2.4</b> Example 4: Wilcoxon Signed Rank Statistic</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="ustat.html"><a href="ustat.html#inference-using-u-statistics"><i class="fa fa-check"></i><b>6.3</b> Inference using U-statistics</a></li>
<li class="chapter" data-level="6.4" data-path="ustat.html"><a href="ustat.html#u-statistics-for-two-sample-problems"><i class="fa fa-check"></i><b>6.4</b> U-statistics for Two-Sample Problems</a><ul>
<li class="chapter" data-level="6.4.1" data-path="ustat.html"><a href="ustat.html#the-mann-whitney-statistic"><i class="fa fa-check"></i><b>6.4.1</b> The Mann-Whitney Statistic</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="ustat.html"><a href="ustat.html#measures-of-association"><i class="fa fa-check"></i><b>6.5</b> Measures of Association</a><ul>
<li class="chapter" data-level="6.5.1" data-path="ustat.html"><a href="ustat.html#spearmans-rank-correlation"><i class="fa fa-check"></i><b>6.5.1</b> Spearmanâs Rank Correlation</a></li>
<li class="chapter" data-level="6.5.2" data-path="ustat.html"><a href="ustat.html#kendalls-tau"><i class="fa fa-check"></i><b>6.5.2</b> Kendallâs tau</a></li>
<li class="chapter" data-level="6.5.3" data-path="ustat.html"><a href="ustat.html#distance-covariance-and-correlation"><i class="fa fa-check"></i><b>6.5.3</b> Distance Covariance and Correlation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="edf.html"><a href="edf.html"><i class="fa fa-check"></i><b>7</b> The Empirical Distribution Function</a><ul>
<li class="chapter" data-level="7.1" data-path="edf.html"><a href="edf.html#definition-and-basic-properties"><i class="fa fa-check"></i><b>7.1</b> Definition and Basic Properties</a></li>
<li class="chapter" data-level="7.2" data-path="edf.html"><a href="edf.html#confidence-intervals-for-ft"><i class="fa fa-check"></i><b>7.2</b> Confidence intervals for F(t)</a></li>
<li class="chapter" data-level="7.3" data-path="edf.html"><a href="edf.html#the-empirical-distribution-function-in-r"><i class="fa fa-check"></i><b>7.3</b> The Empirical Distribution Function in R</a></li>
<li class="chapter" data-level="7.4" data-path="edf.html"><a href="edf.html#the-kolmogorov-smirnov-test"><i class="fa fa-check"></i><b>7.4</b> The Kolmogorov-Smirnov Test</a></li>
<li class="chapter" data-level="7.5" data-path="edf.html"><a href="edf.html#the-empirical-distribution-function-and-statistical-functionals"><i class="fa fa-check"></i><b>7.5</b> The empirical distribution function and statistical functionals</a></li>
<li class="chapter" data-level="7.6" data-path="edf.html"><a href="edf.html#additional-reading-2"><i class="fa fa-check"></i><b>7.6</b> Additional Reading</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="density-estimation.html"><a href="density-estimation.html"><i class="fa fa-check"></i><b>8</b> Density Estimation</a><ul>
<li class="chapter" data-level="8.1" data-path="density-estimation.html"><a href="density-estimation.html#introduction"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="density-estimation.html"><a href="density-estimation.html#histograms"><i class="fa fa-check"></i><b>8.2</b> Histograms</a><ul>
<li class="chapter" data-level="8.2.1" data-path="density-estimation.html"><a href="density-estimation.html#definition-5"><i class="fa fa-check"></i><b>8.2.1</b> Definition</a></li>
<li class="chapter" data-level="8.2.2" data-path="density-estimation.html"><a href="density-estimation.html#histograms-in-r"><i class="fa fa-check"></i><b>8.2.2</b> Histograms in R</a></li>
<li class="chapter" data-level="8.2.3" data-path="density-estimation.html"><a href="density-estimation.html#performance-of-the-histogram-estimate-and-bin-width-selection"><i class="fa fa-check"></i><b>8.2.3</b> Performance of the Histogram Estimate and Bin Width Selection</a></li>
<li class="chapter" data-level="8.2.4" data-path="density-estimation.html"><a href="density-estimation.html#choosing-the-histogram-bin-width"><i class="fa fa-check"></i><b>8.2.4</b> Choosing the Histogram Bin Width</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="density-estimation.html"><a href="density-estimation.html#a-box-type-density-estimate"><i class="fa fa-check"></i><b>8.3</b> A Box-type Density Estimate</a></li>
<li class="chapter" data-level="8.4" data-path="density-estimation.html"><a href="density-estimation.html#kernel-density-estimation"><i class="fa fa-check"></i><b>8.4</b> Kernel Density Estimation</a><ul>
<li class="chapter" data-level="8.4.1" data-path="density-estimation.html"><a href="density-estimation.html#definition-6"><i class="fa fa-check"></i><b>8.4.1</b> Definition</a></li>
<li class="chapter" data-level="8.4.2" data-path="density-estimation.html"><a href="density-estimation.html#bias-variance-and-amise-of-kernel-density-estimates"><i class="fa fa-check"></i><b>8.4.2</b> Bias, Variance, and AMISE of Kernel Density Estimates</a></li>
<li class="chapter" data-level="8.4.3" data-path="density-estimation.html"><a href="density-estimation.html#bandwidth-selection-with-the-normal-reference-rule-and-silvermans-rule-of-thumb"><i class="fa fa-check"></i><b>8.4.3</b> Bandwidth Selection with the Normal Reference Rule and Silvermanâs âRule of Thumbâ</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="density-estimation.html"><a href="density-estimation.html#cross-validation-for-bandwidth-selection"><i class="fa fa-check"></i><b>8.5</b> Cross-Validation for Bandwidth Selection</a><ul>
<li class="chapter" data-level="8.5.1" data-path="density-estimation.html"><a href="density-estimation.html#squared-error-cross-validation"><i class="fa fa-check"></i><b>8.5.1</b> Squared-Error Cross-Validation</a></li>
<li class="chapter" data-level="8.5.2" data-path="density-estimation.html"><a href="density-estimation.html#computing-the-cross-validation-bandwidth"><i class="fa fa-check"></i><b>8.5.2</b> Computing the Cross-validation Bandwidth</a></li>
<li class="chapter" data-level="8.5.3" data-path="density-estimation.html"><a href="density-estimation.html#likelihood-cross-validation"><i class="fa fa-check"></i><b>8.5.3</b> Likelihood Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="density-estimation.html"><a href="density-estimation.html#density-estimation-in-r"><i class="fa fa-check"></i><b>8.6</b> Density Estimation in R</a></li>
<li class="chapter" data-level="8.7" data-path="density-estimation.html"><a href="density-estimation.html#additional-reading-3"><i class="fa fa-check"></i><b>8.7</b> Additional Reading</a></li>
</ul></li>
<li class="part"><span><b>III Quantifying Uncertainty</b></span></li>
<li class="chapter" data-level="9" data-path="bootstrap-main.html"><a href="bootstrap-main.html"><i class="fa fa-check"></i><b>9</b> The Bootstrap</a><ul>
<li class="chapter" data-level="9.1" data-path="bootstrap-main.html"><a href="bootstrap-main.html#introduction-1"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="bootstrap-main.html"><a href="bootstrap-main.html#description-of-the-bootstrap"><i class="fa fa-check"></i><b>9.2</b> Description of the Bootstrap</a><ul>
<li class="chapter" data-level="9.2.1" data-path="bootstrap-main.html"><a href="bootstrap-main.html#description"><i class="fa fa-check"></i><b>9.2.1</b> Description</a></li>
<li class="chapter" data-level="9.2.2" data-path="bootstrap-main.html"><a href="bootstrap-main.html#example-confidence-intervals-for-the-rate-parameter-of-an-exponential-distribution"><i class="fa fa-check"></i><b>9.2.2</b> Example: Confidence Intervals for the Rate Parameter of an Exponential Distribution</a></li>
<li class="chapter" data-level="9.2.3" data-path="bootstrap-main.html"><a href="bootstrap-main.html#example-confidence-intervals-for-the-ratio-of-two-quantiles"><i class="fa fa-check"></i><b>9.2.3</b> Example: Confidence Intervals for the Ratio of Two Quantiles</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="bootstrap-main.html"><a href="bootstrap-main.html#why-is-the-bootstrap-procedure-reasonable"><i class="fa fa-check"></i><b>9.3</b> Why is the Bootstrap Procedure Reasonable?</a></li>
<li class="chapter" data-level="9.4" data-path="bootstrap-main.html"><a href="bootstrap-main.html#pivotal-bootstrap-confidence-intervals"><i class="fa fa-check"></i><b>9.4</b> Pivotal Bootstrap Confidence Intervals</a></li>
<li class="chapter" data-level="9.5" data-path="bootstrap-main.html"><a href="bootstrap-main.html#the-parametric-bootstrap"><i class="fa fa-check"></i><b>9.5</b> The Parametric Bootstrap</a><ul>
<li class="chapter" data-level="9.5.1" data-path="bootstrap-main.html"><a href="bootstrap-main.html#parametric-bootstrap-for-the-median-age-from-the-kidney-data"><i class="fa fa-check"></i><b>9.5.1</b> Parametric Bootstrap for the Median Age from the Kidney Data</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="bootstrap-main.html"><a href="bootstrap-main.html#additional-reading-4"><i class="fa fa-check"></i><b>9.6</b> Additional Reading</a></li>
<li class="chapter" data-level="9.7" data-path="bootstrap-main.html"><a href="bootstrap-main.html#exercises-1"><i class="fa fa-check"></i><b>9.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ci.html"><a href="ci.html"><i class="fa fa-check"></i><b>10</b> Bootstrap Examples and the Jackknife</a><ul>
<li class="chapter" data-level="10.1" data-path="ci.html"><a href="ci.html#the-parametric-bootstrap-for-an-ar1-model"><i class="fa fa-check"></i><b>10.1</b> The Parametric Bootstrap for an AR(1) model</a></li>
<li class="chapter" data-level="10.2" data-path="ci.html"><a href="ci.html#using-the-bootstrap-in-regression"><i class="fa fa-check"></i><b>10.2</b> Using the Bootstrap in Regression</a><ul>
<li class="chapter" data-level="10.2.1" data-path="ci.html"><a href="ci.html#parametric-bootstrap-for-regression"><i class="fa fa-check"></i><b>10.2.1</b> Parametric Bootstrap for Regression</a></li>
<li class="chapter" data-level="10.2.2" data-path="ci.html"><a href="ci.html#nonparametric-bootstrap-for-regression"><i class="fa fa-check"></i><b>10.2.2</b> Nonparametric Bootstrap for Regression</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="ci.html"><a href="ci.html#pointwise-confidence-intervals-for-a-density-function"><i class="fa fa-check"></i><b>10.3</b> Pointwise Confidence Intervals for a Density Function</a></li>
<li class="chapter" data-level="10.4" data-path="ci.html"><a href="ci.html#when-can-the-bootstrap-fail"><i class="fa fa-check"></i><b>10.4</b> When can the Bootstrap Fail?</a><ul>
<li class="chapter" data-level="10.4.1" data-path="ci.html"><a href="ci.html#example-the-shifted-exponential-distribution"><i class="fa fa-check"></i><b>10.4.1</b> Example: The Shifted Exponential Distribution</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="ci.html"><a href="ci.html#the-jackknife"><i class="fa fa-check"></i><b>10.5</b> The Jackknife</a></li>
</ul></li>
<li class="part"><span><b>IV Nonparametric Regression: Part I</b></span></li>
<li class="chapter" data-level="11" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html"><i class="fa fa-check"></i><b>11</b> Kernel Regression and Local Regression</a><ul>
<li class="chapter" data-level="11.1" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#introduction-2"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#kernel-regression"><i class="fa fa-check"></i><b>11.2</b> Kernel Regression</a><ul>
<li class="chapter" data-level="11.2.1" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#the-regressogram"><i class="fa fa-check"></i><b>11.2.1</b> The Regressogram</a></li>
<li class="chapter" data-level="11.2.2" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#the-local-average-estimator"><i class="fa fa-check"></i><b>11.2.2</b> The Local Average Estimator</a></li>
<li class="chapter" data-level="11.2.3" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#k-nearest-neighbor-k-nn-regression"><i class="fa fa-check"></i><b>11.2.3</b> k-Nearest Neighbor (k-NN) Regression</a></li>
<li class="chapter" data-level="11.2.4" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#the-nadaraya-watson-estimator"><i class="fa fa-check"></i><b>11.2.4</b> The Nadaraya-Watson Estimator</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#local-linear-regression"><i class="fa fa-check"></i><b>11.3</b> Local Linear Regression</a><ul>
<li class="chapter" data-level="11.3.1" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#definition-7"><i class="fa fa-check"></i><b>11.3.1</b> Definition</a></li>
<li class="chapter" data-level="11.3.2" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#advantages-of-the-local-linear-estimator"><i class="fa fa-check"></i><b>11.3.2</b> Advantages of the Local Linear Estimator</a></li>
<li class="chapter" data-level="11.3.3" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#an-example-in-r"><i class="fa fa-check"></i><b>11.3.3</b> An Example in R</a></li>
<li class="chapter" data-level="11.3.4" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#local-polynomial-regression"><i class="fa fa-check"></i><b>11.3.4</b> Local Polynomial Regression</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#selecting-the-bandwidthsmoothing-parameter"><i class="fa fa-check"></i><b>11.4</b> Selecting the Bandwidth/Smoothing Parameter</a><ul>
<li class="chapter" data-level="11.4.1" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#representing-in-linear-form"><i class="fa fa-check"></i><b>11.4.1</b> Representing in Linear Form</a></li>
<li class="chapter" data-level="11.4.2" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#the-cp-statistic"><i class="fa fa-check"></i><b>11.4.2</b> The Cp Statistic</a></li>
<li class="chapter" data-level="11.4.3" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>11.4.3</b> Leave-one-out Cross Validation</a></li>
<li class="chapter" data-level="11.4.4" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#example-choosing-the-best-bin-width-for-the-local-average-estimator."><i class="fa fa-check"></i><b>11.4.4</b> Example: Choosing the Best Bin Width for the Local Average Estimator.</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#additional-functions-in-r"><i class="fa fa-check"></i><b>11.5</b> Additional functions in R</a></li>
<li class="chapter" data-level="11.6" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#multivariate-problems"><i class="fa fa-check"></i><b>11.6</b> Multivariate Problems</a></li>
<li class="chapter" data-level="11.7" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#additional-reading-5"><i class="fa fa-check"></i><b>11.7</b> Additional Reading</a></li>
<li class="chapter" data-level="11.8" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#exercises-2"><i class="fa fa-check"></i><b>11.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="inference-for-regression.html"><a href="inference-for-regression.html"><i class="fa fa-check"></i><b>12</b> Splines and Penalized Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="inference-for-regression.html"><a href="inference-for-regression.html#introduction-3"><i class="fa fa-check"></i><b>12.1</b> Introduction</a><ul>
<li class="chapter" data-level="12.1.1" data-path="inference-for-regression.html"><a href="inference-for-regression.html#regressogram-piecewise-constant-estimate"><i class="fa fa-check"></i><b>12.1.1</b> Regressogram (Piecewise Constant Estimate)</a></li>
<li class="chapter" data-level="12.1.2" data-path="inference-for-regression.html"><a href="inference-for-regression.html#piecewise-linear-estimates"><i class="fa fa-check"></i><b>12.1.2</b> Piecewise Linear Estimates</a></li>
<li class="chapter" data-level="12.1.3" data-path="inference-for-regression.html"><a href="inference-for-regression.html#piecewise-cubic-estimates"><i class="fa fa-check"></i><b>12.1.3</b> Piecewise Cubic Estimates</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="inference-for-regression.html"><a href="inference-for-regression.html#piecewise-linear-estimates-with-continuity-linear-splines"><i class="fa fa-check"></i><b>12.2</b> Piecewise Linear Estimates with Continuity (Linear Splines)</a></li>
<li class="chapter" data-level="12.3" data-path="inference-for-regression.html"><a href="inference-for-regression.html#cubic-splines-and-regression-with-splines"><i class="fa fa-check"></i><b>12.3</b> Cubic Splines and Regression with Splines</a><ul>
<li class="chapter" data-level="12.3.1" data-path="inference-for-regression.html"><a href="inference-for-regression.html#example-smooth-piecewise-cubic-model-with-2-knots"><i class="fa fa-check"></i><b>12.3.1</b> Example: Smooth Piecewise Cubic Model with 2 Knots</a></li>
<li class="chapter" data-level="12.3.2" data-path="inference-for-regression.html"><a href="inference-for-regression.html#cubic-splines"><i class="fa fa-check"></i><b>12.3.2</b> Cubic Splines</a></li>
<li class="chapter" data-level="12.3.3" data-path="inference-for-regression.html"><a href="inference-for-regression.html#estimating-the-coefficients-of-a-cubic-spline"><i class="fa fa-check"></i><b>12.3.3</b> Estimating the Coefficients of a Cubic Spline</a></li>
<li class="chapter" data-level="12.3.4" data-path="inference-for-regression.html"><a href="inference-for-regression.html#an-example-in-r-1"><i class="fa fa-check"></i><b>12.3.4</b> An example in R</a></li>
<li class="chapter" data-level="12.3.5" data-path="inference-for-regression.html"><a href="inference-for-regression.html#natural-cubic-splines"><i class="fa fa-check"></i><b>12.3.5</b> Natural Cubic Splines</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="inference-for-regression.html"><a href="inference-for-regression.html#smoothing-splines"><i class="fa fa-check"></i><b>12.4</b> Smoothing Splines</a></li>
<li class="chapter" data-level="12.5" data-path="inference-for-regression.html"><a href="inference-for-regression.html#knotpenalty-term-selection-for-splines"><i class="fa fa-check"></i><b>12.5</b> Knot/Penalty Term Selection for Splines</a><ul>
<li class="chapter" data-level="12.5.1" data-path="inference-for-regression.html"><a href="inference-for-regression.html#the-cp-statistic-1"><i class="fa fa-check"></i><b>12.5.1</b> The Cp Statistic</a></li>
<li class="chapter" data-level="12.5.2" data-path="inference-for-regression.html"><a href="inference-for-regression.html#leave-one-out-cross-validation-1"><i class="fa fa-check"></i><b>12.5.2</b> Leave-one-out Cross-Validation</a></li>
<li class="chapter" data-level="12.5.3" data-path="inference-for-regression.html"><a href="inference-for-regression.html#generalized-cross-validation"><i class="fa fa-check"></i><b>12.5.3</b> Generalized Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="inference-for-regression.html"><a href="inference-for-regression.html#fitting-smoothing-splines-in-r"><i class="fa fa-check"></i><b>12.6</b> Fitting Smoothing Splines in R</a><ul>
<li class="chapter" data-level="12.6.1" data-path="inference-for-regression.html"><a href="inference-for-regression.html#smoothing-parameter-selection-for-the-smoothing-spline-with-the-cp-statistic"><i class="fa fa-check"></i><b>12.6.1</b> Smoothing Parameter Selection for the Smoothing Spline with the Cp statistic</a></li>
<li class="chapter" data-level="12.6.2" data-path="inference-for-regression.html"><a href="inference-for-regression.html#knot-selection-for-regression-splines-with-the-cp-statistic"><i class="fa fa-check"></i><b>12.6.2</b> Knot Selection for Regression Splines with the Cp statistic</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>V Nonparametric Regression: Part II</b></span></li>
<li class="chapter" data-level="13" data-path="decision-tree.html"><a href="decision-tree.html"><i class="fa fa-check"></i><b>13</b> Decision Trees and CART</a><ul>
<li class="chapter" data-level="13.1" data-path="decision-tree.html"><a href="decision-tree.html#introduction-4"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ensemble.html"><a href="ensemble.html"><i class="fa fa-check"></i><b>14</b> Ensemble Methods for Prediction</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Elements of Nonparametric Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="inference-for-regression" class="section level1">
<h1><span class="header-section-number">Chapter 12</span> Splines and Penalized Regression</h1>
<div id="introduction-3" class="section level2">
<h2><span class="header-section-number">12.1</span> Introduction</h2>
<ul>
<li><p>In this chapter, we will focus on using basis functions for estimating
the regression function.</p></li>
<li><p>That is, we will look at regression function estimates of the form
<span class="math display">\[\begin{equation}
\hat{m}(x) = \hat{\beta}_{0}\varphi_{0}(x) + \sum_{j=1}^{p} \hat{\beta}_{j}\varphi_{j}(x)  \nonumber
\end{equation}\]</span>
where <span class="math inline">\(\varphi_{0}(x), \varphi_{1}(x), \ldots, \varphi_{p}(x)\)</span> will be referred to as basis functions.</p></li>
<li><p>We will usually either ignore <span class="math inline">\(\varphi_{0}(x)\)</span> or assume that <span class="math inline">\(\varphi_{0}(x) = 1\)</span>.</p></li>
<li><p>If you use a relatively large number of appropriately chosen basis functions,
you can represent even quite complicated functions with some linear
combination of the basis functions.</p></li>
</ul>
<hr />
<p><strong>Examples</strong></p>
<ul>
<li><p>Basis functions for a straight-line linear regression
<span class="math display">\[\begin{equation}
\varphi_{0}(x) = 1  \qquad \varphi_{1}(x) = x  \nonumber
\end{equation}\]</span></p></li>
<li><p>Basis functions for a polynomial regression with degree 3
<span class="math display">\[\begin{equation}
\varphi_{0}(x) = 1  \qquad \varphi_{1}(x) = x  \qquad \varphi_{2}(x) = x^{2} \qquad \varphi_{3}(x) = x^{3} \nonumber
\end{equation}\]</span></p></li>
<li><p>Basis functions for a regressogram with bins <span class="math inline">\([l_{k}, u_{k})\)</span>, <span class="math inline">\(k = 1, \ldots, K\)</span>:
<span class="math display">\[\begin{equation}
\varphi_{k}(x) = I\big( l_{k} \leq x &lt; u_{k}  \big), \quad k = 1, \ldots, K  \nonumber
\end{equation}\]</span></p></li>
</ul>
<hr />
<div id="regressogram-piecewise-constant-estimate" class="section level3">
<h3><span class="header-section-number">12.1.1</span> Regressogram (Piecewise Constant Estimate)</h3>
<ul>
<li><p>Letâs consider the regressogram again. The regressogram estimate could be written as
<span class="math display">\[\begin{equation}
\hat{m}_{h_{n}}^{R}(x) = \sum_{k=1}^{D_{n}} a_{k, h_{n}}\varphi_{k}(x) = \sum_{k=1}^{D_{n}} a_{k,h_{n}} I\big( x \in B_{k} \big)
\end{equation}\]</span>
where the coefficents <span class="math inline">\(a_{k, h_{n}}\)</span> are given by
<span class="math display">\[\begin{equation}
a_{k, h_{n}} = \frac{1}{ n_{k,h_{n}} } \sum_{i=1}^{n} Y_{i}I\big( x_{i} \in B_{k} \big)  \nonumber
\end{equation}\]</span></p></li>
<li><p>We can think of the regressogram as a basis function estimate with the basis functions
<span class="math display">\[\begin{equation}
\varphi_{1}(x) = I\big( x \in B_{1} \big), \ldots, \varphi_{D_{n}}(x) = I\big( x \in B_{D_{n}} \big) \nonumber
\end{equation}\]</span></p></li>
<li><p>The regressogram estimate will be a piecewise constant function that is constant within each of the bins.</p></li>
</ul>
<div class="figure"><span id="fig:unnamed-chunk-1"></span>
<img src="12-splines_files/figure-html/unnamed-chunk-1-1.png" alt="Basis functions for a regressogram with the following 3 bins: [0,1/3), [1/3, 2/3), [2/3, 1)" width="672" />
<p class="caption">
Figure 8.1: Basis functions for a regressogram with the following 3 bins: [0,1/3), [1/3, 2/3), [2/3, 1)
</p>
</div>
<div class="figure"><span id="fig:unnamed-chunk-2"></span>
<img src="12-splines_files/figure-html/unnamed-chunk-2-1.png" alt="Regressogram estimate of a regression function with 3 bins." width="672" />
<p class="caption">
Figure 12.1: Regressogram estimate of a regression function with 3 bins.
</p>
</div>
</div>
<div id="piecewise-linear-estimates" class="section level3">
<h3><span class="header-section-number">12.1.2</span> Piecewise Linear Estimates</h3>
<ul>
<li><p>Instead of a piecewise constant estimate of <span class="math inline">\(m(x)\)</span>, we could use an estimate which is piecewise linear
by using the following <span class="math inline">\(2p\)</span> basis functions
<span class="math display">\[\begin{eqnarray}
\varphi_{1}(x) &amp;=&amp; I(x \in B_{1}) \nonumber \\
&amp;\vdots&amp;  \nonumber \\
\varphi_{p}(x) &amp;=&amp; I(x \in B_{p}) \nonumber \\
\varphi_{p+1}(x) &amp;=&amp; x I(x \in B_{1})  \nonumber \\
&amp;\vdots&amp; \nonumber \\
\varphi_{2p}(x) &amp;=&amp; x I(x \in B_{p}) \nonumber
\end{eqnarray}\]</span>
if we now let <span class="math inline">\(p = D_{n}\)</span> denote the number of âbinsâ.</p></li>
<li><p>The figure below shows an example of a regression function that is piecewise linear with 3 different bins.</p></li>
<li><p>While a piecewise linear model is perhaps a more flexible method than the regressogram,
the piecwise linear model will still have big jumps at the bin boundaries and have an overall
unpleasant appearance.</p></li>
</ul>
<div class="figure"><span id="fig:pwlinear"></span>
<img src="12-splines_files/figure-html/pwlinear-1.png" alt="Example of a regression function estimate that is piecewise linear within 3 bins." width="672" />
<p class="caption">
Figure 12.2: Example of a regression function estimate that is piecewise linear within 3 bins.
</p>
</div>
</div>
<div id="piecewise-cubic-estimates" class="section level3">
<h3><span class="header-section-number">12.1.3</span> Piecewise Cubic Estimates</h3>
<ul>
<li><p>If we wanted to allow for more flexible forms of the regression function estimate within
each bin we could fit a higher order polynomial model within each bin.</p></li>
<li><p>That is, the regression function estimate within the <span class="math inline">\(k^{th}\)</span> bin will have the form
<span class="math display">\[\begin{equation}
\hat{m}(x)I(x \in B_{k}) = \hat{\beta}_{0k} + \hat{\beta}_{1k}x + \hat{\beta}_{2k}x^{2} + \hat{\beta}_{3k}x^{3} \nonumber 
\end{equation}\]</span></p></li>
<li><p>To fit a piecewise cubic model with <span class="math inline">\(p\)</span> bins, we would need the following <span class="math inline">\(4p\)</span> basis functions
<span class="math display">\[\begin{eqnarray}
\varphi_{1}(x) &amp;=&amp; I(x \in B_{1}),  \ldots,  \varphi_{p}(x) = I(x \in B_{p}) \nonumber \\
\varphi_{p+1}(x) &amp;=&amp; xI(x \in B_{1}),  \ldots, \varphi_{2p}(x) = xI(x \in B_{p}) \nonumber \\
\varphi_{2p+1}(x) &amp;=&amp; x^{2}I(x \in B_{1}),  \ldots,  \varphi_{3p}(x) = x^{2}I(x \in B_{p}) \nonumber \\
\varphi_{3p+1}(x) &amp;=&amp; x^{3}I(x \in B_{1}), \ldots, \varphi_{4p}(x) = x^{3}I(x \in B_{p}) \nonumber \\
\end{eqnarray}\]</span></p></li>
</ul>
<div class="figure"><span id="fig:pwcubic"></span>
<img src="12-splines_files/figure-html/pwcubic-1.png" alt="Example of a regression function estimate that is piecewise cubic within 3 bins." width="672" />
<p class="caption">
Figure 12.3: Example of a regression function estimate that is piecewise cubic within 3 bins.
</p>
</div>
</div>
</div>
<div id="piecewise-linear-estimates-with-continuity-linear-splines" class="section level2">
<h2><span class="header-section-number">12.2</span> Piecewise Linear Estimates with Continuity (Linear Splines)</h2>
<ul>
<li><p>In the spline world, one typically talks about âknotsâ rather than âbinsâ.</p></li>
<li><p>You can think of knots as the dividing points between the bins.</p></li>
<li><p>We will let <span class="math inline">\(u_{1} &lt; u_{2} &lt; \ldots &lt; u_{q}\)</span> denote the choice of knots.</p></li>
<li><p>The bins corresponding to this set of knots would then be <span class="math inline">\(B_{1} = (-\infty, u_{1}), B_{2} = [u_{1}, u_{2}), B_{3} = [u_{2}, u_{3}), \ldots, B_{q+1} = [u_{q}, \infty)\)</span>.</p></li>
<li><p>In other words, <span class="math inline">\(q\)</span> knots defines <span class="math inline">\(B_{q+1}\)</span> âbinsâ of the form <span class="math inline">\(B_{k} = [u_{k-1}, u_{k+1})\)</span>, for <span class="math inline">\(k = 2, \ldots, q\)</span>.</p></li>
</ul>
<hr />
<ul>
<li><p>Letâs return to the piecewise linear estimate shown in Figure <a href="inference-for-regression.html#fig:pwlinear">12.2</a>. This has two knots <span class="math inline">\(u_{1} = 1/3\)</span> and <span class="math inline">\(u_{2} = 2/3\)</span> and
hence <span class="math inline">\(3\)</span> bins.</p></li>
<li><p>Also, this piecewise linear model has <span class="math inline">\(6\)</span> parameters. If we let <span class="math inline">\((\beta_{0k}, \beta_{1k})\)</span> denote the intercept and slope parameters
for the <span class="math inline">\(k^{th}\)</span> bin, there are <span class="math inline">\(6\)</span> parameters in total because we have <span class="math inline">\(3\)</span> bins, and we could write a piecewise linear model as
<span class="math display">\[\begin{equation}
m(x) = 
\begin{cases} 
\beta_{01} + \beta_{11}x &amp; \text{ if } x &lt; u_{1} \nonumber \\
\beta_{02} + \beta_{12}x &amp; \text{ if } u_{1} \leq x &lt; u_{2} \nonumber \\
\beta_{03} + \beta_{13}x &amp; \text{if } x \geq u_{2}
\end{cases}
\end{equation}\]</span></p></li>
<li><p>We can make the estimated regression function look better by ensuring that it is
continuous and does not have discontinuities at the knots.</p></li>
</ul>
<hr />
<ul>
<li><p>To make the estimated regression curve continuous, we just need to make sure it is continuous at the knots.</p></li>
<li><p>That is, the regression coefficients need to satisfy the following two constraints:
<span class="math display">\[\begin{equation}
\beta_{01} + \beta_{11}u_{1} = \beta_{02} + \beta_{12}u_{1} \qquad \textrm{and} \qquad \beta_{02} + \beta_{12}u_{2} = \beta_{03} + \beta_{03}u_{2}  \nonumber
\end{equation}\]</span></p></li>
<li><p>Because we have two linear constraints, we should expect that the number of âfree parametersâ in a piecewise linear model
with continuity constraints should equal <span class="math inline">\(6 - 2 = 4\)</span>.</p></li>
</ul>
<hr />
<ul>
<li><p>Indeed, if we use the fact that under the continuity constraints: <span class="math inline">\(\beta_{02} = \beta_{01} + \beta_{11}u_{1} - \beta_{12}u_{1}\)</span> and <span class="math inline">\(\beta_{03} = \beta_{02} + \beta_{12}u_{2} - \beta_{13}u_{2}\)</span>, then we can rewrite the piecewise linear model as
<span class="math display">\[\begin{equation}
m(x) = 
\begin{cases} 
\beta_{01} + \beta_{11}x &amp; \text{ if } x &lt; u_{1} \nonumber \\
 \beta_{01} + \beta_{11}x + (\beta_{12} - \beta_{11})(x - u_{1})  &amp; \text{ if } u_{1} \leq x &lt; u_{2} \nonumber \\
\beta_{01} + \beta_{11}x + (\beta_{12} - \beta_{11})(x - u_{1}) + (\beta_{13} - \beta_{12})(x - u_{2}) &amp; \text{ if } x \geq u_{2}
\end{cases}
\end{equation}\]</span></p></li>
<li><p>We can rewrite the above more compactly as:
<span class="math display">\[\begin{equation}
m(x) = \beta_{01} + \beta_{11}x + (\beta_{12} - \beta_{11})(x - u_{1})_{+} + (\beta_{13} - \beta_{12})(x - u_{2})_{+} \nonumber
\end{equation}\]</span>
where <span class="math inline">\((x - u_{1})_{+} = \max\{ x - u_{1}, 0\}\)</span>.</p></li>
<li><p>So, the functions <span class="math inline">\(\varphi_{0}(x) = 1\)</span>, <span class="math inline">\(\varphi_{1}(x) = x\)</span>, <span class="math inline">\(\varphi_{2}(x) = (x - u_{1})_{+}\)</span>, <span class="math inline">\(\varphi_{3}(x) = (x - u_{2})_{+}\)</span>
form a <strong>basis</strong> for the set of piecewise linear function with continuity constraints and knots <span class="math inline">\(u_{1}\)</span> and <span class="math inline">\(u_{2}\)</span>.</p></li>
</ul>
<hr />
<ul>
<li><p>In general, a <strong>linear spline</strong> with <span class="math inline">\(q\)</span> knots <span class="math inline">\(u_{1} &lt; u_{2} &lt; \ldots &lt; u_{q}\)</span> is a funcion <span class="math inline">\(m(x)\)</span> that can be expressed as
<span class="math display">\[\begin{equation}
m(x) = \beta_{0} + \beta_{1}x + \sum_{k=1}^{q} \beta_{k+1} (x - u_{k})_{+}  \nonumber
\end{equation}\]</span></p></li>
<li><p>So, the following <span class="math inline">\(q + 2\)</span> functions form a basis for the set of linear splines with knots <span class="math inline">\(u_{1} &lt; u_{2} &lt; \ldots &lt; u_{q}\)</span>
<span class="math display">\[\begin{eqnarray}
\varphi_{0}(x) &amp;=&amp; 1  \nonumber \\
\varphi_{1}(x) &amp;=&amp; x  \nonumber \\
\varphi_{2}(x) &amp;=&amp; (x - u_{1})_{+} \nonumber \\
\varphi_{3}(x) &amp;=&amp; (x - u_{2})_{+} \nonumber \\
&amp;\vdots&amp; \nonumber \\
\varphi_{q + 1}(x) &amp;=&amp; (x - u_{q})_{+} \nonumber
\end{eqnarray}\]</span></p></li>
<li><p>Hence, if we want to fit a linear spline with <span class="math inline">\(q\)</span> knots, we will need to estimate <span class="math inline">\(q + 2\)</span> parameters.</p></li>
</ul>
<div class="figure"><span id="fig:unnamed-chunk-3"></span>
<img src="12-splines_files/figure-html/unnamed-chunk-3-1.png" alt="A linear spline with knots at 1/3 and 2/3. A linear spline is a piecewise linear function that is constrained to be continuous." width="672" />
<p class="caption">
Figure 12.4: A linear spline with knots at 1/3 and 2/3. A linear spline is a piecewise linear function that is constrained to be continuous.
</p>
</div>
</div>
<div id="cubic-splines-and-regression-with-splines" class="section level2">
<h2><span class="header-section-number">12.3</span> Cubic Splines and Regression with Splines</h2>
<div id="example-smooth-piecewise-cubic-model-with-2-knots" class="section level3">
<h3><span class="header-section-number">12.3.1</span> Example: Smooth Piecewise Cubic Model with 2 Knots</h3>
<ul>
<li><p>Let us go back to the piecewise cubic model shown in Figure <a href="inference-for-regression.html#fig:pwcubic">12.3</a>.</p></li>
<li><p>This model assumes that the regression function is of the form:
<span class="math display">\[\begin{equation}
m(x) = 
\begin{cases} 
\beta_{01} + \beta_{11}x + \beta_{21}x^{2} + \beta_{31}x^{3} &amp; \text{ if } 0 &lt; x &lt; u_{1} \nonumber \\
\beta_{02} + \beta_{12}x + \beta_{22}x^{2} + \beta_{32}x^{3} &amp; \text{ if } u_{1} \leq x &lt; u_{2} \nonumber \\
\beta_{03} + \beta_{13}x + \beta_{23}x^{2} + \beta_{33}x^{3} &amp; \text{ if } 1 &gt; x \geq u_{2}
\end{cases}
\end{equation}\]</span>
Notice that this model has 12 parameters.</p></li>
<li><p>Like in the linear spline example, if we wanted to make this piecewise cubic model continuous at the knots <span class="math inline">\(u_{1}\)</span> and <span class="math inline">\(u_{2}\)</span> we would
need to impose the following constraints on the coefficients <span class="math inline">\(\beta_{jk}\)</span>:
<span class="math display" id="eq:continuity-constraints">\[\begin{eqnarray}
\beta_{01} + \beta_{11}u_{1} + \beta_{21}u_{1}^{2} + \beta_{31}u_{1}^{3} &amp;=&amp; \beta_{02} + \beta_{12}u_{1} + \beta_{22}u_{1}^{2} + \beta_{32}u_{1}^{3} \nonumber \\ \beta_{02} + \beta_{12}u_{2} + \beta_{22}u_{2}^{2} + \beta_{21}u_{2}^{3} &amp;=&amp; \beta_{03} + \beta_{13}u_{2} + \beta_{23}u_{2}^{2} + \beta_{33}u_{2}^{3}
\tag{12.1}
\end{eqnarray}\]</span></p></li>
</ul>
<hr />
<ul>
<li><p>However, only forcing the piecewise cubic model to be continuous is not enough if we want a smooth estimate for the regression function that will not have
obvious changes at the knots.</p></li>
<li><p>We actually need the first and the second derivatives of the function to be continuous if we want a function
that is smooth and does not have changes at the knots that we can detect visually.</p></li>
<li><p>So, for the example that we have in Figure <a href="inference-for-regression.html#fig:pwcubic">12.3</a> with the knots <span class="math inline">\(u_{1}\)</span> and <span class="math inline">\(u_{2}\)</span>, we need to enforce the additional four constraints:
<span class="math display">\[\begin{eqnarray}
\beta_{11} + 2\beta_{21}u_{1} + 3\beta_{31}u_{1}^{2} &amp;=&amp;  \beta_{12} + 2\beta_{22}u_{1} + 3\beta_{32}u_{1}^{2} \nonumber \\ 
\beta_{12} + 2\beta_{22}u_{2} + 3\beta_{21}u_{2}^{2} &amp;=&amp;  \beta_{13} + 2\beta_{23}u_{2} + 3\beta_{33}u_{2}^{2} \nonumber \\
2\beta_{21} + 6\beta_{31}u_{1} &amp;=&amp;  2\beta_{22} + 6\beta_{32}u_{1} \nonumber \\ 
2\beta_{22} + 6\beta_{21}u_{2} &amp;=&amp; 2\beta_{23} + 6\beta_{33}u_{2} \nonumber
\end{eqnarray}\]</span></p></li>
<li><p>Hence, the piecewise cubic model that has the two continuity constraints <a href="inference-for-regression.html#eq:continuity-constraints">(12.1)</a> and the four first and second derivative constraints
will have <span class="math inline">\(12 - 2 - 4 = 6\)</span> parameters in total.</p></li>
<li><p>In a similar way to how we derived the basis for the linear spline with knots <span class="math inline">\(u_{1}\)</span> and <span class="math inline">\(u_{2}\)</span>, you can show that the following <span class="math inline">\(6\)</span>
functions form a basis for the set of piecewise cubic functions with knots <span class="math inline">\(u_{1}\)</span> and <span class="math inline">\(u_{2}\)</span> that are continuous and have continuous
first and second derivatives:
<span class="math display">\[\begin{eqnarray}
\varphi_{0}(x) &amp;=&amp; 1 \qquad \varphi_{1}(x) = x \qquad \varphi_{2}(x) = x^{2} \qquad \varphi_{3}(x) = x^{3} \nonumber \\
\varphi_{4}(x) &amp;=&amp; (x - u_{1})_{+}^{3} \qquad \varphi_{5}(x) = (x - u_{2})_{+}^{3} \nonumber
\end{eqnarray}\]</span></p></li>
</ul>
</div>
<div id="cubic-splines" class="section level3">
<h3><span class="header-section-number">12.3.2</span> Cubic Splines</h3>
<ul>
<li><p>The above example with knots <span class="math inline">\(u_{1}\)</span> and <span class="math inline">\(u_{2}\)</span> is an example of a <strong>cubic spline</strong>.</p></li>
<li><strong>Definition</strong>: A <strong>cubic spline</strong> with knots <span class="math inline">\(u_{1} &lt; u_{2} &lt; \ldots &lt; u_{q}\)</span> is a function <span class="math inline">\(f(x)\)</span> such that
<ul>
<li><span class="math inline">\(f(x)\)</span> is a cubic function over each of the intervals <span class="math inline">\((-\infty, u_{1}], [u_{1}, u_{2}], \ldots, [u_{q-1}, u_{q}], [u_{q}, \infty)\)</span>.</li>
<li><span class="math inline">\(f(x)\)</span>, <span class="math inline">\(f&#39;(x)\)</span>, and <span class="math inline">\(f&#39;&#39;(x)\)</span> are all continuous functions.</li>
</ul></li>
<li><p>One basis for the set of cubic splines with knots <span class="math inline">\(u_{1} &lt; u_{2} &lt; \ldots &lt; u_{q}\)</span> is the following
<strong>truncated power</strong> basis which consists of <span class="math inline">\(q + 4\)</span> basis functions:
<span class="math display">\[\begin{eqnarray}
\varphi_{0}(x) &amp;=&amp; 1 \quad \varphi_{1}(x) = x \quad \varphi_{2}(x) = x^{2} \quad \varphi_{3}(x) = x^{3} \nonumber \\
\varphi_{k+3}(x) &amp;=&amp; (x - u_{k})_{+}^{3}, \quad k=1,\ldots, q
\end{eqnarray}\]</span></p></li>
</ul>
<hr />
<ul>
<li><p>A common basis for the set of cubic splines with knots <span class="math inline">\(u_{1} &lt; u_{2} &lt; \ldots &lt; u_{q}\)</span> is the <strong>B-spline</strong> basis.</p></li>
<li><p>Using the B-spline basis functions is mathematically equivalent to using the truncated power basis functions. Mathematically,
using either basis would give you the same fitted curve.</p></li>
<li><p>However, there are computational advantages to using the B-spline basis functions, and using the B-spline seems to be
much more common in software implementations of spline fitting procedures.</p></li>
<li><p>For a cubic spline with knots <span class="math inline">\(u_{1}, \ldots, u_{q}\)</span>, we will let the following <span class="math inline">\(q + 4\)</span> denote the corresponding set of
B-spline basis functions
<span class="math display">\[\begin{equation}
\varphi_{1, B}(x), \ldots, \varphi_{q+4, B}(x)  \nonumber
\end{equation}\]</span></p></li>
</ul>
</div>
<div id="estimating-the-coefficients-of-a-cubic-spline" class="section level3">
<h3><span class="header-section-number">12.3.3</span> Estimating the Coefficients of a Cubic Spline</h3>
<ul>
<li><p>When fitting a cubic spline, we assume that the knots <span class="math inline">\(\mathbf{u} = (u_{1}, u_{2}, \ldots, u_{q})\)</span> are fixed first.</p></li>
<li><p>Because the B-spline functions form a basis for the set of cubic splines with these knots, we can
assume that our estimated regression function will have the form
<span class="math display">\[\begin{equation}
m(x) = \sum_{k=1}^{q + 4} \beta_{k}\varphi_{k,B}(x) \nonumber
\end{equation}\]</span></p></li>
<li><p>To find the best coeffients <span class="math inline">\(\beta_{k}\)</span> in this cubic spline model, we will minimize the residual
sum-of-residuals-squared criterion
<span class="math display">\[\begin{equation}
\sum_{i=1}^{n} \Big( Y_{i} -  \hat{m}(x_{i}) \Big)^{2} = \sum_{i=1}^{n} \Big( Y_{i} -  \sum_{k=1}^{q + 4} \beta_{k}\varphi_{k,B}(x_{i}) \Big)^{2}
\end{equation}\]</span></p></li>
</ul>
<hr />
<ul>
<li><p>Finding the coefficients <span class="math inline">\(\hat{\beta}_{1}, \ldots, \hat{\beta}_{q+4}\)</span> that minimize this sum-of-residuals-squared criterion
can be viewed as solving a regression problem with response vector <span class="math inline">\(\mathbf{Y} = (Y_{1}, \ldots, Y_{n})\)</span> and
âdesign matrixâ <span class="math inline">\(\mathbf{X}_{\mathbf{u}}\)</span>
<span class="math display">\[\begin{equation}
\mathbf{X}_{\mathbf{u}} = \begin{bmatrix} \varphi_{1, B}(x_{1}) &amp; \varphi_{2, B}(x_{1}) &amp; \ldots &amp; \varphi_{q+4, B}(x_{1}) \\ \varphi_{1, B}(x_{2}) &amp; \varphi_{2, B}(x_{2}) &amp; \ldots &amp; \varphi_{q+4,B}(x_{2}) \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \varphi_{1,B}(x_{n}) &amp; \varphi_{2,B}(x_{n}) &amp; \ldots &amp; \varphi_{q+4,B}(x_{n}) \end{bmatrix} \nonumber
\end{equation}\]</span></p></li>
<li><p>When written in this form, the vector of estimated regression coeffients can be expressed as
<span class="math display">\[\begin{equation}
\begin{bmatrix} \hat{\beta}_{1} \\ \hat{\beta}_{2} \\ \vdots \\ \hat{\beta}_{q + 4} \end{bmatrix} = (\mathbf{X}_{\mathbf{u}}^{T}\mathbf{X}_{\mathbf{u}})^{-1}\mathbf{X}_{\mathbf{u}}^{T}\mathbf{Y} \nonumber
\end{equation}\]</span>
and hence the vector of fitted values <span class="math inline">\(\hat{\mathbf{m}} = \big( \hat{m}(x_{1}), \ldots, \hat{m}(x_{n}) \big)\)</span> can be
written as
<span class="math display">\[\begin{equation}
\hat{\mathbf{m}} = \mathbf{X}_{\mathbf{u}}(\mathbf{X}_{\mathbf{u}}^{T}\mathbf{X}_{\mathbf{u}})^{-1}\mathbf{X}_{\mathbf{u}}^{T}\mathbf{Y} \nonumber
\end{equation}\]</span></p></li>
</ul>
</div>
<div id="an-example-in-r-1" class="section level3">
<h3><span class="header-section-number">12.3.4</span> An example in R</h3>
<ul>
<li>Regression splines can be fitted in R by using the <code>splines</code> package</li>
</ul>
<div class="sourceCode" id="cb246"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb246-1" data-line-number="1"><span class="kw">library</span>(splines)</a></code></pre></div>
<ul>
<li>The <code>bs</code> function from the <code>splines</code> package is useful for fitting a linear or cubic spline.
This function generates the B-spline âdesignâ matrix <span class="math inline">\(\mathbf{X}_{\mathbf{u}}\)</span> described above.</li>
</ul>
<div class="sourceCode" id="cb247"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb247-1" data-line-number="1"><span class="kw">bs</span>(x, df, knots, degree)</a></code></pre></div>
<ul>
<li><code>x</code> - vector of covariates values. This can also just be the name of a variable when <code>bs</code> is used inside the <code>lm</code> function.</li>
<li><code>df</code> - the âdegrees of freedomâ. For a cubic spline this is actually <span class="math inline">\(q + 3\)</span> rather than <span class="math inline">\(q + 4\)</span>. If you just enter <code>df</code>, the <code>bs</code> function will pick the knots for you.</li>
<li><code>knots</code> - the vector of knots. If you donât want to pick the knots, you can just enter a number for the <code>df</code>.</li>
<li><code>degree</code> - the degree of the piecewise polynomial. degree=1 for a linear spline and degree=3 for a cubic spline.</li>
</ul>
<hr />
<ul>
<li><p>As an example of what the <code>bs</code> function returns, suppose we input the vector of covariates <span class="math inline">\((x_{1}, \ldots, x_{10}) = (1, 2, \ldots, 10)\)</span> with knots <span class="math inline">\(u_{1} = 3.5\)</span> and <span class="math inline">\(u_{2} = 6.5\)</span>.</p></li>
<li><p>The <code>bs</code> function will return the âdesign matrixâ <span class="math inline">\(\mathbf{X}_{\mathbf{u}}\)</span> for this setup without the intercept column. When degree is 3, the dimensions of <span class="math inline">\(\mathbf{X}_{\mathbf{u}}\)</span> should be <span class="math inline">\(10 \times 6\)</span> (because in this case <span class="math inline">\(q = 2\)</span>). So, the <code>bs</code> function will return a <span class="math inline">\(10 \times 5\)</span> matrix (because the first column of <span class="math inline">\(\mathbf{X}_{u}\)</span> is dropped by <code>bs</code>):</p></li>
</ul>
<div class="sourceCode" id="cb248"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb248-1" data-line-number="1">xx &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">10</span></a>
<a class="sourceLine" id="cb248-2" data-line-number="2">Xu &lt;-<span class="st"> </span><span class="kw">bs</span>(xx, <span class="dt">knots=</span><span class="kw">c</span>(<span class="fl">3.5</span>, <span class="fl">6.5</span>))</a>
<a class="sourceLine" id="cb248-3" data-line-number="3"><span class="kw">dim</span>(Xu)</a></code></pre></div>
<pre><code>## [1] 10  5</code></pre>
<div class="sourceCode" id="cb250"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb250-1" data-line-number="1"><span class="kw">head</span>(Xu)  </a></code></pre></div>
<pre><code>##            1     2       3        4 5
## [1,] 0.00000 0.000 0.00000 0.000000 0
## [2,] 0.60813 0.168 0.00808 0.000000 0
## [3,] 0.45779 0.470 0.06465 0.000000 0
## [4,] 0.17218 0.612 0.21463 0.000986 0
## [5,] 0.03719 0.515 0.42131 0.026627 0
## [6,] 0.00138 0.309 0.56631 0.123274 0</code></pre>
<hr />
<ul>
<li><p>Letâs try an example with a linear spline to see how to use the <code>bs</code> function to fit a spline within the <code>lm</code> function.</p></li>
<li><p>We will use the <code>bone</code> data again with age as the covariate. We will use the knots <span class="math inline">\(\mathbf{u} = (12, 15, 18, 21, 24)\)</span>.</p></li>
</ul>
<div class="sourceCode" id="cb252"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb252-1" data-line-number="1">bonedat &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;~/Documents/STAT685Notes/Data/bone.csv&quot;</span>)</a>
<a class="sourceLine" id="cb252-2" data-line-number="2">knot.seq &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">12</span>, <span class="dv">15</span>, <span class="dv">18</span>, <span class="dv">21</span>, <span class="dv">24</span>)</a>
<a class="sourceLine" id="cb252-3" data-line-number="3">linspline.bone &lt;-<span class="st"> </span><span class="kw">lm</span>(spnbmd <span class="op">~</span><span class="st"> </span><span class="kw">bs</span>(age, <span class="dt">knots=</span>knot.seq, <span class="dt">degree=</span><span class="dv">1</span>), <span class="dt">data=</span>bonedat)</a></code></pre></div>
<ul>
<li><p>Because we are using <span class="math inline">\(5\)</span> knots, based on our discussion in Section 12.2 we should expect that there should be <span class="math inline">\(7\)</span> columns in
the design matrix <span class="math inline">\(\mathbf{X}_{u}\)</span> for this linear spline model.</p></li>
<li><p>You can check this by using the following <code>R</code> code:</p></li>
</ul>
<div class="sourceCode" id="cb253"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb253-1" data-line-number="1">XX &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(linspline.bone)</a>
<a class="sourceLine" id="cb253-2" data-line-number="2"><span class="kw">dim</span>(XX)</a></code></pre></div>
<pre><code>## [1] 261   7</code></pre>
<ul>
<li>If you want to compute the estimated spline function <span class="math inline">\(\hat{m}(t_{j})\)</span> at a sequence of points <span class="math inline">\(t_{1}, \ldots, t_{l}\)</span>,
you can use the <code>predict</code> function on the fitted <code>lm</code> object. This is done with the following code
for the points <span class="math inline">\(9.5, 10, 10.5, 11, \ldots, 24.5, 25\)</span>.</li>
</ul>
<div class="sourceCode" id="cb255"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb255-1" data-line-number="1">tt &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="fl">9.5</span>, <span class="dv">25</span>, <span class="dt">by=</span><span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb255-2" data-line-number="2"><span class="kw">plot</span>(bonedat<span class="op">$</span>age, bonedat<span class="op">$</span>spnbmd, <span class="dt">xlab=</span><span class="st">&quot;age&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Relative Change in Bone MD&quot;</span>, </a>
<a class="sourceLine" id="cb255-3" data-line-number="3">     <span class="dt">main=</span><span class="st">&quot;Bone Data: Fitted Linear Spline&quot;</span>, <span class="dt">las=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb255-4" data-line-number="4"><span class="kw">lines</span>(tt, <span class="kw">predict</span>(linspline.bone, <span class="kw">data.frame</span>(<span class="dt">age=</span>tt)), <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb255-5" data-line-number="5"><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>) {</a>
<a class="sourceLine" id="cb255-6" data-line-number="6">    <span class="co">## Plot vertical lines at the knots</span></a>
<a class="sourceLine" id="cb255-7" data-line-number="7">    <span class="kw">abline</span>(<span class="dt">v=</span>knot.seq[k], <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb255-8" data-line-number="8">}</a></code></pre></div>
<p><img src="12-splines_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<hr />
<ul>
<li><p>Now letâs try fitting a cubic spline model to the bone data.</p></li>
<li><p>The procedure is almost exactly the same as fitting the linear spline model. We only need to change <code>degree=1</code> to <code>degree=3</code> in the <code>bs</code> function.</p></li>
<li><p>We will use the same knots as we did for the linear spline.</p></li>
</ul>
<div class="sourceCode" id="cb256"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb256-1" data-line-number="1">knot.seq &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">12</span>, <span class="dv">15</span>, <span class="dv">18</span>, <span class="dv">21</span>, <span class="dv">24</span>)</a>
<a class="sourceLine" id="cb256-2" data-line-number="2">cubspline.bone &lt;-<span class="st"> </span><span class="kw">lm</span>(spnbmd <span class="op">~</span><span class="st"> </span><span class="kw">bs</span>(age, <span class="dt">knots=</span>knot.seq, <span class="dt">degree=</span><span class="dv">3</span>), <span class="dt">data=</span>bonedat)</a></code></pre></div>
<ul>
<li><p>Because we are using <span class="math inline">\(5\)</span> knots, we should expect that there will be <span class="math inline">\(5 + 4 = 9\)</span> columns in
the design matrix <span class="math inline">\(\mathbf{X}_{u}\)</span> for this cubic spline model.</p></li>
<li><p>You can check this by using the following <code>R</code> code:</p></li>
</ul>
<div class="sourceCode" id="cb257"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb257-1" data-line-number="1">XX &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(cubspline.bone)</a>
<a class="sourceLine" id="cb257-2" data-line-number="2"><span class="kw">dim</span>(XX)</a></code></pre></div>
<pre><code>## [1] 261   9</code></pre>
<ul>
<li>Using <code>predict</code> again, we will compute the estimated regression function <span class="math inline">\(\hat{m}(x)\)</span> at the points: <span class="math inline">\(9.5, 9.6, \ldots, 24.9, 25\)</span> and plot the result.</li>
</ul>
<div class="sourceCode" id="cb259"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb259-1" data-line-number="1">tt &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="fl">9.5</span>, <span class="dv">25</span>, <span class="dt">by=</span><span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb259-2" data-line-number="2"><span class="kw">plot</span>(bonedat<span class="op">$</span>age, bonedat<span class="op">$</span>spnbmd, <span class="dt">xlab=</span><span class="st">&quot;age&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Relative Change in Bone MD&quot;</span>, </a>
<a class="sourceLine" id="cb259-3" data-line-number="3">     <span class="dt">main=</span><span class="st">&quot;Bone Data: Fitted Cubic Spline&quot;</span>, <span class="dt">las=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb259-4" data-line-number="4"><span class="kw">lines</span>(tt, <span class="kw">predict</span>(cubspline.bone, <span class="kw">data.frame</span>(<span class="dt">age=</span>tt)), <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb259-5" data-line-number="5"><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>) {</a>
<a class="sourceLine" id="cb259-6" data-line-number="6">    <span class="co">## Plot vertical lines at the knots</span></a>
<a class="sourceLine" id="cb259-7" data-line-number="7">    <span class="kw">abline</span>(<span class="dt">v=</span>knot.seq[k], <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb259-8" data-line-number="8">}</a></code></pre></div>
<p><img src="12-splines_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
</div>
<div id="natural-cubic-splines" class="section level3">
<h3><span class="header-section-number">12.3.5</span> Natural Cubic Splines</h3>
<ul>
<li><p>Cubic splines can often have highly variable behavior near the edges of the data (i.e., for points near the smallest and largest <span class="math inline">\(x_{i}\)</span>).</p></li>
<li><p>One approach for addressing this problem is to use a spline which is linear for both <span class="math inline">\(x &lt; u_{1}\)</span> and <span class="math inline">\(x &gt; u_{q}\)</span>.</p></li>
<li><p>A <strong>natural cubic spline</strong> is a function that is still a piecewise cubic function over each of the intervals <span class="math inline">\([u_{j}, u_{j+1}]\)</span> for <span class="math inline">\(j=1, \ldots, q-1\)</span>,
but is linear for <span class="math inline">\(x &lt; u_{1}\)</span> and <span class="math inline">\(x &gt; u_{q}\)</span>. A natural cubic spline is still assumed to satisfy all of the
continuity and derivative continuity conditions of the usual cubic spline.</p></li>
<li><p>Natural cubic splines are also useful for fitting smoothing splines.</p></li>
</ul>
<hr />
<ul>
<li><p>Because we are using linear rather than cubic functions in the regions <span class="math inline">\((-\infty, u_{1})\)</span> and <span class="math inline">\((u_{q}, \infty)\)</span> this should reduce
the number of necessary basis functions by <span class="math inline">\(4\)</span> (so we would only need <span class="math inline">\(q\)</span> rather <span class="math inline">\(q + 4\)</span> basis functions).</p></li>
<li><p>Indeed, one basis for the set of natural cubic splines with knots <span class="math inline">\(u_{1}, \ldots, u_{q}\)</span> is the following collection
of <span class="math inline">\(q\)</span> basis functions:
<span class="math display">\[\begin{equation}
N_{1}(x) = 1 \qquad N_{2}(x) = x \qquad N_{k+2} = d_{k}(x) - d_{q-1}(x), k = 1, \ldots, q-2,  \nonumber
\end{equation}\]</span>
where the functions <span class="math inline">\(d_{k}(x)\)</span> are defined as
<span class="math display">\[\begin{equation}
d_{k}(x) = \frac{ (x - u_{k})_{+}^{3} - (x - u_{q})_{+}^{3} }{ u_{q} - u_{k}} \nonumber
\end{equation}\]</span></p></li>
</ul>
</div>
</div>
<div id="smoothing-splines" class="section level2">
<h2><span class="header-section-number">12.4</span> Smoothing Splines</h2>
<ul>
<li><p>When we used cubic splines for regression, we placed knots at a number of fixed points
in between the smallest and largest <span class="math inline">\(x_{i}\)</span> values.</p></li>
<li><p>With what are referred to as âsmoothing splinesâ every point <span class="math inline">\(x_{i}\)</span> is treated as a
potential knot, and the overall smoothness of the regression function estimate
is determined through penalizing the roughness of the function.</p></li>
</ul>
<hr />
<ul>
<li>Motivation for smoothing splines can come from considering the following minimization problem:
<ul>
<li>For <span class="math inline">\(\lambda \geq 0\)</span>, suppose we want to find the function <span class="math inline">\(m(x)\)</span> which solves the optimization problem:
<span class="math display" id="eq:spline-smooth-motivation">\[\begin{equation}
\textrm{minimize:} \quad \sum_{i=1}^{n} \{ Y_{i} - m( x_{i} ) \}^{2} + \lambda \int \{ m&#39;&#39;(x) \}^{2} dx
\tag{12.2}
\end{equation}\]</span>
subject to the constraint that <span class="math inline">\(m&#39;(x)\)</span> and <span class="math inline">\(m&#39;&#39;(x)\)</span> are both continuous.</li>
</ul></li>
<li><p>The second derivative <span class="math inline">\(m&#39;&#39;(x)\)</span> represents the curvature of <span class="math inline">\(m\)</span> at <span class="math inline">\(x\)</span>. So, the penalty <span class="math inline">\(\int \{ m&#39;&#39;(x) \}^{2} dx\)</span>
is like an average squared curvature of the function <span class="math inline">\(m(x)\)</span>.</p></li>
<li><p>If <span class="math inline">\(\lambda = 0\)</span>, then the function <span class="math inline">\(m(x)\)</span> which minimizes <a href="inference-for-regression.html#eq:spline-smooth-motivation">(12.2)</a> is any function
such that <span class="math inline">\(m(x_{i}) = Y_{i}\)</span> for each <span class="math inline">\(i\)</span>. This will usually be an extremely âwigglyâ function.</p></li>
<li><p>If <span class="math inline">\(\lambda = \infty\)</span>, then we want a function <span class="math inline">\(m(x)\)</span> such that <span class="math inline">\(m&#39;&#39;(x) = 0\)</span> for all <span class="math inline">\(x\)</span>. The best function
in this case would be a linear function <span class="math inline">\(m(x) = \beta_{0} + \beta_{1}x\)</span>.</p></li>
<li><p>By choosing <span class="math inline">\(0 &lt; \lambda &lt; 0\)</span>, we will get a function that can be nonlinear and capture
some curvature but cannot be extremely âwigglyâ.</p></li>
</ul>
<hr />
<ul>
<li><p>It is possible to show that the function <span class="math inline">\(\hat{m}_{\lambda}(x)\)</span> which minimizes <a href="inference-for-regression.html#eq:spline-smooth-motivation">(12.2)</a> is
a <strong>natural cubic spline</strong> with <span class="math inline">\(n\)</span> knots at the covariate values <span class="math inline">\((x_{1}, \ldots, x_{n})\)</span>.</p></li>
<li><p>So, we can restrict our search to functions which can be written as
<span class="math display">\[\begin{equation}
m(x) = \sum_{j=1}^{n} \beta_{j}N_{j}(x), \nonumber
\end{equation}\]</span>
where <span class="math inline">\(N_{1}(x), \ldots, N_{n}(x)\)</span> is the set of basis functions for the set of natural cubic
splines with these knots.</p></li>
</ul>
<hr />
<ul>
<li><p>So, assuming that <span class="math inline">\(m(x) = \sum_{j=1}^{n} \beta_{j}N_{j}(x)\)</span>, we can re-write the minimization problem as
<span class="math display">\[\begin{equation}
\textrm{minimize:}_{\beta_{1}, \ldots, \beta_{n}} \sum_{i = 1}^{n} \{ Y_{i} - \sum_{j=1}^{n} \beta_{j}N_{j}(x_{i}) \}^{2} + \lambda \sum_{j=1}^{n} \sum_{k=1}^{n} \beta_{j}\beta_{k} \int N_{j}&#39;&#39;(x) N_{k}&#39;&#39;(x) dx \nonumber
\end{equation}\]</span></p></li>
<li><p>In matrix-vector notation, this minimization problem can be written as
<span class="math display" id="eq:spline-smooth-minimization">\[\begin{equation}
\textrm{minimize:}_{\boldsymbol\beta} \quad (\mathbf{Y} - \mathbf{N}\boldsymbol\beta)^{T}(\mathbf{Y} - \mathbf{N}\boldsymbol\beta) + \lambda \boldsymbol\beta^{T}\boldsymbol\Omega\boldsymbol\beta,
\tag{12.3}
\end{equation}\]</span>
where <span class="math inline">\(\mathbf{N}\)</span> is the <span class="math inline">\(n \times n\)</span> matrix whose <span class="math inline">\((j,k)\)</span> element is <span class="math inline">\(N_{k}(x_{j})\)</span> and <span class="math inline">\(\boldsymbol\Omega\)</span> is the <span class="math inline">\(n \times n\)</span> matrix whose <span class="math inline">\((j,k)\)</span> element is <span class="math inline">\(\Omega_{jk} = \int N_{j}&#39;&#39;(x)N_{k}&#39;&#39;(x) dx\)</span>.</p></li>
<li><p>The vector of coefficients which solves the minimization problem <a href="inference-for-regression.html#eq:spline-smooth-minimization">(12.3)</a> is given by
<span class="math display">\[\begin{equation}
\hat{\boldsymbol\beta} = \begin{bmatrix} \hat{\beta}_{1} \\ \vdots \\ \hat{\beta}_{n} \end{bmatrix} = (\mathbf{N}^{T}\mathbf{N} + \lambda \boldsymbol\Omega)^{-1}\mathbf{N}^{T}\mathbf{Y} \nonumber
\end{equation}\]</span></p></li>
</ul>
</div>
<div id="knotpenalty-term-selection-for-splines" class="section level2">
<h2><span class="header-section-number">12.5</span> Knot/Penalty Term Selection for Splines</h2>
<ul>
<li><p>For both regression splines and smoothing splines, we can write the vector of fitted values <span class="math inline">\(\hat{\mathbf{m}} = \big( \hat{m}(x_{1}), \ldots, \hat{m}(x_{n}) \big)\)</span> as
<span class="math display">\[\begin{equation}
\hat{\mathbf{m}} = \mathbf{A}\mathbf{Y}, \nonumber
\end{equation}\]</span>
for an appropriately chosen <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span>.</p></li>
<li><p>For the case of a cubic regression spline with fixed knot sequence <span class="math inline">\(\mathbf{u} = (u_{1}, \ldots, u_{q})\)</span>, we have that
<span class="math inline">\(\hat{\mathbf{m}} = \mathbf{A}_{\mathbf{u}}\mathbf{Y}\)</span> where
<span class="math display">\[\begin{equation}
\mathbf{A}_{\mathbf{u}} = \mathbf{X}_{\mathbf{u}}(\mathbf{X}_{\mathbf{u}}^{T}\mathbf{X}_{\mathbf{u}})^{-1}\mathbf{X}_{\mathbf{u}}^{T}
\end{equation}\]</span>
Note that, in this case,
<span class="math display">\[\begin{equation}
\textrm{tr}( \mathbf{A}_{\mathbf{u}} ) = \textrm{tr}( \mathbf{X}_{\mathbf{u}}(\mathbf{X}_{\mathbf{u}}^{T}\mathbf{X}_{\mathbf{u}})^{-1}\mathbf{X}_{\mathbf{u}}^{T} ) = \textrm{tr}( (\mathbf{X}_{\mathbf{u}}^{T}\mathbf{X}_{\mathbf{u}})^{-1}\mathbf{X}_{\mathbf{u}}^{T}\mathbf{X}_{u} ) = q + 4
\end{equation}\]</span></p></li>
<li><p>For the case of a smoothing spline with penalty term <span class="math inline">\(\lambda &gt; 0\)</span>, we have that <span class="math inline">\(\hat{\mathbf{m}} = \mathbf{A}_{\lambda}\mathbf{Y}\)</span> where
<span class="math display">\[\begin{equation}
\mathbf{A}_{\lambda} = \mathbf{N}(\mathbf{N}^{T}\mathbf{N} + \lambda\boldsymbol\Omega)^{-1}\mathbf{N}^{T} \nonumber
\end{equation}\]</span></p></li>
</ul>
<div id="the-cp-statistic-1" class="section level3">
<h3><span class="header-section-number">12.5.1</span> The Cp Statistic</h3>
<ul>
<li><p>As with kernel and local regression method described in Chapter 11, the <span class="math inline">\(C_{p}\)</span> statistic is defined as
the mean residual sum of squares plus a penalty which depends on the matrix <span class="math inline">\(\mathbf{A}\)</span>.</p></li>
<li><p>In the context of regression splines where <span class="math inline">\(\hat{\mathbf{m}} = \mathbf{A}_{\mathbf{u}}\mathbf{Y}\)</span>, the <span class="math inline">\(C_{p}\)</span> statistic can be written as:
<span class="math display">\[\begin{eqnarray}
C_{p}(q) &amp;=&amp; \frac{1}{n}\sum_{i=1}^{n}\{ Y_{i} - \hat{m}(x_{i}) \}^{2}  + \frac{2\hat{\sigma}^{2}}{n}\textrm{tr}(\mathbf{A}_{u})  \nonumber \\
&amp;=&amp; \frac{1}{n}\sum_{i=1}^{n}\{ Y_{i} - \hat{m}(x_{i}) \}^{2} + \frac{2\hat{\sigma}^{2}(q + 4)}{n} \nonumber
\end{eqnarray}\]</span></p></li>
<li><p>In the context of smoothing splines where <span class="math inline">\(\hat{\mathbf{m}} = \mathbf{A}_{\lambda}\mathbf{Y}\)</span>, the <span class="math inline">\(C_{p}\)</span> statistic can be written as
<span class="math display">\[\begin{eqnarray}
C_{p}(\lambda) &amp;=&amp; \frac{1}{n}\sum_{i=1}^{n}\{ Y_{i} - \hat{m}(x_{i}) \}^{2}  + \frac{2\hat{\sigma}^{2}}{n}\textrm{tr}(\mathbf{A}_{\lambda})  \nonumber \\
&amp;=&amp; \frac{1}{n}\sum_{i=1}^{n}\{ Y_{i} - \hat{m}(x_{i}) \}^{2} + \frac{2\hat{\sigma}^{2}}{n}\textrm{tr}\Big( (\mathbf{N}^{T}\mathbf{N} + \lambda\boldsymbol\Omega)^{-1}\mathbf{N}^{T}\mathbf{N} \Big) \nonumber
\end{eqnarray}\]</span></p></li>
</ul>
</div>
<div id="leave-one-out-cross-validation-1" class="section level3">
<h3><span class="header-section-number">12.5.2</span> Leave-one-out Cross-Validation</h3>
<ul>
<li><p>As mentioned in Chapter 11, the leave-one-out cross-validation can be expressed as a weighted sum-of-squared residuals with
the weights coming from the diagonals of the âsmoothingâ matrix <span class="math inline">\(\mathbf{A}\)</span>.</p></li>
<li><p>For the smoothing spline, the leave-one-out cross-validation criterion is
<span class="math display">\[\begin{equation}
\textrm{LOOCV}(\lambda) = \frac{1}{n}\sum_{i=1}^{n} \Big( \frac{ Y_{i} - \hat{m}_{\lambda}(x_{i})}{ 1 - a_{i}^{\lambda}( x_{i} )  } \Big)^{2}  \nonumber
\end{equation}\]</span>
where <span class="math inline">\(a_{i}^{\lambda}(x_{i})\)</span> denotes the <span class="math inline">\(i^{th}\)</span> diagonal of the matrix <span class="math inline">\(\mathbf{A}_{\lambda}\)</span>.</p></li>
</ul>
</div>
<div id="generalized-cross-validation" class="section level3">
<h3><span class="header-section-number">12.5.3</span> Generalized Cross-Validation</h3>
<ul>
<li><p>A criterion which we did not mention in Chapter 11 is Generalized Cross-Validation (GCV).</p></li>
<li><p>For the smoothing parameter <span class="math inline">\(\lambda\)</span>, the GCV criterion is defined as
<span class="math display">\[\begin{equation}
\textrm{GCV}(\lambda) = \frac{1}{n}\sum_{i=1}^{n} \Big( \frac{ Y_{i} - \hat{m}_{\lambda}(x_{i})}{ 1 - \textrm{tr}(\mathbf{A}_{\lambda})/n  } \Big)^{2} = \Big(\frac{n}{n - \textrm{tr}(\mathbf{A}_{\lambda})} \Big)^{2} \frac{1}{n} \sum_{i=1}^{n} \{Y_{i} - \hat{m}_{\lambda}(x_{i}) \}^{2} \nonumber
\end{equation}\]</span></p></li>
<li><p>The GCV criterion can be seen as replacing the individual diagonal elements in the <span class="math inline">\(\textrm{LOOCV}\)</span> criterion with their average value <span class="math inline">\(\textrm{tr}(\mathbf{A}_{\lambda})/n\)</span>.</p></li>
<li><p>GCV can often perform better when some of the diagonal elements of <span class="math inline">\(\mathbf{A}_{\lambda}\)</span> are close to <span class="math inline">\(1\)</span>.</p></li>
</ul>
</div>
</div>
<div id="fitting-smoothing-splines-in-r" class="section level2">
<h2><span class="header-section-number">12.6</span> Fitting Smoothing Splines in R</h2>
<ul>
<li>The <code>R</code> function <code>smooth.spline</code> will fit smoothing splines.</li>
</ul>
<div class="sourceCode" id="cb260"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb260-1" data-line-number="1"><span class="kw">smooth.spline</span>(x, y, df, lambda, <span class="dt">cv=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<ul>
<li><strong>x</strong> - the vector of covariate values.</li>
<li><strong>y</strong> - the vector of responses.</li>
<li><strong>df</strong> - the trace of the âsmootherâ matrix. This is the trace of the matrix <span class="math inline">\(\mathbf{A}_{\lambda} = \mathbf{N}(\mathbf{N}^{T}\mathbf{N} + \lambda\boldsymbol\Omega)^{-1}\mathbf{N}^{T}\mathbf{Y}\)</span>. This must be less than or equal to <span class="math inline">\(n\)</span> (where <span class="math inline">\(n\)</span> here is the number of unique values of the <span class="math inline">\(x_{i}\)</span>).</li>
<li><strong>lambda</strong> - the value of <span class="math inline">\(\lambda\)</span> in the matrix <span class="math inline">\(\mathbf{A}_{\lambda} = \mathbf{N}(\mathbf{N}^{T}\mathbf{N} + \lambda\boldsymbol\Omega)^{-1}\mathbf{N}^{T}\mathbf{Y}\)</span>. Note that you should only enter one of the <strong>df</strong> or <strong>lambda</strong> arguments.</li>
<li><p><strong>cv</strong> - the smooth.spline function will perform cross-validation whenever both the <strong>df</strong> and <strong>lambda</strong> arguments are left empty. When both of these arguments are left empty and <code>cv=FALSE</code>, the function will use GCV to select the smoothing parameter. When both <strong>df</strong> and <strong>lambda</strong> are left empty and <code>cv = TRUE</code>, the function will use LOOCV to select the smoothing parameter.</p></li>
<li><p>Notice that when only input the <code>x</code> and <code>y</code> vectors, the <code>smooth.spline</code> will automatically use generalized cross-validation to select the smoothing parameter.</p></li>
</ul>
<hr />
<ul>
<li><p>To see how to use the <code>smooth.spline</code> function and how to use the <span class="math inline">\(C_{p}\)</span> statistic, LOOCV, or GCV for selecting the smoothing parameter in smoothing splines or the number of knots in regression splines, we will again consider the <code>bone</code> data.</p></li>
<li><p>To start, letâs use the <code>smooth.spline</code> function on the bone data using GCV to find the smoothing parameter:</p></li>
</ul>
<div class="sourceCode" id="cb261"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb261-1" data-line-number="1">ss.bone &lt;-<span class="st"> </span><span class="kw">smooth.spline</span>(<span class="dt">x=</span>bonedat<span class="op">$</span>age, <span class="dt">y=</span>bonedat<span class="op">$</span>spnbmd)</a>
<a class="sourceLine" id="cb261-2" data-line-number="2"></a>
<a class="sourceLine" id="cb261-3" data-line-number="3"><span class="kw">plot</span>(bonedat<span class="op">$</span>age, bonedat<span class="op">$</span>spnbmd, <span class="dt">las=</span><span class="dv">1</span>, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">xlab=</span><span class="st">&quot;age&quot;</span>, </a>
<a class="sourceLine" id="cb261-4" data-line-number="4">     <span class="dt">ylab=</span><span class="st">&quot;Relative Change in Bone MD&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Bone Data: Smoothing Spline using GCV&quot;</span>)</a>
<a class="sourceLine" id="cb261-5" data-line-number="5"><span class="kw">lines</span>(ss.bone<span class="op">$</span>x, ss.bone<span class="op">$</span>y, <span class="dt">lwd=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:unnamed-chunk-15"></span>
<img src="12-splines_files/figure-html/unnamed-chunk-15-1.png" alt="Smoothing spline fit for the bone data. This used the smooth.spline function with all the default settings." width="672" />
<p class="caption">
Figure 12.5: Smoothing spline fit for the bone data. This used the smooth.spline function with all the default settings.
</p>
</div>
<ul>
<li>If you just type in <code>ss.smooth</code>, this will show the degrees of freedom used, the value of <span class="math inline">\(\lambda\)</span> used, and the value of the GCV criterion for the chosen <span class="math inline">\(\lambda\)</span></li>
</ul>
<div class="sourceCode" id="cb262"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb262-1" data-line-number="1">ss.bone</a></code></pre></div>
<pre><code>## Call:
## smooth.spline(x = bonedat$age, y = bonedat$spnbmd)
## 
## Smoothing Parameter  spar= 0.712  lambda= 0.000239 (15 iterations)
## Equivalent Degrees of Freedom (Df): 12.2
## Penalized Criterion (RSS): 0.222
## GCV: 0.00152</code></pre>
<ul>
<li><p>According to the GCV criterion, the best value for <span class="math inline">\(\lambda\)</span> is about <span class="math inline">\(\lambda^{*} \approx 0.0002\)</span> and the corresponding value for the degrees of freedom is <span class="math inline">\(df^{*} = \textrm{tr}(\mathbf{A}_{\lambda^{*}}) \approx 12.15\)</span>.</p></li>
<li><p>Using the LOOCV criterion, the best value for the degrees of freedom is <span class="math inline">\(13\)</span>.</p></li>
</ul>
<div class="sourceCode" id="cb264"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb264-1" data-line-number="1">ss.bone2 &lt;-<span class="st"> </span><span class="kw">smooth.spline</span>(<span class="dt">x=</span>bonedat<span class="op">$</span>age, <span class="dt">y=</span>bonedat<span class="op">$</span>spnbmd, <span class="dt">cv=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## Warning in smooth.spline(x = bonedat$age, y = bonedat$spnbmd, cv = TRUE): cross-
## validation with non-unique &#39;x&#39; values seems doubtful</code></pre>
<div class="sourceCode" id="cb266"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb266-1" data-line-number="1">ss.bone2</a></code></pre></div>
<pre><code>## Call:
## smooth.spline(x = bonedat$age, y = bonedat$spnbmd, cv = TRUE)
## 
## Smoothing Parameter  spar= 0.694  lambda= 0.000177 (13 iterations)
## Equivalent Degrees of Freedom (Df): 13
## Penalized Criterion (RSS): 0.219
## PRESS(l.o.o. CV): 0.00149</code></pre>
<div id="smoothing-parameter-selection-for-the-smoothing-spline-with-the-cp-statistic" class="section level3">
<h3><span class="header-section-number">12.6.1</span> Smoothing Parameter Selection for the Smoothing Spline with the Cp statistic</h3>
<ul>
<li><p>Suppose we wanted to find the best value of <span class="math inline">\(\textrm{tr}( \mathbf{A}_{\lambda})\)</span> of the smoothing spline using the <span class="math inline">\(C_{p}\)</span> statistic.</p></li>
<li><p>The first thing we want to find is an estimate of <span class="math inline">\(\sigma^{2}\)</span> that we can keep fixed across different values of the smoothing
parameter. Letâs use the same value of <span class="math inline">\(\hat{\sigma}^{2} = 0.0015\)</span> that we used in Chapter 11:</p></li>
</ul>
<div class="sourceCode" id="cb268"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb268-1" data-line-number="1">sigsq.est &lt;-<span class="st"> </span><span class="fl">0.0015</span></a></code></pre></div>
<ul>
<li>Now, letâs write a function that computes the <span class="math inline">\(C_{p}\)</span> statistic for a smoothing spline model given that we input the data, the degrees of freedom <span class="math inline">\(\textrm{tr}(\mathbf{A}_{\lambda})\)</span>, and <span class="math inline">\(\hat{\sigma}^{2}\)</span>.</li>
</ul>
<div class="sourceCode" id="cb269"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb269-1" data-line-number="1">CpStatSmoothSpline &lt;-<span class="st"> </span><span class="cf">function</span>(x, y, df, sigsq.hat) {</a>
<a class="sourceLine" id="cb269-2" data-line-number="2">  n &lt;-<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb269-3" data-line-number="3">  ss.obj &lt;-<span class="st"> </span><span class="kw">smooth.spline</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>y, <span class="dt">df=</span>df)</a>
<a class="sourceLine" id="cb269-4" data-line-number="4">  <span class="co">## Now, compute the vector of residuals from this fitted smoothing spline</span></a>
<a class="sourceLine" id="cb269-5" data-line-number="5">  residu &lt;-<span class="st"> </span>y <span class="op">-</span><span class="st"> </span><span class="kw">fitted</span>(ss.obj)</a>
<a class="sourceLine" id="cb269-6" data-line-number="6">  ans &lt;-<span class="st"> </span><span class="kw">mean</span>(residu<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span>(<span class="dv">2</span><span class="op">*</span>sigsq.hat<span class="op">/</span>n)<span class="op">*</span>df</a>
<a class="sourceLine" id="cb269-7" data-line-number="7">  <span class="kw">return</span>(ans)</a>
<a class="sourceLine" id="cb269-8" data-line-number="8">}</a></code></pre></div>
<ul>
<li>Now, compute the <span class="math inline">\(C_{p}\)</span> statistic for values of the degrees of freedom between <span class="math inline">\(4\)</span> and <span class="math inline">\(20\)</span>. From the plot of the <span class="math inline">\(C_{p}\)</span> vs.Â the degrees of freedom it looks like the best value for the degrees of freedom is about <span class="math inline">\(12\)</span></li>
</ul>
<div class="sourceCode" id="cb270"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb270-1" data-line-number="1">df.seq &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">4</span>, <span class="dv">20</span>, <span class="dt">by=</span>.<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb270-2" data-line-number="2">nx &lt;-<span class="st"> </span><span class="kw">length</span>(df.seq)</a>
<a class="sourceLine" id="cb270-3" data-line-number="3">Cp.seq &lt;-<span class="st"> </span><span class="kw">numeric</span>(nx)</a>
<a class="sourceLine" id="cb270-4" data-line-number="4"><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nx) {</a>
<a class="sourceLine" id="cb270-5" data-line-number="5">    Cp.seq[k] &lt;-<span class="st"> </span><span class="kw">CpStatSmoothSpline</span>(<span class="dt">x=</span>bonedat<span class="op">$</span>age, <span class="dt">y=</span>bonedat<span class="op">$</span>spnbmd, <span class="dt">df=</span>df.seq[k], </a>
<a class="sourceLine" id="cb270-6" data-line-number="6">                                    <span class="dt">sigsq.hat=</span>sigsq.est)</a>
<a class="sourceLine" id="cb270-7" data-line-number="7">}</a>
<a class="sourceLine" id="cb270-8" data-line-number="8"></a>
<a class="sourceLine" id="cb270-9" data-line-number="9"><span class="kw">plot</span>(df.seq, Cp.seq, <span class="dt">ylab=</span><span class="st">&quot;CP Stat&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Bone Data: Cp(df) vs. </span></a>
<a class="sourceLine" id="cb270-10" data-line-number="10"><span class="st">     df for the Smoothing Spline&quot;</span>)</a></code></pre></div>
<p><img src="12-splines_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<ul>
<li>More specifically, itâs about <span class="math inline">\(11.6\)</span>:</li>
</ul>
<div class="sourceCode" id="cb271"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb271-1" data-line-number="1">df.seq[<span class="kw">which.min</span>(Cp.seq)]</a></code></pre></div>
<pre><code>## [1] 11.6</code></pre>
</div>
<div id="knot-selection-for-regression-splines-with-the-cp-statistic" class="section level3">
<h3><span class="header-section-number">12.6.2</span> Knot Selection for Regression Splines with the Cp statistic</h3>
<ul>
<li><p>Letâs go back to the regression spline methods we described in Section 12.3 also use the bone data to try to find the best number of knots for the case of regression splines.</p></li>
<li><p>We will consider knots <span class="math inline">\(u_{1}, \ldots, u_{q}\)</span> that are computed automatically by the <code>bs</code> function when we specify the degrees of freedom. These are based on the quantiles of the covariate <span class="math inline">\(x_{i}\)</span>. We will consider values of <span class="math inline">\(q\)</span> between <span class="math inline">\(0\)</span> and <span class="math inline">\(20\)</span>.</p></li>
</ul>
<div class="sourceCode" id="cb273"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb273-1" data-line-number="1">sigsq.hat &lt;-<span class="st"> </span><span class="fl">.0015</span></a>
<a class="sourceLine" id="cb273-2" data-line-number="2">n &lt;-<span class="st"> </span><span class="kw">nrow</span>(bonedat)</a>
<a class="sourceLine" id="cb273-3" data-line-number="3"></a>
<a class="sourceLine" id="cb273-4" data-line-number="4">qqseq &lt;-<span class="st"> </span><span class="dv">0</span><span class="op">:</span><span class="dv">20</span></a>
<a class="sourceLine" id="cb273-5" data-line-number="5">Cp.seq &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">length</span>(qqseq))</a>
<a class="sourceLine" id="cb273-6" data-line-number="6">nq &lt;-<span class="st"> </span><span class="kw">length</span>(qqseq)</a>
<a class="sourceLine" id="cb273-7" data-line-number="7"><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nq) {</a>
<a class="sourceLine" id="cb273-8" data-line-number="8">  q =<span class="st"> </span>qqseq[k]</a>
<a class="sourceLine" id="cb273-9" data-line-number="9">  tt &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span>q</a>
<a class="sourceLine" id="cb273-10" data-line-number="10">  </a>
<a class="sourceLine" id="cb273-11" data-line-number="11">  <span class="co">#uu &lt;- 9.4 + (15.7/(q+1))*tt</span></a>
<a class="sourceLine" id="cb273-12" data-line-number="12">  <span class="cf">if</span>(q <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb273-13" data-line-number="13">    tmp &lt;-<span class="st"> </span><span class="kw">lm</span>(bonedat<span class="op">$</span>spnbmd <span class="op">~</span><span class="st"> </span><span class="kw">bs</span>(bonedat<span class="op">$</span>age, <span class="dt">df=</span>q<span class="op">+</span><span class="dv">3</span>))</a>
<a class="sourceLine" id="cb273-14" data-line-number="14">  } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb273-15" data-line-number="15">    tmp &lt;-<span class="st"> </span><span class="kw">lm</span>(bonedat<span class="op">$</span>spnbmd <span class="op">~</span><span class="st"> </span><span class="kw">bs</span>(bonedat<span class="op">$</span>age, <span class="dt">df=</span>q<span class="op">+</span><span class="dv">3</span>))</a>
<a class="sourceLine" id="cb273-16" data-line-number="16">  }</a>
<a class="sourceLine" id="cb273-17" data-line-number="17">  RSS &lt;-<span class="st"> </span><span class="kw">mean</span>((bonedat<span class="op">$</span>spnbmd <span class="op">-</span><span class="st"> </span>tmp<span class="op">$</span>fitted.values)<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb273-18" data-line-number="18">  Cp.seq[k] &lt;-<span class="st"> </span>RSS <span class="op">+</span><span class="st"> </span>(<span class="dv">2</span><span class="op">*</span>sigsq.hat<span class="op">*</span>(q <span class="op">+</span><span class="st"> </span><span class="dv">4</span>))<span class="op">/</span>n</a>
<a class="sourceLine" id="cb273-19" data-line-number="19">}</a>
<a class="sourceLine" id="cb273-20" data-line-number="20"><span class="kw">plot</span>(qqseq, Cp.seq)</a></code></pre></div>
<p><img src="12-splines_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<ul>
<li>From the plot, it looks like the best number of knots when using the <span class="math inline">\(C_{p}\)</span> statistic is <span class="math inline">\(q = 10\)</span>.</li>
</ul>

</div>
</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="kernel-regression-and-local-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="decision-tree.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ElementsNonparStat.pdf"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
