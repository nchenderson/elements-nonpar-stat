<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 11 Kernel Regression and Local Regression | Elements of Nonparametric Statistics</title>
  <meta name="description" content="Course notes for Biostatistics 685/Statistics 560 (Winter 2020)." />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 11 Kernel Regression and Local Regression | Elements of Nonparametric Statistics" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://nchenderson.github.io/elements-nonpar-stat/" />
  
  <meta property="og:description" content="Course notes for Biostatistics 685/Statistics 560 (Winter 2020)." />
  <meta name="github-repo" content="nchenderson/elements-nonpar-stat" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 11 Kernel Regression and Local Regression | Elements of Nonparametric Statistics" />
  
  <meta name="twitter:description" content="Course notes for Biostatistics 685/Statistics 560 (Winter 2020)." />
  

<meta name="author" content="Nicholas Henderson" />


<meta name="date" content="2020-04-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ci.html"/>
<link rel="next" href="inference-for-regression.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Biostat 685/Stat 560</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#sec:whatisnonpar"><i class="fa fa-check"></i><b>1.1</b> What is Nonparametric Statistics?</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#sec:course-outline"><i class="fa fa-check"></i><b>1.2</b> Outline of Course</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#sec:example-nonpar-tests"><i class="fa fa-check"></i><b>1.3</b> Example 1: Nonparametric vs. Parametric Two-Sample Testing</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#sec:example-nonpar-estimation"><i class="fa fa-check"></i><b>1.4</b> Example 2: Nonparametric Estimation</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#sec:example-nonpar-confint"><i class="fa fa-check"></i><b>1.5</b> Example 3: Confidence Intervals</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#sec:example-nonpar-regress1"><i class="fa fa-check"></i><b>1.6</b> Example 4: Nonparametric Regression with a Single Covariate</a></li>
<li class="chapter" data-level="1.7" data-path="intro.html"><a href="intro.html#sec:example-nonpar-regress2"><i class="fa fa-check"></i><b>1.7</b> Example 5: Classification and Regression Trees (CART)</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>2</b> Working with R</a></li>
<li class="part"><span><b>I Nonparametric Testing</b></span></li>
<li class="chapter" data-level="3" data-path="rank-tests.html"><a href="rank-tests.html"><i class="fa fa-check"></i><b>3</b> Rank and Sign Statistics</a><ul>
<li class="chapter" data-level="3.1" data-path="rank-tests.html"><a href="rank-tests.html#ranks"><i class="fa fa-check"></i><b>3.1</b> Ranks</a><ul>
<li class="chapter" data-level="3.1.1" data-path="rank-tests.html"><a href="rank-tests.html#definition"><i class="fa fa-check"></i><b>3.1.1</b> Definition</a></li>
<li class="chapter" data-level="3.1.2" data-path="rank-tests.html"><a href="rank-tests.html#handling-ties"><i class="fa fa-check"></i><b>3.1.2</b> Handling Ties</a></li>
<li class="chapter" data-level="3.1.3" data-path="rank-tests.html"><a href="rank-tests.html#properties-of-ranks"><i class="fa fa-check"></i><b>3.1.3</b> Properties of Ranks</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="rank-tests.html"><a href="rank-tests.html#the-wilcoxon-rank-sum-wrs-test-a-two-sample-test"><i class="fa fa-check"></i><b>3.2</b> The Wilcoxon Rank Sum (WRS) Test: A Two-Sample Test</a><ul>
<li class="chapter" data-level="3.2.1" data-path="rank-tests.html"><a href="rank-tests.html#goal-of-the-test"><i class="fa fa-check"></i><b>3.2.1</b> Goal of the Test</a></li>
<li class="chapter" data-level="3.2.2" data-path="rank-tests.html"><a href="rank-tests.html#definition-of-the-wrs-test-statistic"><i class="fa fa-check"></i><b>3.2.2</b> Definition of the WRS Test Statistic</a></li>
<li class="chapter" data-level="3.2.3" data-path="rank-tests.html"><a href="rank-tests.html#computing-p-values-for-the-wrs-test"><i class="fa fa-check"></i><b>3.2.3</b> Computing p-values for the WRS Test</a></li>
<li class="chapter" data-level="3.2.4" data-path="rank-tests.html"><a href="rank-tests.html#computing-the-wrs-test-in-r"><i class="fa fa-check"></i><b>3.2.4</b> Computing the WRS test in R</a></li>
<li class="chapter" data-level="3.2.5" data-path="rank-tests.html"><a href="rank-tests.html#additional-notes-for-the-wrs-test"><i class="fa fa-check"></i><b>3.2.5</b> Additional Notes for the WRS test</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="rank-tests.html"><a href="rank-tests.html#one-sample-tests"><i class="fa fa-check"></i><b>3.3</b> One Sample Tests</a><ul>
<li class="chapter" data-level="3.3.1" data-path="rank-tests.html"><a href="rank-tests.html#sign-test"><i class="fa fa-check"></i><b>3.3.1</b> The Sign Test</a></li>
<li class="chapter" data-level="3.3.2" data-path="rank-tests.html"><a href="rank-tests.html#the-wilcoxon-signed-rank-test"><i class="fa fa-check"></i><b>3.3.2</b> The Wilcoxon Signed Rank Test</a></li>
<li class="chapter" data-level="3.3.3" data-path="rank-tests.html"><a href="rank-tests.html#using-r-to-perform-the-sign-and-wilcoxon-tests"><i class="fa fa-check"></i><b>3.3.3</b> Using R to Perform the Sign and Wilcoxon Tests</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="rank-tests.html"><a href="rank-tests.html#power-and-comparisons-with-parametric-tests"><i class="fa fa-check"></i><b>3.4</b> Power and Comparisons with Parametric Tests</a><ul>
<li class="chapter" data-level="3.4.1" data-path="rank-tests.html"><a href="rank-tests.html#the-power-function-of-a-test"><i class="fa fa-check"></i><b>3.4.1</b> The Power Function of a Test</a></li>
<li class="chapter" data-level="3.4.2" data-path="rank-tests.html"><a href="rank-tests.html#power-comparisons-and-asymptotic-relative-efficiency"><i class="fa fa-check"></i><b>3.4.2</b> Power Comparisons and Asymptotic Relative Efficiency</a></li>
<li class="chapter" data-level="3.4.3" data-path="rank-tests.html"><a href="rank-tests.html#efficiency-examples"><i class="fa fa-check"></i><b>3.4.3</b> Efficiency Examples</a></li>
<li class="chapter" data-level="3.4.4" data-path="rank-tests.html"><a href="rank-tests.html#efficiency-comparisons-for-several-distributions"><i class="fa fa-check"></i><b>3.4.4</b> Efficiency Comparisons for Several Distributions</a></li>
<li class="chapter" data-level="3.4.5" data-path="rank-tests.html"><a href="rank-tests.html#a-power-contest"><i class="fa fa-check"></i><b>3.4.5</b> A Power “Contest”</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="rank-tests.html"><a href="rank-tests.html#linear-rank-statistics-in-general"><i class="fa fa-check"></i><b>3.5</b> Linear Rank Statistics in General</a><ul>
<li class="chapter" data-level="3.5.1" data-path="rank-tests.html"><a href="rank-tests.html#definition-1"><i class="fa fa-check"></i><b>3.5.1</b> Definition</a></li>
<li class="chapter" data-level="3.5.2" data-path="rank-tests.html"><a href="rank-tests.html#properties-of-linear-rank-statistics"><i class="fa fa-check"></i><b>3.5.2</b> Properties of Linear Rank Statistics</a></li>
<li class="chapter" data-level="3.5.3" data-path="rank-tests.html"><a href="rank-tests.html#other-examples-of-linear-rank-statistics"><i class="fa fa-check"></i><b>3.5.3</b> Other Examples of Linear Rank Statistics</a></li>
<li class="chapter" data-level="3.5.4" data-path="rank-tests.html"><a href="rank-tests.html#choosing-the-scores-a_ni"><i class="fa fa-check"></i><b>3.5.4</b> Choosing the scores <span class="math inline">\(a_{N}(i)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="rank-tests.html"><a href="rank-tests.html#additional-reading"><i class="fa fa-check"></i><b>3.6</b> Additional Reading</a></li>
<li class="chapter" data-level="3.7" data-path="rank-tests.html"><a href="rank-tests.html#exercises"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="krusk-wallis.html"><a href="krusk-wallis.html"><i class="fa fa-check"></i><b>4</b> Rank Tests for Multiple Groups</a><ul>
<li class="chapter" data-level="4.1" data-path="krusk-wallis.html"><a href="krusk-wallis.html#the-kruskal-wallis-test"><i class="fa fa-check"></i><b>4.1</b> The Kruskal-Wallis Test</a><ul>
<li class="chapter" data-level="4.1.1" data-path="krusk-wallis.html"><a href="krusk-wallis.html#definition-2"><i class="fa fa-check"></i><b>4.1.1</b> Definition</a></li>
<li class="chapter" data-level="4.1.2" data-path="krusk-wallis.html"><a href="krusk-wallis.html#asymptotic-distribution-and-connection-to-one-way-anova"><i class="fa fa-check"></i><b>4.1.2</b> Asymptotic Distribution and Connection to One-Way ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="krusk-wallis.html"><a href="krusk-wallis.html#performing-the-kruskal-wallis-test-in-r"><i class="fa fa-check"></i><b>4.2</b> Performing the Kruskal-Wallis Test in R</a></li>
<li class="chapter" data-level="4.3" data-path="krusk-wallis.html"><a href="krusk-wallis.html#comparison-of-specific-groups"><i class="fa fa-check"></i><b>4.3</b> Comparison of Specific Groups</a></li>
<li class="chapter" data-level="4.4" data-path="krusk-wallis.html"><a href="krusk-wallis.html#an-additional-example"><i class="fa fa-check"></i><b>4.4</b> An Additional Example</a></li>
<li class="chapter" data-level="4.5" data-path="krusk-wallis.html"><a href="krusk-wallis.html#additional-reading-1"><i class="fa fa-check"></i><b>4.5</b> Additional Reading</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="permutation.html"><a href="permutation.html"><i class="fa fa-check"></i><b>5</b> Permutation Tests</a><ul>
<li class="chapter" data-level="5.1" data-path="permutation.html"><a href="permutation.html#notation"><i class="fa fa-check"></i><b>5.1</b> Notation</a></li>
<li class="chapter" data-level="5.2" data-path="permutation.html"><a href="permutation.html#permutation-tests-for-the-two-sample-problem"><i class="fa fa-check"></i><b>5.2</b> Permutation Tests for the Two-Sample Problem</a><ul>
<li class="chapter" data-level="5.2.1" data-path="permutation.html"><a href="permutation.html#example-1"><i class="fa fa-check"></i><b>5.2.1</b> Example 1</a></li>
<li class="chapter" data-level="5.2.2" data-path="permutation.html"><a href="permutation.html#permutation-test-p-values"><i class="fa fa-check"></i><b>5.2.2</b> Permutation Test p-values</a></li>
<li class="chapter" data-level="5.2.3" data-path="permutation.html"><a href="permutation.html#example-2-ratios-of-means"><i class="fa fa-check"></i><b>5.2.3</b> Example 2: Ratios of Means</a></li>
<li class="chapter" data-level="5.2.4" data-path="permutation.html"><a href="permutation.html#example-3-differences-in-quantiles"><i class="fa fa-check"></i><b>5.2.4</b> Example 3: Differences in Quantiles</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="permutation.html"><a href="permutation.html#the-permutation-test-as-a-conditional-test"><i class="fa fa-check"></i><b>5.3</b> The Permutation Test as a Conditional Test</a></li>
<li class="chapter" data-level="5.4" data-path="permutation.html"><a href="permutation.html#a-permutation-test-for-correlation"><i class="fa fa-check"></i><b>5.4</b> A Permutation Test for Correlation</a></li>
<li class="chapter" data-level="5.5" data-path="permutation.html"><a href="permutation.html#a-permutation-test-for-variable-importance-in-regression-and-machine-learning"><i class="fa fa-check"></i><b>5.5</b> A Permutation Test for Variable Importance in Regression and Machine Learning</a></li>
</ul></li>
<li class="part"><span><b>II Nonparametric Estimation</b></span></li>
<li class="chapter" data-level="6" data-path="ustat.html"><a href="ustat.html"><i class="fa fa-check"></i><b>6</b> U-Statistics</a><ul>
<li class="chapter" data-level="6.1" data-path="ustat.html"><a href="ustat.html#definition-3"><i class="fa fa-check"></i><b>6.1</b> Definition</a></li>
<li class="chapter" data-level="6.2" data-path="ustat.html"><a href="ustat.html#examples"><i class="fa fa-check"></i><b>6.2</b> Examples</a><ul>
<li class="chapter" data-level="6.2.1" data-path="ustat.html"><a href="ustat.html#example-1-the-sample-mean"><i class="fa fa-check"></i><b>6.2.1</b> Example 1: The Sample Mean</a></li>
<li class="chapter" data-level="6.2.2" data-path="ustat.html"><a href="ustat.html#example-2-the-sample-variance"><i class="fa fa-check"></i><b>6.2.2</b> Example 2: The Sample Variance</a></li>
<li class="chapter" data-level="6.2.3" data-path="ustat.html"><a href="ustat.html#example-3-ginis-mean-difference"><i class="fa fa-check"></i><b>6.2.3</b> Example 3: Gini’s Mean Difference</a></li>
<li class="chapter" data-level="6.2.4" data-path="ustat.html"><a href="ustat.html#example-4-wilcoxon-signed-rank-statistic"><i class="fa fa-check"></i><b>6.2.4</b> Example 4: Wilcoxon Signed Rank Statistic</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="ustat.html"><a href="ustat.html#inference-using-u-statistics"><i class="fa fa-check"></i><b>6.3</b> Inference using U-statistics</a></li>
<li class="chapter" data-level="6.4" data-path="ustat.html"><a href="ustat.html#u-statistics-for-two-sample-problems"><i class="fa fa-check"></i><b>6.4</b> U-statistics for Two-Sample Problems</a><ul>
<li class="chapter" data-level="6.4.1" data-path="ustat.html"><a href="ustat.html#the-mann-whitney-statistic"><i class="fa fa-check"></i><b>6.4.1</b> The Mann-Whitney Statistic</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="ustat.html"><a href="ustat.html#measures-of-association"><i class="fa fa-check"></i><b>6.5</b> Measures of Association</a><ul>
<li class="chapter" data-level="6.5.1" data-path="ustat.html"><a href="ustat.html#spearmans-rank-correlation"><i class="fa fa-check"></i><b>6.5.1</b> Spearman’s Rank Correlation</a></li>
<li class="chapter" data-level="6.5.2" data-path="ustat.html"><a href="ustat.html#kendalls-tau"><i class="fa fa-check"></i><b>6.5.2</b> Kendall’s tau</a></li>
<li class="chapter" data-level="6.5.3" data-path="ustat.html"><a href="ustat.html#distance-covariance-and-correlation"><i class="fa fa-check"></i><b>6.5.3</b> Distance Covariance and Correlation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="edf.html"><a href="edf.html"><i class="fa fa-check"></i><b>7</b> The Empirical Distribution Function</a><ul>
<li class="chapter" data-level="7.1" data-path="edf.html"><a href="edf.html#definition-and-basic-properties"><i class="fa fa-check"></i><b>7.1</b> Definition and Basic Properties</a></li>
<li class="chapter" data-level="7.2" data-path="edf.html"><a href="edf.html#confidence-intervals-for-ft"><i class="fa fa-check"></i><b>7.2</b> Confidence intervals for F(t)</a></li>
<li class="chapter" data-level="7.3" data-path="edf.html"><a href="edf.html#the-empirical-distribution-function-in-r"><i class="fa fa-check"></i><b>7.3</b> The Empirical Distribution Function in R</a></li>
<li class="chapter" data-level="7.4" data-path="edf.html"><a href="edf.html#the-kolmogorov-smirnov-test"><i class="fa fa-check"></i><b>7.4</b> The Kolmogorov-Smirnov Test</a></li>
<li class="chapter" data-level="7.5" data-path="edf.html"><a href="edf.html#the-empirical-distribution-function-and-statistical-functionals"><i class="fa fa-check"></i><b>7.5</b> The empirical distribution function and statistical functionals</a></li>
<li class="chapter" data-level="7.6" data-path="edf.html"><a href="edf.html#additional-reading-2"><i class="fa fa-check"></i><b>7.6</b> Additional Reading</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="density-estimation.html"><a href="density-estimation.html"><i class="fa fa-check"></i><b>8</b> Density Estimation</a><ul>
<li class="chapter" data-level="8.1" data-path="density-estimation.html"><a href="density-estimation.html#introduction"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="density-estimation.html"><a href="density-estimation.html#histograms"><i class="fa fa-check"></i><b>8.2</b> Histograms</a><ul>
<li class="chapter" data-level="8.2.1" data-path="density-estimation.html"><a href="density-estimation.html#definition-5"><i class="fa fa-check"></i><b>8.2.1</b> Definition</a></li>
<li class="chapter" data-level="8.2.2" data-path="density-estimation.html"><a href="density-estimation.html#histograms-in-r"><i class="fa fa-check"></i><b>8.2.2</b> Histograms in R</a></li>
<li class="chapter" data-level="8.2.3" data-path="density-estimation.html"><a href="density-estimation.html#performance-of-the-histogram-estimate-and-bin-width-selection"><i class="fa fa-check"></i><b>8.2.3</b> Performance of the Histogram Estimate and Bin Width Selection</a></li>
<li class="chapter" data-level="8.2.4" data-path="density-estimation.html"><a href="density-estimation.html#choosing-the-histogram-bin-width"><i class="fa fa-check"></i><b>8.2.4</b> Choosing the Histogram Bin Width</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="density-estimation.html"><a href="density-estimation.html#a-box-type-density-estimate"><i class="fa fa-check"></i><b>8.3</b> A Box-type Density Estimate</a></li>
<li class="chapter" data-level="8.4" data-path="density-estimation.html"><a href="density-estimation.html#kernel-density-estimation"><i class="fa fa-check"></i><b>8.4</b> Kernel Density Estimation</a><ul>
<li class="chapter" data-level="8.4.1" data-path="density-estimation.html"><a href="density-estimation.html#definition-6"><i class="fa fa-check"></i><b>8.4.1</b> Definition</a></li>
<li class="chapter" data-level="8.4.2" data-path="density-estimation.html"><a href="density-estimation.html#bias-variance-and-amise-of-kernel-density-estimates"><i class="fa fa-check"></i><b>8.4.2</b> Bias, Variance, and AMISE of Kernel Density Estimates</a></li>
<li class="chapter" data-level="8.4.3" data-path="density-estimation.html"><a href="density-estimation.html#bandwidth-selection-with-the-normal-reference-rule-and-silvermans-rule-of-thumb"><i class="fa fa-check"></i><b>8.4.3</b> Bandwidth Selection with the Normal Reference Rule and Silverman’s “Rule of Thumb”</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="density-estimation.html"><a href="density-estimation.html#cross-validation-for-bandwidth-selection"><i class="fa fa-check"></i><b>8.5</b> Cross-Validation for Bandwidth Selection</a><ul>
<li class="chapter" data-level="8.5.1" data-path="density-estimation.html"><a href="density-estimation.html#squared-error-cross-validation"><i class="fa fa-check"></i><b>8.5.1</b> Squared-Error Cross-Validation</a></li>
<li class="chapter" data-level="8.5.2" data-path="density-estimation.html"><a href="density-estimation.html#computing-the-cross-validation-bandwidth"><i class="fa fa-check"></i><b>8.5.2</b> Computing the Cross-validation Bandwidth</a></li>
<li class="chapter" data-level="8.5.3" data-path="density-estimation.html"><a href="density-estimation.html#likelihood-cross-validation"><i class="fa fa-check"></i><b>8.5.3</b> Likelihood Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="density-estimation.html"><a href="density-estimation.html#density-estimation-in-r"><i class="fa fa-check"></i><b>8.6</b> Density Estimation in R</a></li>
<li class="chapter" data-level="8.7" data-path="density-estimation.html"><a href="density-estimation.html#additional-reading-3"><i class="fa fa-check"></i><b>8.7</b> Additional Reading</a></li>
</ul></li>
<li class="part"><span><b>III Quantifying Uncertainty</b></span></li>
<li class="chapter" data-level="9" data-path="bootstrap-main.html"><a href="bootstrap-main.html"><i class="fa fa-check"></i><b>9</b> The Bootstrap</a><ul>
<li class="chapter" data-level="9.1" data-path="bootstrap-main.html"><a href="bootstrap-main.html#introduction-1"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="bootstrap-main.html"><a href="bootstrap-main.html#description-of-the-bootstrap"><i class="fa fa-check"></i><b>9.2</b> Description of the Bootstrap</a><ul>
<li class="chapter" data-level="9.2.1" data-path="bootstrap-main.html"><a href="bootstrap-main.html#description"><i class="fa fa-check"></i><b>9.2.1</b> Description</a></li>
<li class="chapter" data-level="9.2.2" data-path="bootstrap-main.html"><a href="bootstrap-main.html#example-confidence-intervals-for-the-rate-parameter-of-an-exponential-distribution"><i class="fa fa-check"></i><b>9.2.2</b> Example: Confidence Intervals for the Rate Parameter of an Exponential Distribution</a></li>
<li class="chapter" data-level="9.2.3" data-path="bootstrap-main.html"><a href="bootstrap-main.html#example-confidence-intervals-for-the-ratio-of-two-quantiles"><i class="fa fa-check"></i><b>9.2.3</b> Example: Confidence Intervals for the Ratio of Two Quantiles</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="bootstrap-main.html"><a href="bootstrap-main.html#why-is-the-bootstrap-procedure-reasonable"><i class="fa fa-check"></i><b>9.3</b> Why is the Bootstrap Procedure Reasonable?</a></li>
<li class="chapter" data-level="9.4" data-path="bootstrap-main.html"><a href="bootstrap-main.html#pivotal-bootstrap-confidence-intervals"><i class="fa fa-check"></i><b>9.4</b> Pivotal Bootstrap Confidence Intervals</a></li>
<li class="chapter" data-level="9.5" data-path="bootstrap-main.html"><a href="bootstrap-main.html#the-parametric-bootstrap"><i class="fa fa-check"></i><b>9.5</b> The Parametric Bootstrap</a><ul>
<li class="chapter" data-level="9.5.1" data-path="bootstrap-main.html"><a href="bootstrap-main.html#parametric-bootstrap-for-the-median-age-from-the-kidney-data"><i class="fa fa-check"></i><b>9.5.1</b> Parametric Bootstrap for the Median Age from the Kidney Data</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="bootstrap-main.html"><a href="bootstrap-main.html#additional-reading-4"><i class="fa fa-check"></i><b>9.6</b> Additional Reading</a></li>
<li class="chapter" data-level="9.7" data-path="bootstrap-main.html"><a href="bootstrap-main.html#exercises-1"><i class="fa fa-check"></i><b>9.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ci.html"><a href="ci.html"><i class="fa fa-check"></i><b>10</b> Bootstrap Examples and the Jackknife</a><ul>
<li class="chapter" data-level="10.1" data-path="ci.html"><a href="ci.html#the-parametric-bootstrap-for-an-ar1-model"><i class="fa fa-check"></i><b>10.1</b> The Parametric Bootstrap for an AR(1) model</a></li>
<li class="chapter" data-level="10.2" data-path="ci.html"><a href="ci.html#using-the-bootstrap-in-regression"><i class="fa fa-check"></i><b>10.2</b> Using the Bootstrap in Regression</a><ul>
<li class="chapter" data-level="10.2.1" data-path="ci.html"><a href="ci.html#parametric-bootstrap-for-regression"><i class="fa fa-check"></i><b>10.2.1</b> Parametric Bootstrap for Regression</a></li>
<li class="chapter" data-level="10.2.2" data-path="ci.html"><a href="ci.html#nonparametric-bootstrap-for-regression"><i class="fa fa-check"></i><b>10.2.2</b> Nonparametric Bootstrap for Regression</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="ci.html"><a href="ci.html#pointwise-confidence-intervals-for-a-density-function"><i class="fa fa-check"></i><b>10.3</b> Pointwise Confidence Intervals for a Density Function</a></li>
<li class="chapter" data-level="10.4" data-path="ci.html"><a href="ci.html#when-can-the-bootstrap-fail"><i class="fa fa-check"></i><b>10.4</b> When can the Bootstrap Fail?</a><ul>
<li class="chapter" data-level="10.4.1" data-path="ci.html"><a href="ci.html#example-the-shifted-exponential-distribution"><i class="fa fa-check"></i><b>10.4.1</b> Example: The Shifted Exponential Distribution</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="ci.html"><a href="ci.html#the-jackknife"><i class="fa fa-check"></i><b>10.5</b> The Jackknife</a></li>
</ul></li>
<li class="part"><span><b>IV Nonparametric Regression: Part I</b></span></li>
<li class="chapter" data-level="11" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html"><i class="fa fa-check"></i><b>11</b> Kernel Regression and Local Regression</a><ul>
<li class="chapter" data-level="11.1" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#introduction-2"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#kernel-regression"><i class="fa fa-check"></i><b>11.2</b> Kernel Regression</a><ul>
<li class="chapter" data-level="11.2.1" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#the-regressogram"><i class="fa fa-check"></i><b>11.2.1</b> The Regressogram</a></li>
<li class="chapter" data-level="11.2.2" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#the-local-average-estimator"><i class="fa fa-check"></i><b>11.2.2</b> The Local Average Estimator</a></li>
<li class="chapter" data-level="11.2.3" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#k-nearest-neighbor-k-nn-regression"><i class="fa fa-check"></i><b>11.2.3</b> k-Nearest Neighbor (k-NN) Regression</a></li>
<li class="chapter" data-level="11.2.4" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#the-nadaraya-watson-estimator"><i class="fa fa-check"></i><b>11.2.4</b> The Nadaraya-Watson Estimator</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#local-linear-regression"><i class="fa fa-check"></i><b>11.3</b> Local Linear Regression</a><ul>
<li class="chapter" data-level="11.3.1" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#definition-7"><i class="fa fa-check"></i><b>11.3.1</b> Definition</a></li>
<li class="chapter" data-level="11.3.2" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#advantages-of-the-local-linear-estimator"><i class="fa fa-check"></i><b>11.3.2</b> Advantages of the Local Linear Estimator</a></li>
<li class="chapter" data-level="11.3.3" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#an-example-in-r"><i class="fa fa-check"></i><b>11.3.3</b> An Example in R</a></li>
<li class="chapter" data-level="11.3.4" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#local-polynomial-regression"><i class="fa fa-check"></i><b>11.3.4</b> Local Polynomial Regression</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#selecting-the-bandwidthsmoothing-parameter"><i class="fa fa-check"></i><b>11.4</b> Selecting the Bandwidth/Smoothing Parameter</a><ul>
<li class="chapter" data-level="11.4.1" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#representing-in-linear-form"><i class="fa fa-check"></i><b>11.4.1</b> Representing in Linear Form</a></li>
<li class="chapter" data-level="11.4.2" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#the-cp-statistic"><i class="fa fa-check"></i><b>11.4.2</b> The Cp Statistic</a></li>
<li class="chapter" data-level="11.4.3" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>11.4.3</b> Leave-one-out Cross Validation</a></li>
<li class="chapter" data-level="11.4.4" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#example-choosing-the-best-bin-width-for-the-local-average-estimator."><i class="fa fa-check"></i><b>11.4.4</b> Example: Choosing the Best Bin Width for the Local Average Estimator.</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#additional-functions-in-r"><i class="fa fa-check"></i><b>11.5</b> Additional functions in R</a></li>
<li class="chapter" data-level="11.6" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#multivariate-problems"><i class="fa fa-check"></i><b>11.6</b> Multivariate Problems</a></li>
<li class="chapter" data-level="11.7" data-path="kernel-regression-and-local-regression.html"><a href="kernel-regression-and-local-regression.html#additional-reading-5"><i class="fa fa-check"></i><b>11.7</b> Additional Reading</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="inference-for-regression.html"><a href="inference-for-regression.html"><i class="fa fa-check"></i><b>12</b> Splines and Penalized Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="inference-for-regression.html"><a href="inference-for-regression.html#introduction-3"><i class="fa fa-check"></i><b>12.1</b> Introduction</a><ul>
<li class="chapter" data-level="12.1.1" data-path="inference-for-regression.html"><a href="inference-for-regression.html#regressogram-piecewise-constant-estimate"><i class="fa fa-check"></i><b>12.1.1</b> Regressogram (Piecewise Constant Estimate)</a></li>
<li class="chapter" data-level="12.1.2" data-path="inference-for-regression.html"><a href="inference-for-regression.html#piecewise-linear-estimates"><i class="fa fa-check"></i><b>12.1.2</b> Piecewise Linear Estimates</a></li>
<li class="chapter" data-level="12.1.3" data-path="inference-for-regression.html"><a href="inference-for-regression.html#piecewise-cubic-estimates"><i class="fa fa-check"></i><b>12.1.3</b> Piecewise Cubic Estimates</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="inference-for-regression.html"><a href="inference-for-regression.html#piecewise-linear-estimates-with-continuity-linear-splines"><i class="fa fa-check"></i><b>12.2</b> Piecewise Linear Estimates with Continuity (Linear Splines)</a></li>
<li class="chapter" data-level="12.3" data-path="inference-for-regression.html"><a href="inference-for-regression.html#cubic-splines-and-spline-basis-functions"><i class="fa fa-check"></i><b>12.3</b> Cubic Splines and Spline Basis Functions</a></li>
<li class="chapter" data-level="12.4" data-path="inference-for-regression.html"><a href="inference-for-regression.html#smoothing-splinespenalized-regression"><i class="fa fa-check"></i><b>12.4</b> Smoothing Splines/Penalized Regression</a></li>
</ul></li>
<li class="part"><span><b>V Nonparametric Regression: Part II</b></span></li>
<li class="chapter" data-level="13" data-path="decision-tree.html"><a href="decision-tree.html"><i class="fa fa-check"></i><b>13</b> Decision Trees and CART</a></li>
<li class="chapter" data-level="14" data-path="ensemble.html"><a href="ensemble.html"><i class="fa fa-check"></i><b>14</b> Ensemble Methods for Prediction</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Elements of Nonparametric Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="kernel-regression-and-local-regression" class="section level1">
<h1><span class="header-section-number">Chapter 11</span> Kernel Regression and Local Regression</h1>
<div id="introduction-2" class="section level2">
<h2><span class="header-section-number">11.1</span> Introduction</h2>
<ul>
<li><p>In regression we are interested in characterizing, in some way, the relationship
between a collection of responses <span class="math inline">\(Y_{1},\ldots,Y_{n}\)</span> and covariate vectors
<span class="math inline">\((\mathbf{x}_{1}, \ldots, \mathbf{x}_{n})\)</span>.</p></li>
<li><p>Linear regression is one way of approaching this problem. This assumes
the expectation of <span class="math inline">\(Y_{i}\)</span> can be expressed as a linear combination
of the covariates:
<span class="math display">\[\begin{equation}
E(Y_{i}| \mathbf{x}_{i}) = \beta_{0} + \sum_{j=1}^{p} x_{ij}\beta_{j}  \nonumber
\end{equation}\]</span></p></li>
<li>More generally, we can consider the following model
<span class="math display">\[\begin{equation}
Y_{i} = m( \mathbf{x}_{i} ) + \varepsilon_{i}  \nonumber
\end{equation}\]</span>
<ul>
<li><span class="math inline">\(m(\mathbf{x}_{i})\)</span> - the “mean function” or “regression function”</li>
<li><span class="math inline">\(\mathbf{x}_{i} = (x_{i1}, \ldots, x_{ip})\)</span> - the <span class="math inline">\(i^{th}\)</span> covariate vector</li>
</ul></li>
<li><p>The residuals <span class="math inline">\(\varepsilon_{1}, \ldots, \varepsilon_{n}\)</span> are assumed to
be i.i.d. and have mean zero.</p></li>
</ul>
<hr />
<ul>
<li><p>In a nonparametric approach, we will try to estimate <span class="math inline">\(m(\mathbf{x})\)</span> without
making any strong assumptions about the form of <span class="math inline">\(m( \mathbf{x} )\)</span>.</p></li>
<li><p>The regression function <span class="math inline">\(m(\mathbf{x})\)</span> can be thought of as the function which returns
the expectation of <span class="math inline">\(Y_{i}\)</span> given that <span class="math inline">\(\mathbf{x}_{i} = \mathbf{x}\)</span>
<span class="math display">\[\begin{equation}
m(\mathbf{x} ) = E(Y_{i}|\mathbf{x}_{i}=\mathbf{x}) 
\end{equation}\]</span></p></li>
<li>Let
<ul>
<li><span class="math inline">\(f_{Y|X}(y|\mathbf{x})\)</span> denote the conditional density of <span class="math inline">\(Y_{i}\)</span> given <span class="math inline">\(\mathbf{x}_{i}\)</span>.</li>
<li><span class="math inline">\(f_{Y,X}(y, \mathbf{x})\)</span> denote the joint density of <span class="math inline">\((Y_{i}, \mathbf{x}_{i})\)</span></li>
<li><span class="math inline">\(f_{X}(\mathbf{x})\)</span> denote the density of <span class="math inline">\(\mathbf{x}_{i}\)</span></li>
</ul></li>
<li><p>We can express the regression function as
<span class="math display">\[\begin{equation}
m(\mathbf{x}) = \int_{-\infty}^{\infty} y f_{Y|X}(y|\mathbf{x}) dy = \frac{\int y f_{Y,X}(y, \mathbf{x}) dy}{ f_{X}(\mathbf{x})  }  \nonumber
\end{equation}\]</span></p></li>
</ul>
</div>
<div id="kernel-regression" class="section level2">
<h2><span class="header-section-number">11.2</span> Kernel Regression</h2>
<ul>
<li>In this section, we will assume that the covariates are univariate.
That is, <span class="math inline">\(p=1\)</span> and <span class="math inline">\(\mathbf{x}_{i} = x_{i}\)</span> where <span class="math inline">\(x_{i}\)</span> is a real number.</li>
</ul>
<div id="the-regressogram" class="section level3">
<h3><span class="header-section-number">11.2.1</span> The Regressogram</h3>
<ul>
<li><p>The regressogram is an estimate of the mean function <span class="math inline">\(m(x)\)</span> which is
has many similarities in its construction to the histogram.</p></li>
<li><p>Similar to how we constructed the histogram, let us think about an estimate <span class="math inline">\(m(x)\)</span>
that will be constant within each of a series of bins <span class="math inline">\(B_{1}, \ldots, B_{D_{n}}\)</span>
<span class="math display">\[\begin{eqnarray}
B_{1} &amp;=&amp; [ x_{0}, x_{0} + h_{n})  \nonumber \\
B_{2} &amp;=&amp; [x_{0} + h_{n}, x_{0} + 2h_{n})  \nonumber \\
&amp;\vdots&amp;  \nonumber \\
B_{D_{n}} &amp;=&amp; [x_{0} + (D_{n} - 1)h_{n}, x_{0} + D_{n}h_{n})  \nonumber
\end{eqnarray}\]</span></p></li>
<li><p>Suppose we want to estimate <span class="math inline">\(m(x)\)</span>, where <span class="math inline">\(x\)</span> belongs to the <span class="math inline">\(k^{th}\)</span> bin.
A direct estimate of this is the average of the <span class="math inline">\(Y_{i}&#39;s\)</span> among those
<span class="math inline">\(x_{i}&#39;s\)</span> which fall into the <span class="math inline">\(k^{th}\)</span> bin.</p></li>
<li><p>Specifically, if <span class="math inline">\(x \in B_{k}\)</span>, then we estimate <span class="math inline">\(m(x)\)</span> with
<span class="math display">\[\begin{equation}
\hat{m}_{h_{n}}^{R}(x) =  \frac{ \sum_{i=1}^{n} Y_{i} I\big( x_{i} \in B_{k} \big) }{ \sum_{i=1}^{n} I\big( x_{i} \in B_{k} \big) } 
= \frac{1}{n_{k,h_{n}}} \sum_{i=1}^{n} Y_{i} I\big( x_{i} \in B_{k} \big), \nonumber
\end{equation}\]</span>
where <span class="math inline">\(n_{k,h_{n}}\)</span> is the number of <span class="math inline">\(x_{i}\)</span> that fall into the <span class="math inline">\(k^{th}\)</span> bin when using bin width <span class="math inline">\(h_{n}\)</span>.</p></li>
</ul>
<hr />
<ul>
<li><p>The estimate <span class="math inline">\(\hat{m}_{h_{n}}^{R}(x)\)</span> of the regression function is called the <strong>regressogram</strong>.</p></li>
<li><p>The intuition for this estimate is: if <span class="math inline">\(x \in B_{k}\)</span>,
then taking an average of the reponses for <span class="math inline">\(x_{i}\)</span> in a small bin containing <span class="math inline">\(x\)</span>
should give us a reasonable approximation for the expectation of <span class="math inline">\(Y_{i}\)</span> given that <span class="math inline">\(x_{i} = x\)</span>.</p></li>
<li><p>Another way of looking at the regressogram is to note that if we think of the <span class="math inline">\(x_{i}\)</span> as random variables, then for <span class="math inline">\(x \in B_{k}\)</span>
<span class="math display" id="eq:regressogram-numerator">\[\begin{eqnarray}
E\Big\{ \frac{1}{n} \sum_{i=1}^{n} Y_{i} I\big( x_{i} \in B_{k} \big) \Big\}
&amp;=&amp; E\Big\{  Y_{1} I\big( x_{1} \in B_{k} \big) \Big\}  \nonumber \\
&amp;=&amp; \int_{-\infty}^{\infty} \int_{x_{0} + (k-1)h_{n}}^{x_{0} + kh_{n}} y f_{Y,X}(y, t) dt dy  \nonumber \\
&amp;\approx&amp; h_{n} \int_{-\infty}^{\infty} y f_{Y,X}(y, x) dy
\tag{11.1}
\end{eqnarray}\]</span>
and, similarly,
<span class="math display" id="eq:regressogram-denominator">\[\begin{eqnarray}
E\Big\{ \frac{1}{n} \sum_{i=1}^{n}  I\big( x_{i} \in B_{k} \big) \Big\}
&amp;=&amp; E\Big\{  I\big( x_{1} \in B_{k} \big) \Big\}  \nonumber \\
&amp;=&amp; \int_{x_{0} + (k-1)h_{n}}^{x_{0} + kh_{n}}  f_{X}(t) dt  \nonumber \\
&amp;\approx&amp; h_{n} f_{X}(x) 
\tag{11.2}
\end{eqnarray}\]</span></p></li>
<li><p>Equations <a href="kernel-regression-and-local-regression.html#eq:regressogram-numerator">(11.1)</a> and <a href="kernel-regression-and-local-regression.html#eq:regressogram-denominator">(11.2)</a> suggest that <span class="math inline">\(\hat{m}_{h_{n}}^{R}(x)\)</span>
should be a reasonable estimate of the ratio
<span class="math display">\[\begin{equation}
\int_{-\infty}^{\infty} y f_{Y,X}(y, x) dy \big/ f_{X}(x) \nonumber 
\end{equation}\]</span></p></li>
</ul>
<hr />
<div class="figure"><span id="fig:unnamed-chunk-1"></span>
<img src="11-kernel-regression_files/figure-html/unnamed-chunk-1-1.png" alt="Framingham Data. Regressogram estimate for a regression model with diastolic blood pressure as the response and age as the covariate. Ages from 31-71 were separated into bins of width 5 years." width="672" />
<p class="caption">
Figure 8.1: Framingham Data. Regressogram estimate for a regression model with diastolic blood pressure as the response and age as the covariate. Ages from 31-71 were separated into bins of width 5 years.
</p>
</div>
<p><img src="11-kernel-regression_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<hr />
<ul>
<li><strong>Exercise 11.1</strong> Let
<span class="math display">\[\begin{equation}
\hat{\mathbf{m}} = \big( \hat{m}_{h_{n}}^{R}(x_{1}), \ldots, \hat{m}_{h_{n}}^{R}(x_{n}) \big)  \nonumber
\end{equation}\]</span>
denote the vector of “fitted values” from a regressogram estimate that has <span class="math inline">\(D_{n}\)</span> bins.
If <span class="math inline">\(\mathbf{Y} = (Y_{1}, \ldots, Y_{n})\)</span>, show that you can express <span class="math inline">\(\hat{\mathbf{m}}\)</span> as
<span class="math display">\[\begin{equation}
\hat{\mathbf{m}} = \mathbf{A}\mathbf{Y}, \nonumber
\end{equation}\]</span>
for an appropriately chosen <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span>.
What is the value of <span class="math inline">\(\textrm{tr}(\mathbf{A})\)</span>?</li>
</ul>
<hr />
</div>
<div id="the-local-average-estimator" class="section level3">
<h3><span class="header-section-number">11.2.2</span> The Local Average Estimator</h3>
<ul>
<li><p>The regressogram can be thought of as a regression analogue of the histogram.</p></li>
<li><p>The local average estimator can be thought of as a regression analogue of the
“box-type” density estimator that we described in Chapter 8.</p></li>
</ul>
<hr />
<ul>
<li><p>For each point <span class="math inline">\(x\)</span>, we are going to use a regression function estimate which
has a bin “centered” at <span class="math inline">\(x\)</span>.</p></li>
<li><p>Specifically, for each <span class="math inline">\(x\)</span>, we will form a bin of width <span class="math inline">\(2h_{n}\)</span> around
<span class="math inline">\(x\)</span> and compute the mean of the <span class="math inline">\(Y_{i}\)</span> among those observations where the <span class="math inline">\(x_{i}\)</span> fall
into this bin.</p></li>
<li><p>In other words, we are computing an average of the <span class="math inline">\(Y_{i}\)</span> in a small region
around <span class="math inline">\(x\)</span>.</p></li>
<li><p>The local average estimator <span class="math inline">\(\hat{m}_{h_{n}}^{loc}(x)\)</span> at <span class="math inline">\(x\)</span> is defined as:
<span class="math display">\[\begin{eqnarray}
\hat{m}_{h_{n}}^{loc}(x) &amp;=&amp;
\frac{ \sum_{i=1}^{n} Y_{i}I\big( x - h_{n} &lt; x_{i} &lt; x + h_{n} \big) }{ \sum_{i=1}^{n} I\big( x - h_{n} &lt; x_{i} &lt; x + h_{n} \big) } \nonumber \\
&amp;=&amp; \frac{1}{n_{h_{n}}(x)} \sum_{i=1}^{n} Y_{i}I\big( x - h_{n} &lt; x_{i} &lt; x + h_{n} \big) \nonumber 
\end{eqnarray}\]</span>
where <span class="math inline">\(n_{h_{n}}(x) = \sum_{i=1}^{n} I\big( x - h_{n} &lt; x_{i} &lt; x + h_{n} \big)\)</span>.</p></li>
<li><p>The local average estimator does not need to have a
constant value within each of a few pre-specified bins.</p></li>
</ul>
<hr />
<ul>
<li><p>We can also express the local average estimator in the following way:
<span class="math display" id="eq:local-average-estimator">\[\begin{equation}
\hat{m}_{h_{n}}^{loc}(x)
= \frac{\sum_{i=1}^{n} Y_{i} w\Big( \frac{x - X_{i}}{h_{n}} \Big)}{\sum_{i=1}^{n} w\Big( \frac{x - X_{i}}{h_{n}} \Big)}, 
\tag{11.3}
\end{equation}\]</span>
where <span class="math inline">\(w(t)\)</span> is the “box” function defined as
<span class="math display">\[\begin{equation}
w(t) = 
\begin{cases}
\frac{1}{2} &amp; \textrm{ if } |t| &lt; 1 \nonumber \\
0 &amp; \textrm{ otherwise}  \nonumber
\end{cases}
\end{equation}\]</span></p></li>
<li><p>While a local average estimate will not be a “step function” like the regressogram, the local average
estimate will typically be non-smooth and have a jagged appearance.</p></li>
</ul>
<hr />
<ul>
<li><p>Like kernel density estimation, there is a bias/variance tradeoff to the choice of <span class="math inline">\(h_{n}\)</span>.</p></li>
<li><p>Smaller values of <span class="math inline">\(h_{n}\)</span> usually imply higher variance because you will be taking an
average over a relatively small number of observations.</p></li>
<li><p>Larger values of <span class="math inline">\(h_{n}\)</span> usually imply higher bias because
you will be esitmating <span class="math inline">\(m(x)\)</span> by averaging over a wide range
of <span class="math inline">\(x_{i}\)</span> values, and <span class="math inline">\(m(x)\)</span> could vary substantially over this range of <span class="math inline">\(x_{i}\)</span> values.</p></li>
<li><p>Our experience in Chapter 8 suggests that we can get a smoother estimate of
the regression if we simply replace the “box function” <span class="math inline">\(w(t)\)</span> in <a href="kernel-regression-and-local-regression.html#eq:local-average-estimator">(11.3)</a> with a
smoother kernel function <span class="math inline">\(K(t)\)</span>.</p></li>
</ul>
<hr />
<ul>
<li><code>R</code> code for computing a local average estimate <span class="math inline">\(\hat{m}_{2}^{loc}(x)\)</span> at the
points <span class="math inline">\(x = 31, 32, 33, ...., 71\)</span> is given below</li>
</ul>
<div class="sourceCode" id="cb218"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb218-1" data-line-number="1">xseq &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">31</span>, <span class="dv">71</span>, <span class="dt">by=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb218-2" data-line-number="2">hn &lt;-<span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb218-3" data-line-number="3">nx &lt;-<span class="st"> </span><span class="kw">length</span>(xseq)</a>
<a class="sourceLine" id="cb218-4" data-line-number="4">m.hat.loc &lt;-<span class="st"> </span><span class="kw">numeric</span>(nx)</a>
<a class="sourceLine" id="cb218-5" data-line-number="5"><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nx) {</a>
<a class="sourceLine" id="cb218-6" data-line-number="6">    in.bin &lt;-<span class="st"> </span>framingham<span class="op">$</span>age <span class="op">&gt;</span><span class="st"> </span>xseq[k] <span class="op">-</span><span class="st"> </span>hn <span class="op">&amp;</span><span class="st"> </span>framingham<span class="op">$</span>age <span class="op">&lt;</span><span class="st"> </span>xseq[k] <span class="op">+</span><span class="st"> </span>hn</a>
<a class="sourceLine" id="cb218-7" data-line-number="7">    m.hat.loc[k] &lt;-<span class="st"> </span><span class="kw">mean</span>(framingham<span class="op">$</span>diaBP[in.bin])</a>
<a class="sourceLine" id="cb218-8" data-line-number="8">}</a>
<a class="sourceLine" id="cb218-9" data-line-number="9"></a>
<a class="sourceLine" id="cb218-10" data-line-number="10"><span class="kw">plot</span>(framingham<span class="op">$</span>age, framingham<span class="op">$</span>diaBP, <span class="dt">las=</span><span class="dv">1</span>, <span class="dt">ylab=</span><span class="st">&quot;Diastolic Blood Pressure&quot;</span>, </a>
<a class="sourceLine" id="cb218-11" data-line-number="11">     <span class="dt">xlab=</span><span class="st">&quot;Age&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Local Average Estimate with hn=2&quot;</span>, <span class="dt">type=</span><span class="st">&quot;n&quot;</span>)</a>
<a class="sourceLine" id="cb218-12" data-line-number="12"><span class="kw">points</span>(framingham<span class="op">$</span>age, framingham<span class="op">$</span>diaBP, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">cex=</span><span class="fl">0.7</span>)</a>
<a class="sourceLine" id="cb218-13" data-line-number="13"><span class="kw">lines</span>(xseq, m.hat.loc, <span class="dt">lwd=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a></code></pre></div>
<p><img src="11-kernel-regression_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<hr />
<ul>
<li><p>Let’s also look at a local average estimate of the regression function for the bone mineral density dataset.</p></li>
<li><p>The responses in this dataset are relative changes in the bone mineral density of adolescents.</p></li>
<li><p>Specifically, reponses <span class="math inline">\(Y_{i}\)</span> and covariates <span class="math inline">\(x_{i}\)</span> are defined as
<span class="math display">\[\begin{eqnarray}
Y_{i} &amp;=&amp; \frac{\textrm{Mineral Density at Visit 2}_{i} - \textrm{Mineral Density at Visit 1}_{i}}{\tfrac{1}{2}(\textrm{Mineral Density at Visit 2}_{i} + \textrm{Mineral Density at Visit 1}_{i})}  \nonumber \\
x_{i} &amp;=&amp; \frac{1}{2}(\textrm{Age at Visit 2}_{i} + \textrm{Age at Visit 1}_{i})  \nonumber
\end{eqnarray}\]</span></p></li>
</ul>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb219-1" data-line-number="1">tmp &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&quot;https://web.stanford.edu/~hastie/ElemStatLearn/datasets/bone.data&quot;</span>, </a>
<a class="sourceLine" id="cb219-2" data-line-number="2">                  <span class="dt">header=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb219-3" data-line-number="3">bonedat &lt;-<span class="st"> </span>tmp[<span class="op">!</span><span class="kw">duplicated</span>(tmp<span class="op">$</span>idnum),]  <span class="co">## only keep the first observation of a person</span></a></code></pre></div>
<div class="sourceCode" id="cb220"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb220-1" data-line-number="1">xseq &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="fl">9.4</span>, <span class="fl">25.2</span>, <span class="dt">by=</span>.<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb220-2" data-line-number="2">hn &lt;-<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb220-3" data-line-number="3">nx &lt;-<span class="st"> </span><span class="kw">length</span>(xseq)</a>
<a class="sourceLine" id="cb220-4" data-line-number="4">m.hat.loc &lt;-<span class="st"> </span><span class="kw">numeric</span>(nx)</a>
<a class="sourceLine" id="cb220-5" data-line-number="5"><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nx) {</a>
<a class="sourceLine" id="cb220-6" data-line-number="6">    in.bin &lt;-<span class="st"> </span>bonedat<span class="op">$</span>age <span class="op">&gt;</span><span class="st"> </span>xseq[k] <span class="op">-</span><span class="st"> </span>hn <span class="op">&amp;</span><span class="st"> </span>bonedat<span class="op">$</span>age <span class="op">&lt;</span><span class="st"> </span>xseq[k] <span class="op">+</span><span class="st"> </span>hn</a>
<a class="sourceLine" id="cb220-7" data-line-number="7">    m.hat.loc[k] &lt;-<span class="st"> </span><span class="kw">mean</span>(bonedat<span class="op">$</span>spnbmd[in.bin])</a>
<a class="sourceLine" id="cb220-8" data-line-number="8">}</a>
<a class="sourceLine" id="cb220-9" data-line-number="9"></a>
<a class="sourceLine" id="cb220-10" data-line-number="10"><span class="kw">plot</span>(bonedat<span class="op">$</span>age, bonedat<span class="op">$</span>spnbmd, <span class="dt">las=</span><span class="dv">1</span>, <span class="dt">ylab=</span><span class="st">&quot;Relative Change in Bone MD&quot;</span>, </a>
<a class="sourceLine" id="cb220-11" data-line-number="11">     <span class="dt">xlab=</span><span class="st">&quot;Age&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Bone Data: Local Average Estimate with hn=1&quot;</span>, <span class="dt">type=</span><span class="st">&quot;n&quot;</span>)</a>
<a class="sourceLine" id="cb220-12" data-line-number="12"><span class="kw">points</span>(bonedat<span class="op">$</span>age, bonedat<span class="op">$</span>spnbmd, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">cex=</span><span class="fl">0.7</span>)</a>
<a class="sourceLine" id="cb220-13" data-line-number="13"><span class="kw">lines</span>(xseq, m.hat.loc, <span class="dt">lwd=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a>
<a class="sourceLine" id="cb220-14" data-line-number="14"><span class="kw">abline</span>(<span class="dv">0</span>, <span class="dv">0</span>)</a></code></pre></div>
<p><img src="11-kernel-regression_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p><img src="11-kernel-regression_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div id="k-nearest-neighbor-k-nn-regression" class="section level3">
<h3><span class="header-section-number">11.2.3</span> k-Nearest Neighbor (k-NN) Regression</h3>
<ul>
<li><p>k-nearest neighbor regression is fairly similar to the local average estimator
of the regression function.</p></li>
<li><p>With k-NN, we still estimate the regression function at a particular point
by taking a type of local average around this point.</p></li>
<li><p>However, k-NN takes the average over the k “nearest observations” to <span class="math inline">\(x\)</span>
rather than taking an average over all the observations which fall into
a bin centered at <span class="math inline">\(x\)</span>.</p></li>
</ul>
<hr />
<ul>
<li><p>The k-NN estimator of the regression function <span class="math inline">\(\hat{m}_{k}^{kNN}(x)\)</span> is defined as
<span class="math display">\[\begin{equation}
\hat{m}_{k}^{kNN}(x) = \frac{1}{k}\sum_{i=1}^{n} y_{i} I\big( x_{i} \in N_{k}(x) \big) \nonumber
\end{equation}\]</span></p></li>
<li><p>Here, <span class="math inline">\(N_{k}(x)\)</span> is defined as the set of the k <span class="math inline">\(x_{i}&#39;s\)</span> which are
closest to <span class="math inline">\(x\)</span>. That is, <span class="math inline">\(N_{k}(x)\)</span> is the set of the k “nearest neighbors”
to <span class="math inline">\(x\)</span>.</p></li>
<li><p>Mathematically, if we define
<span class="math display">\[\begin{equation}
d_{i}(x) = |x_{i} - x| \nonumber
\end{equation}\]</span>
and order them so that <span class="math inline">\(d_{(1)}(x) \leq d_{(2)}(x) \leq \ldots \leq d_{(n)}(x)\)</span>.
Then, the k nearest neighbors of <span class="math inline">\(x\)</span> would be those
observations which correspond to the <span class="math inline">\(d_{(1)}(x)\)</span> through <span class="math inline">\(d_{(k)}(x)\)</span>.</p></li>
</ul>
<hr />
<ul>
<li>Like the local average estimator, increasing the value of <span class="math inline">\(k\)</span>
will increase the bias of the k-NN regression function estimate while
decreasing the value of <span class="math inline">\(k\)</span> will increase the variance of the k-NN regression function estimate.</li>
</ul>
<hr />
<ul>
<li><strong>Exercise 8.2</strong> Suppose <span class="math inline">\(n=6\)</span> and that we have the following covariate values and responses
<span class="math display">\[\begin{eqnarray}
(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}, x_{6}) &amp;=&amp; (1/7, 2/7, 3/7, 4/7, 5/7, 6/7)  \nonumber \\
(Y_{1}, Y_{2}, Y_{3}, Y_{4}, Y_{5}, Y_{6}) &amp;=&amp; (1.4, 0.7, 1.1, 1.3, 0.9, 1.7)  \nonumber
\end{eqnarray}\]</span>
<ul>
<li>Compute the local average estimate of the regression function at <span class="math inline">\(x = 0.25\)</span> and <span class="math inline">\(x=0.75\)</span> assuming that <span class="math inline">\(h_{n} = 1/2\)</span>.</li>
<li>Compute the k nearest neighbors estimate of the regression function at <span class="math inline">\(x = 0.25\)</span> and <span class="math inline">\(x = 0.75\)</span> assuming that <span class="math inline">\(k = 2\)</span>.</li>
</ul></li>
</ul>
<hr />
</div>
<div id="the-nadaraya-watson-estimator" class="section level3">
<h3><span class="header-section-number">11.2.4</span> The Nadaraya-Watson Estimator</h3>
<ul>
<li><p>The Nadaraya-Watson estimator <span class="math inline">\(\hat{m}_{h_{n}}^{NW}\)</span> of the regression function with bandwidth <span class="math inline">\(h_{n}\)</span> is defined as
<span class="math display">\[\begin{equation}
\hat{m}_{h_{n}}^{NW}(x) = \frac{ \sum_{i=1}^{n} Y_{i}K\Big( \frac{x - x_{i}}{ h_{n} }\Big) }{ \sum_{i=1}^{n} K\Big( \frac{x - x_{i}}{ h_{n} }\Big)  } \nonumber
\end{equation}\]</span></p></li>
<li><p>The Nadaraya-Watson estimator has the same basic form as the local average estimator. We have just replaced the “box” function
<span class="math inline">\(w(t)\)</span> with the kernel function <span class="math inline">\(K(t)\)</span>.</p></li>
<li><p>You can think of <span class="math inline">\(\hat{m}_{h_{n}}^{NW}(x)\)</span> as a weighted average of the <span class="math inline">\(Y_{i}\)</span>.
That is,
<span class="math display">\[\begin{equation}
\hat{m}_{h_{n}}^{NW}(x) = \sum_{i=1}^{n} a_{i}(x) Y_{i} \nonumber
\end{equation}\]</span></p></li>
<li><p>The bandwidth <span class="math inline">\(h_{n}\)</span> can also be referred to as the “smoothing parameter” since its value affects how smooth
the fitted regression curve appears.</p></li>
<li><p>The weights <span class="math inline">\(a_{1}(x), \ldots, a_{n}(x)\)</span>, in this case, are defined as
<span class="math display">\[\begin{equation}
a_{i}(x) = \frac{ K(\tfrac{x - x_{i}}{h_{n}})}{ \sum_{i=1}^{n} K(\tfrac{x - x_{i}}{ h_{n}}) } \nonumber
\end{equation}\]</span>
So, we are using weights which are larger the closer you are to <span class="math inline">\(x\)</span>.</p></li>
</ul>
<hr />
<ul>
<li><p>The Nadaraya-Watson estimator suffers from two main drawbacks. These are
<strong>design bias</strong> and <strong>boundary bias</strong>.</p></li>
<li><p>Design bias refers to the effect of the spacing of the <span class="math inline">\(x_{i}\)</span> on the performance
of the Nadaraya-Watson estimator.</p></li>
<li><p>Boundary bias refers to the performance of the Nadaraya-Watson estimator
near the smallest and largest <span class="math inline">\(x_{i}\)</span>.</p></li>
</ul>
<hr />
<ul>
<li><p>If we assume that the <span class="math inline">\(x_{i}\)</span> are random and have probability density <span class="math inline">\(f_{X}(x)\)</span>, then it can be shown
that the mean-squared error of the Nadaraya-Watson estimator at a particular point <span class="math inline">\(x\)</span> has
the following approximation
<span class="math display">\[\begin{eqnarray}
\textrm{MSE}(x) &amp;=&amp; E\Big[ \{ m(x) - \hat{m}_{h_{n}}^{NW}(x) \}^{2} \Big] \nonumber \\
&amp;\approx&amp; \frac{h_{n}^{4}\mu_{2}^{2}(K)}{4}\Bigg\{ m&#39;&#39;(x) + \frac{2m&#39;(x)f_{X}&#39;(x)}{f_{X}(x)} \Bigg\}^{2} + \frac{\sigma^{2}}{n h_{n} f_{X}(x) }, \nonumber
\end{eqnarray}\]</span>
where <span class="math inline">\(\mu_{2}(K) = \int_{-\infty}^{\infty} u^{2} K(u) du\)</span> and <span class="math inline">\(\kappa_{2}(K) = \int_{-\infty}^{\infty} K^{2}(u) du\)</span>.</p></li>
<li><p>The term <span class="math inline">\(2m&#39;(x)f_{X}&#39;(x)/f_{X}(x)\)</span> is referred to as the design bias. Notice that this should be zero if
the <span class="math inline">\(x_{i}\)</span> are drawn from a Uniform distribution. In other words, if the <span class="math inline">\(x_{i}\)</span> are roughly equally spaced, then
the design bias should be small.</p></li>
</ul>
<hr />
<p><strong>The Nadaraya-Watson estimator in R</strong></p>
<ul>
<li>The Nadaraya-Watson estimator can be computed in <code>R</code> with the <code>ksmooth</code> function.</li>
</ul>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb221-1" data-line-number="1"><span class="kw">ksmooth</span>(x, y, kernel, bandwidth, x.points, ...)</a></code></pre></div>
<ul>
<li><ul>
<li><code>x</code> - vector of covariate values</li>
<li><code>y</code> - vector of responses</li>
<li><code>kernel</code> - choice of kernel function; default is <code>box</code>; use <code>normal</code> if you want a Gaussian kernel</li>
<li><code>bandwidth</code> - value of the bandwidth; default is <span class="math inline">\(0.5\)</span></li>
<li><code>x.points</code> - points at which to estimate the regression function; default is to use <span class="math inline">\(n\)</span> equally spaced points.</li>
</ul></li>
<li><p>The <code>x</code> vector from the fitted <code>ksmooth</code> object will be the vector of points at which the regression function
is estimated. The <code>y</code> vector from the fitted <code>ksmooth</code> object will be a vector containing the estimated values of the regression function.</p></li>
<li><p>Note that the bandwidth used by this function for the Gaussian kernel is approximately <span class="math inline">\(2.7\)</span> times smaller than the bandwith in our definition of the Gaussian kernel.</p></li>
</ul>
<hr />
<ul>
<li>If you wanted to write your own function that computed the Nadaraya-Watson estimate
at a vector of desired points <span class="math inline">\(x.points = (t_{1}, \ldots, t_{q})\)</span>, you could use
something like</li>
</ul>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb222-1" data-line-number="1">MyNWEst &lt;-<span class="st"> </span><span class="cf">function</span>(x, y, bandwidth, x.points) {</a>
<a class="sourceLine" id="cb222-2" data-line-number="2">    q &lt;-<span class="st"> </span><span class="kw">length</span>(x.points) </a>
<a class="sourceLine" id="cb222-3" data-line-number="3">    nw.est &lt;-<span class="st"> </span><span class="kw">numeric</span>(q)</a>
<a class="sourceLine" id="cb222-4" data-line-number="4">    <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>q) {</a>
<a class="sourceLine" id="cb222-5" data-line-number="5">        ww &lt;-<span class="st"> </span><span class="kw">dnorm</span>(x.points[k], <span class="dt">mean=</span>x, <span class="dt">sd=</span>bandwidth) </a>
<a class="sourceLine" id="cb222-6" data-line-number="6">        nw.est[k] &lt;-<span class="st"> </span><span class="kw">sum</span>(ww<span class="op">*</span>y)<span class="op">/</span><span class="kw">sum</span>(ww)</a>
<a class="sourceLine" id="cb222-7" data-line-number="7">    }    </a>
<a class="sourceLine" id="cb222-8" data-line-number="8">    <span class="kw">return</span>(nw.est)</a>
<a class="sourceLine" id="cb222-9" data-line-number="9">}</a></code></pre></div>
<hr />
<ul>
<li>To compute the Nadraya-Watson estimate at a set of equally spaced of points from <span class="math inline">\(10\)</span> to <span class="math inline">\(25\)</span>
using bandwidth <span class="math inline">\(0.5\)</span> and plot the result, you could use the following code:</li>
</ul>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb223-1" data-line-number="1">xseq &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">10</span>, <span class="dv">25</span>, <span class="dt">by=</span>.<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb223-2" data-line-number="2">bone.nwest &lt;-<span class="st"> </span><span class="kw">ksmooth</span>(<span class="dt">x=</span>bonedat<span class="op">$</span>age, <span class="dt">y=</span>bonedat<span class="op">$</span>spnbmd, <span class="dt">kernel=</span><span class="st">&quot;normal&quot;</span>, </a>
<a class="sourceLine" id="cb223-3" data-line-number="3">                      <span class="dt">bandwidth=</span><span class="fl">2.7</span><span class="op">*</span><span class="fl">0.5</span>, <span class="dt">x.points=</span>xseq)</a>
<a class="sourceLine" id="cb223-4" data-line-number="4"></a>
<a class="sourceLine" id="cb223-5" data-line-number="5"><span class="kw">plot</span>(bonedat<span class="op">$</span>age, bonedat<span class="op">$</span>spnbmd, <span class="dt">las=</span><span class="dv">1</span>, <span class="dt">ylab=</span><span class="st">&quot;Relative Change in Bone MD&quot;</span>, </a>
<a class="sourceLine" id="cb223-6" data-line-number="6">     <span class="dt">xlab=</span><span class="st">&quot;Age&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Bone Data: Nadaraya-Watson Estimate with hn=0.5 and </span></a>
<a class="sourceLine" id="cb223-7" data-line-number="7"><span class="st">     Gaussian Kernel&quot;</span>, <span class="dt">type=</span><span class="st">&quot;n&quot;</span>)</a>
<a class="sourceLine" id="cb223-8" data-line-number="8"><span class="kw">points</span>(bonedat<span class="op">$</span>age, bonedat<span class="op">$</span>spnbmd, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">cex=</span><span class="fl">0.7</span>)</a>
<a class="sourceLine" id="cb223-9" data-line-number="9"><span class="kw">lines</span>(bone.nwest<span class="op">$</span>x, bone.nwest<span class="op">$</span>y, <span class="dt">lwd=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a></code></pre></div>
<p><img src="11-kernel-regression_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<div class="sourceCode" id="cb224"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb224-1" data-line-number="1"><span class="co">## Note that bone.nwest$x should equal xseq</span></a></code></pre></div>
</div>
</div>
<div id="local-linear-regression" class="section level2">
<h2><span class="header-section-number">11.3</span> Local Linear Regression</h2>
<div id="definition-7" class="section level3">
<h3><span class="header-section-number">11.3.1</span> Definition</h3>
<ul>
<li><p>You can think of both the regressogram and the local average as methods
which fit local intercept models.</p></li>
<li><p>For the regressogram, we fit an intercept model (that is a flat line curve) within
a small bin that contains <span class="math inline">\(x\)</span>.</p></li>
<li><p>For the local average estimator, we fit an intercept model for a small
bin around <span class="math inline">\(x\)</span>.</p></li>
<li><p>The Nadaraya-Watson estimator can be thought of as just smoothing out the
local intercept approach of the local average estimator.</p></li>
</ul>
<hr />
<ul>
<li><p>Instead of fitting local intercept models, we could fit local
linear models that have an intercept and a slope term.</p></li>
<li><p>To be specific, suppose we estimated the regression function at <span class="math inline">\(x\)</span>
by fitting a linear model using only data from the <span class="math inline">\(x_{i}\)</span> that fell into the bin
<span class="math inline">\((x - h_{n}, x + h_{n})\)</span>.</p></li>
<li><p>In this case, we would first fit the linear model
<span class="math inline">\(\hat{s}_{x}(x_{i}) = \hat{\beta}_{0x} + \hat{\beta}_{1x}(x_{i} - x)\)</span> where <span class="math inline">\(\hat{\beta}_{0x}\)</span>, <span class="math inline">\(\hat{\beta}_{1x}\)</span>
solved the following local least-squares problem
<span class="math display" id="eq:simple-loclin-regression">\[\begin{equation}
\hat{\beta}_{0x}, \hat{\beta}_{1x} \textrm{ minimize: } \quad \sum_{i=1}^{n}\{ Y_{i} - \beta_{0x} - \beta_{1x}(x_{i} - x) \}^{2}I\big( x - h_{n} &lt; x_{i} &lt; x + h_{n} \big)  
\tag{11.4}
\end{equation}\]</span></p></li>
<li><p>Then, we would estimate <span class="math inline">\(m(x)\)</span> by using the value of <span class="math inline">\(\hat{s}_{x}(\cdot)\)</span> at <span class="math inline">\(x\)</span>. That is, <span class="math inline">\(\hat{s}_{x}(x) = \hat{\beta}_{0x}\)</span>.</p></li>
</ul>
<hr />
<ul>
<li><p><strong>Local Linear Regression</strong> uses the same idea as <a href="kernel-regression-and-local-regression.html#eq:simple-loclin-regression">(11.4)</a>, but replaces the indicator function <span class="math inline">\(I( x - h_{n} &lt; x_{i} &lt; x + h_{n})\)</span> with a smooth kernel function.</p></li>
<li><p>So, the local linear regression estimate of the regression function at <span class="math inline">\(x\)</span> is
<span class="math display">\[\begin{eqnarray}
\hat{m}_{h_{n}}^{loclin}(x) &amp;=&amp; \hat{\beta}_{0x} \quad \textrm{ where } \nonumber \\
\hat{\beta}_{0x}, \hat{\beta}_{1x} &amp;=&amp;  \textrm{argmin}_{\beta_{0x},\beta_{1x}} \sum_{i=1}^{n}\{ Y_{i} - \beta_{0x} - \beta_{1x}(x_{i} - x) \}^{2}K\Big( \frac{x - x_{i}}{h_{n}} \Big) \nonumber
\end{eqnarray}\]</span></p></li>
</ul>
<hr />
<ul>
<li><strong>Exercise 8.3</strong> Suppose we define an estimator <span class="math inline">\(\tilde{m}_{h_{n}}(x)\)</span> of the regression function as
<span class="math display">\[\begin{eqnarray}
\tilde{m}_{h_{n}}(x) &amp;=&amp; \hat{\beta}_{0x} \quad \textrm{ where } \nonumber \\
\hat{\beta}_{0x} &amp;=&amp;  \textrm{argmin}_{\beta_{0x}} \sum_{i=1}^{n}\{ Y_{i} - \beta_{0x} \}^{2}K\Big( \frac{x - x_{i}}{h_{n}} \Big) \nonumber
\end{eqnarray}\]</span>
Show that <span class="math inline">\(\tilde{m}_{h_{n}}(x) = \hat{m}_{h_{n}}^{NW}(x)\)</span>.</li>
</ul>
<hr />
</div>
<div id="advantages-of-the-local-linear-estimator" class="section level3">
<h3><span class="header-section-number">11.3.2</span> Advantages of the Local Linear Estimator</h3>
<ul>
<li><p>The local linear regression estimator can reduce the effects of design and boundary bias.</p></li>
<li><p>If we write the local linear estimate at the point <span class="math inline">\(x\)</span> as
<span class="math inline">\(\hat{m}_{h_{n}}^{loclin}(x) = \sum_{i=1}^{n} a_{i}^{h_{n}}(x)Y_{i}\)</span>, then the bias is appoximately
<span class="math display">\[\begin{equation}
E\{ \hat{m}_{h_{n}}^{loclin}(x) \} - m(x)
\approx m&#39;(x)\sum_{i=1}^{n} (x_{i} - x)a_{i}^{h_{n}}(x) + \frac{m&#39;&#39;(x)}{2}\sum_{i=1}^{n} (x_{i} - x)^{2}a_{i}^{h_{n}}(x) \nonumber 
\end{equation}\]</span></p></li>
<li><p>For local linear regression, the term <span class="math inline">\(m&#39;(x)\sum_{i=1}^{n} (x_{i} - x)a_{i}^{h_{n}}(x)\)</span> equals zero. If
the weights <span class="math inline">\(a_{i}^{h_{n}}(x)\)</span> were the weights from the Nadaraya-Watson estimator, this term would not necessarily
equal zero.</p></li>
<li><p>Also, the local linear estimator can help to reduce the boundary bias that arises
from asymmetry near the boundary (draw a picture).</p></li>
</ul>
<hr />
</div>
<div id="an-example-in-r" class="section level3">
<h3><span class="header-section-number">11.3.3</span> An Example in R</h3>
<ul>
<li>An <code>R</code> function which implements local linear regression is the following. The
input for this function has the same structure as our earlier Nadaraya-Watson <code>R</code> function.</li>
</ul>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb225-1" data-line-number="1">MyLocLinear &lt;-<span class="st"> </span><span class="cf">function</span>(x, y, bandwidth, x.points) {</a>
<a class="sourceLine" id="cb225-2" data-line-number="2">  q &lt;-<span class="st"> </span><span class="kw">length</span>(x.points) </a>
<a class="sourceLine" id="cb225-3" data-line-number="3">  loclin.est &lt;-<span class="st"> </span><span class="kw">numeric</span>(q)</a>
<a class="sourceLine" id="cb225-4" data-line-number="4">  <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>q) {</a>
<a class="sourceLine" id="cb225-5" data-line-number="5">    <span class="co">## First create weights with Gaussian kernel</span></a>
<a class="sourceLine" id="cb225-6" data-line-number="6">    xtmp &lt;-<span class="st"> </span>x <span class="op">-</span><span class="st"> </span>x.points[k]</a>
<a class="sourceLine" id="cb225-7" data-line-number="7">    ww &lt;-<span class="st"> </span><span class="kw">dnorm</span>(xtmp, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span>bandwidth) </a>
<a class="sourceLine" id="cb225-8" data-line-number="8">    </a>
<a class="sourceLine" id="cb225-9" data-line-number="9">    <span class="co">## Now, compute the intercept with a weighted linear regression</span></a>
<a class="sourceLine" id="cb225-10" data-line-number="10">    loclin.est[k] &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>xtmp, <span class="dt">weights=</span>ww)<span class="op">$</span>coef[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb225-11" data-line-number="11">  }    </a>
<a class="sourceLine" id="cb225-12" data-line-number="12">  <span class="kw">return</span>(loclin.est)</a>
<a class="sourceLine" id="cb225-13" data-line-number="13">}</a></code></pre></div>
<ul>
<li><p>Let’s try this function with the <code>bonedat</code> dataset again.</p></li>
<li><p>Using age as the covariate, we will estimate the regression function at the points <span class="math inline">\(10, 10.1, 10.2, ..., 25\)</span>:</p></li>
</ul>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb226-1" data-line-number="1">xseq &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">10</span>, <span class="dv">25</span>, <span class="dt">by=</span>.<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb226-2" data-line-number="2">bone.loclin &lt;-<span class="st"> </span><span class="kw">MyLocLinear</span>(<span class="dt">x=</span>bonedat<span class="op">$</span>age, <span class="dt">y=</span>bonedat<span class="op">$</span>spnbmd, </a>
<a class="sourceLine" id="cb226-3" data-line-number="3">                           <span class="dt">bandwidth=</span><span class="fl">0.5</span>, <span class="dt">x.points=</span>xseq)</a>
<a class="sourceLine" id="cb226-4" data-line-number="4"></a>
<a class="sourceLine" id="cb226-5" data-line-number="5"><span class="kw">plot</span>(bonedat<span class="op">$</span>age, bonedat<span class="op">$</span>spnbmd, <span class="dt">las=</span><span class="dv">1</span>, <span class="dt">ylab=</span><span class="st">&quot;Relative Change in Bone MD&quot;</span>, </a>
<a class="sourceLine" id="cb226-6" data-line-number="6">     <span class="dt">xlab=</span><span class="st">&quot;Age&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Local Linear Estimator with hn=0.5&quot;</span>, <span class="dt">type=</span><span class="st">&quot;n&quot;</span>)</a>
<a class="sourceLine" id="cb226-7" data-line-number="7"><span class="kw">points</span>(bonedat<span class="op">$</span>age, bonedat<span class="op">$</span>spnbmd, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">cex=</span><span class="fl">0.7</span>)</a>
<a class="sourceLine" id="cb226-8" data-line-number="8"><span class="kw">lines</span>(xseq, bone.loclin, <span class="dt">lwd=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a></code></pre></div>
<p><img src="11-kernel-regression_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<ul>
<li>Let’s compare this with the Nadaraya-Watson esitmate that we computed earlier
<img src="11-kernel-regression_files/figure-html/unnamed-chunk-12-1.png" width="672" /></li>
</ul>
</div>
<div id="local-polynomial-regression" class="section level3">
<h3><span class="header-section-number">11.3.4</span> Local Polynomial Regression</h3>
<ul>
<li><p>There is no reason why we must restrict ourselves to local linear fits. We
could also fit local polynomial models.</p></li>
<li><p>Similar to the way we approached local linear regression, for a fixed <span class="math inline">\(x\)</span>
we will fit the local model
<span class="math display">\[\begin{equation}
\hat{s}_{x}^{p}(x_{i}) = \hat{\beta}_{0x,p} + \hat{\beta}_{1x,p}(x_{i} - x) + \hat{\beta}_{2x,p}(x_{i} - x)^{2} + \ldots + \beta_{px,p}(x_{i} - x)^{p}, \nonumber
\end{equation}\]</span>
where the estimated regression coefficients <span class="math inline">\(\hat{\beta}_{0x,p}, \hat{\beta}_{1x,p}, \ldots, \hat{\beta}_{px,p}\)</span> are found
by solving the least-squares problem
<span class="math display">\[\begin{equation}
\sum_{i=1}^{n}\{ Y_{i} - \beta_{0x,p} - \beta_{1x,p}(x_{i} - x) - \ldots - \beta_{px,p}(x_{i} - x)^{p} \}^{2}K\Big( \frac{x - x_{i}}{h_{n}} \Big) 
\end{equation}\]</span></p></li>
<li><p>Then, we estimate the regression function at <span class="math inline">\(x\)</span> with <span class="math inline">\(\hat{m}_{h_{n}}^{locpoly}(x) = \hat{s}_{x}^{p}(x) = \hat{\beta}_{0x,p}\)</span>.</p></li>
<li><p>Note that the local linear regression estimate is just a special case of local polynomial regression with <span class="math inline">\(p=1\)</span>.</p></li>
</ul>
<hr />
<ul>
<li><p>To find the estimates of <span class="math inline">\(\beta_{0x,p}\)</span> for linear and polynomial regression, you can use the
formulas for the regression coefficient estimates in weighted least squares.</p></li>
<li><p>Define the <span class="math inline">\(n \times n\)</span> diagonal matrix of weights <span class="math inline">\(\mathbf{W}_{x, h_{n}}\)</span> as
<span class="math display">\[\begin{equation}
\mathbf{W}_{x, h_{n}}
= \begin{bmatrix} K\Big( \frac{x - x_{1}}{h_{n}} \Big) &amp; 0 &amp; \ldots &amp; 0 \\
0 &amp; K\Big( \frac{x - x_{2}}{h_{n}} \Big) &amp; \ldots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \ldots &amp; K\Big( \frac{x - x_{n}}{h_{n}} \Big)
\end{bmatrix} \nonumber
\end{equation}\]</span>
and define the <span class="math inline">\(n \times (p+1)\)</span> matrix <span class="math inline">\(\mathbf{X}_{x,p}\)</span> as
<span class="math display">\[\begin{equation}
\mathbf{X}_{x, p} = \begin{bmatrix} 1 &amp; (x_{1} - x) &amp; \ldots &amp; (x_{1} - x)^{p} \\ 
1 &amp; (x_{2} - x) &amp; \ldots &amp; (x_{2} - x)^{p} \\
\vdots &amp; \vdots &amp;  &amp; \vdots \\
1 &amp; (x_{n} - x) &amp; \ldots &amp; (x_{n} - x)^{p}
\end{bmatrix}  \nonumber
\end{equation}\]</span></p></li>
<li><p>The vector of estimated regression coefficients is obtained from the following formula
<span class="math display">\[\begin{equation}
\begin{bmatrix}
\hat{\beta}_{0x,p} \\ \hat{\beta}_{1x,p} \\ \ldots \\ \hat{\beta}_{px,p}
\end{bmatrix}
= (\mathbf{X}_{x,p}^{T}\mathbf{W}_{x,h_{n}}\mathbf{X}_{x,p})^{-1}\mathbf{X}_{x,p}^{T}\mathbf{W}_{x,h_{n}}\mathbf{Y}  \nonumber
\end{equation}\]</span></p></li>
</ul>
<hr />
<ul>
<li><p>While using local polynomial regression with higher order polynomials offer more flexibility, they come
at the price of more variance.</p></li>
<li><p>For fixed <span class="math inline">\(h_{n}\)</span>, increasing the degree <span class="math inline">\(p\)</span> can decrease bias but will increase variance.</p></li>
<li><p>In practice, <span class="math inline">\(p = 1\)</span> or <span class="math inline">\(p = 2\)</span> seems to be most common in practice. There is often not
much benefit to using a degree of <span class="math inline">\(3\)</span> or more.</p></li>
</ul>
</div>
</div>
<div id="selecting-the-bandwidthsmoothing-parameter" class="section level2">
<h2><span class="header-section-number">11.4</span> Selecting the Bandwidth/Smoothing Parameter</h2>
<div id="representing-in-linear-form" class="section level3">
<h3><span class="header-section-number">11.4.1</span> Representing in Linear Form</h3>
<ul>
<li><p>Let <span class="math inline">\(\mathbf{Y} = (Y_{1}, \ldots, Y_{n})\)</span> and let <span class="math inline">\(\hat{\mathbf{m}} = (\hat{m}(x_{1}), \ldots, \hat{m}(x_{n}))\)</span> denote
the vector of “fitted values” from a vector of estimates of the regression function at <span class="math inline">\(x_{1}, \ldots, x_{n}\)</span>.</p></li>
<li><p>You can represent the fitted values all of the nonparametric estimators discussed thus far as
<span class="math display">\[\begin{equation}
\hat{\mathbf{m}} = \mathbf{A}_{h_{n}}\mathbf{Y} \nonumber
\end{equation}\]</span>
for an appropriately chosen <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(\mathbf{A}_{h_{n}}\)</span>.</p></li>
</ul>
<hr />
<ul>
<li><p>For the local average estimator, we have <span class="math inline">\(\hat{\mathbf{m}} = \mathbf{A}_{h_{n}}\mathbf{Y}\)</span> where
<span class="math inline">\(\mathbf{A}_{h_{n}}\)</span> is defined as
<span class="math display">\[\begin{equation}
\mathbf{A}_{h_{n}}
= \begin{bmatrix} \frac{1}{n_{h_{n}}(x_{1})}I(x_{1} - h_{n} &lt; x_{1} &lt; x_{1} + h_{n}) &amp; \ldots &amp;  \frac{1}{n_{h_{n}}(x_{1})}I(x_{1} - h_{n} &lt; x_{n} &lt; x_{1} + h_{n}) \\
\frac{1}{n_{h_{n}}(x_{2})}I(x_{2} - h_{n} &lt; x_{1} &lt; x_{2} + h_{n}) &amp; \ldots &amp;  \frac{1}{n_{h_{n}}(x_{2})}I(x_{2} - h_{n} &lt; x_{n} &lt; x_{2} + h_{n}) \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{1}{n_{h_{n}}(x_{n})}I(x_{n} - h_{n} &lt; x_{1} &lt; x_{n} + h_{n}) &amp; \ldots &amp;  \frac{1}{n_{h_{n}}(x_{n})}I(x_{n} - h_{n} &lt; x_{n} &lt; x_{n} + h_{n})
\end{bmatrix}
\end{equation}\]</span>
where <span class="math inline">\(n_{h_{n}}(x) = \sum_{i=1}^{n}I(x - h_{n} &lt; x_{i} &lt; x + h_{n})\)</span>.</p></li>
<li><p>In other words, the <span class="math inline">\((i,j)\)</span> element of <span class="math inline">\(\mathbf{A}_{h_{n}}\)</span> is <span class="math inline">\(a_{j}(x_{i})\)</span> where
<span class="math display">\[\begin{equation}
a_{j}(x_{i}) = \frac{1}{n_{h_{n}}(x_{i})}I(x_{i} - h_{n} &lt; x_{j} &lt; x_{i} + h_{n})  \nonumber 
\end{equation}\]</span></p></li>
</ul>
<hr />
<ul>
<li>For the Nadaraya-Watson estimator, the <span class="math inline">\(\mathbf{A}_{h_{n}}\)</span> matrix is
<span class="math display">\[\begin{equation}
\mathbf{A}_{h_{n}}
= \begin{bmatrix} \frac{1}{K_{h_{n}}(x_{1}, \cdot)  }K(0) &amp;  \frac{1}{K_{h_{n}}(x_{1}, \cdot)  }K(\tfrac{x_{2} - x_{1}}{h_{n}}) &amp; \ldots &amp;  \frac{1}{K_{h_{n}}(x_{1}, \cdot)  }K(\tfrac{x_{n} - x_{1}}{h_{n}}) \\
\frac{1}{K_{h_{n}}(x_{2}, \cdot)  }K(\tfrac{x_{1} - x_{2}}{h_{n}}) &amp;  \frac{1}{K_{h_{n}}(x_{2}, \cdot)  }K(0) &amp; \ldots &amp;  \frac{1}{K_{h_{n}}(x_{2}, \cdot)  }K(\tfrac{x_{n} - x_{2}}{h_{n}}) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{1}{K_{h_{n}}(x_{n}, \cdot)  }K(\tfrac{x_{1}-x_{n}}{h_{n}}) &amp;  \frac{1}{K_{h_{n}}(x_{n}, \cdot)  }K(\tfrac{x_{2} - x_{n}}{h_{n}}) &amp; \ldots &amp;  \frac{1}{K_{h_{n}}(x_{n}, \cdot)  }K(0)
\end{bmatrix} \nonumber
\end{equation}\]</span>
where
<span class="math display">\[\begin{equation}
K_{h_{n}}(x_{i}, \cdot) = \sum_{j=1}^{n}K\Big( \frac{x_{i} - x_{j}}{h_{n}}  \Big) \nonumber
\end{equation}\]</span></li>
</ul>
<hr />
<ul>
<li>For the local linear regression estimator, the <span class="math inline">\(i^{th}\)</span> row of <span class="math inline">\(\mathbf{A}_{h_{n}}\)</span> equals the first row
of the following <span class="math inline">\(2 \times n\)</span> matrix:
<span class="math display">\[\begin{equation}
(\mathbf{X}_{x_{i},1}^{T}\mathbf{W}_{x_{i},h_{n}}\mathbf{X}_{x_{i},1})^{-1}\mathbf{X}_{x_{i},1}^{T}\mathbf{W}_{x_{i},h_{n}}  \nonumber
\end{equation}\]</span>
Here, <span class="math inline">\(\mathbf{X}_{x,1}\)</span> and <span class="math inline">\(\mathbf{W}_{x, h_{n}}\)</span> are as defined in the section on local linear regression.</li>
</ul>
<hr />
<ul>
<li><p>In “classic” linear regression where you would try to fit the straight line model
<span class="math inline">\(Y_{i} = \beta_{0} + \beta_{1}x_{i} + \varepsilon_{i}\)</span>, the vector of fitted values would be
<span class="math display">\[\begin{equation}
\hat{\mathbf{m}} = (\hat{m}(x_{1}), \ldots, \hat{m}(x_{n}))
= (\hat{\beta}_{0} + \hat{\beta}_{1}x_{1}, \ldots, \hat{\beta}_{0} + \hat{\beta}_{1}x_{n})  \nonumber
\end{equation}\]</span></p></li>
<li><p>In this case, you can represent <span class="math inline">\(\hat{\mathbf{m}}\)</span> as
<span class="math display">\[\begin{equation}
\hat{\mathbf{m}} = \mathbf{X}(\mathbf{X}^{T}\mathbf{X})^{-1}\mathbf{X}^{T}\mathbf{Y}  \nonumber
\end{equation}\]</span>
where <span class="math inline">\(\mathbf{X}\)</span> is the following <span class="math inline">\(n \times 2\)</span> “design” matrix
<span class="math display">\[\begin{equation}
\mathbf{X} = \begin{bmatrix} 1 &amp; x_{1} \\ 1 &amp; x_{2} \\ \vdots &amp; \vdots \\ 1 &amp; x_{n} \end{bmatrix} \nonumber
\end{equation}\]</span></p></li>
</ul>
</div>
<div id="the-cp-statistic" class="section level3">
<h3><span class="header-section-number">11.4.2</span> The Cp Statistic</h3>
<ul>
<li><p><strong>Theorem:</strong> If a random vector <span class="math inline">\(\mathbf{Z}\)</span> has mean vector <span class="math inline">\(\mathbf{\mu}\)</span> and covariance matrix <span class="math inline">\(\mathbf{\Sigma}\)</span>,
then
<span class="math display">\[\begin{equation}
E\{ \mathbf{Z}^{T}\mathbf{Z} \} = E\Big\{ \sum_{i=1}^{n} Z_{i}^{2} \Big\} = \mathbf{\mu}^{T}\mathbf{\mu} + \textrm{tr}( \mathbf{\Sigma} ) \nonumber
\end{equation}\]</span></p></li>
<li><p>Notice that the vector <span class="math inline">\(\mathbf{m} - \mathbf{A}_{h_{n}}\mathbf{Y}\)</span> has
<span class="math display">\[\begin{equation}
E( \mathbf{m} - \mathbf{A}_{h_{n}}\mathbf{Y} ) = (\mathbf{I} - \mathbf{A}_{h_{n}})\mathbf{m}  \qquad \qquad \textrm{Var}(\mathbf{m} - \mathbf{A}_{h_{n}}\mathbf{Y}) = \sigma^{2}\mathbf{A}_{h_{n}}\mathbf{A}_{h_{n}}^{T} \nonumber
\end{equation}\]</span></p></li>
<li><p>Also, the vector <span class="math inline">\(\mathbf{Y} - \mathbf{A}_{h_{n}}\mathbf{Y} = (\mathbf{I} - \mathbf{A}_{h_{n}})\mathbf{Y}\)</span> has
<span class="math display">\[\begin{equation}
E\{ (\mathbf{I} - \mathbf{A}_{h_{n}})\mathbf{Y} \} = (\mathbf{I} - \mathbf{A}_{h_{n}})\mathbf{m}  \qquad \qquad \textrm{Var}\{ (\mathbf{I} - \mathbf{A}_{h_{n}})\mathbf{Y}) = \sigma^{2} (\mathbf{I} - \mathbf{A}_{h_{n}})(\mathbf{I} - \mathbf{A}_{h_{n}})^{T} \nonumber
\end{equation}\]</span></p></li>
</ul>
<hr />
<ul>
<li><p>Ideally, we would like to choose the smoothing parameter <span class="math inline">\(h_{n}\)</span> to minimize the following mean averaged squared error
<span class="math display">\[\begin{eqnarray}
\textrm{MASE}(h_{n}) &amp;=&amp; \frac{1}{n}\sum_{i=1}^{n} E\Big[ \{ m(x_{i}) - \hat{m}(x_{i}) \}^{2} \Big]  \nonumber \\
&amp;=&amp; E\{ \frac{1}{n}[\mathbf{m} - \mathbf{A}_{h_{n}}\mathbf{Y}]^{T}[\mathbf{m} - \mathbf{A}_{h_{n}}\mathbf{Y}] \} \nonumber
\end{eqnarray}\]</span></p></li>
<li><p>If we apply the above Theorem to the vector <span class="math inline">\(\mathbf{m} - \mathbf{A}_{h_{n}}\mathbf{Y}\)</span>, we can notice that
<span class="math display">\[\begin{eqnarray}
\textrm{MASE}( h_{n} ) &amp;=&amp; E\{ \frac{1}{n}( \mathbf{m} - \mathbf{A}_{h_{n}}\mathbf{Y} )^{T}(\mathbf{m} - \mathbf{A}_{h_{n}}\mathbf{Y}) \}  \nonumber \\
&amp;=&amp; \frac{1}{n}[(\mathbf{I} - \mathbf{A}_{h_{n}})\mathbf{m}]^{T}[(\mathbf{I} - \mathbf{A}_{h_{n}})\mathbf{m}] + \frac{\sigma^{2}}{n}\textrm{tr}(\mathbf{A}_{h_{n}}\mathbf{A}_{h_{n}}^{T})
\end{eqnarray}\]</span></p></li>
<li><p>Now, using the mean and covariance matrix for <span class="math inline">\((\mathbf{I} - \mathbf{A}_{h_{n}})\mathbf{Y}\)</span>, we also have that
<span class="math display">\[\begin{eqnarray}
&amp;&amp; E\{\frac{1}{n}[(\mathbf{I} - \mathbf{A}_{h_{n}})\mathbf{Y}]^{T}[(\mathbf{I} - \mathbf{A}_{h_{n}})\mathbf{Y}] \} \nonumber \\
&amp;=&amp; \frac{1}{n}[(\mathbf{I} - \mathbf{A}_{h_{n}})\mathbf{m}]^{T}[(\mathbf{I} - \mathbf{A}_{h_{n}})\mathbf{m}] + \frac{\sigma^{2}}{n}\textrm{tr}\{ (\mathbf{I} - \mathbf{A}_{h_{n}})(\mathbf{I} - \mathbf{A}_{h_{n}})^{T} \} \nonumber \\
&amp;=&amp; \textrm{MASE}(h_{n}) + \sigma^{2} - \frac{2\sigma^{2}}{n}\textrm{tr}( \mathbf{A}_{h_{n}})
\end{eqnarray}\]</span></p></li>
<li><p>So, if <span class="math inline">\(\sigma^{2}\)</span> is known, then <span class="math inline">\(\widehat{\textrm{MASE}}( h_{n} )\)</span> is an unbiased estimate of <span class="math inline">\(\textrm{MASE}( h_{n} )\)</span>
<span class="math display">\[\begin{eqnarray}
\widehat{\textrm{MASE}}( h_{n} ) &amp;=&amp; \frac{1}{n}[(\mathbf{I} - \mathbf{A}_{h_{n}})\mathbf{Y}]^{T}[(\mathbf{I} - \mathbf{A}_{h_{n}})\mathbf{Y}] - \sigma^{2} + \frac{2\sigma^{2}}{n}\textrm{tr}( \mathbf{A}_{h_{n}}) \nonumber \\
&amp;=&amp; \frac{1}{n}\sum_{i=1}^{n} \{ Y_{i} - \hat{m}_{h_{n}}(x_{i}) \}^{2} - \sigma^{2} + \frac{2\sigma^{2}}{n}\textrm{tr}( \mathbf{A}_{h_{n}})  \nonumber
\end{eqnarray}\]</span></p></li>
</ul>
<hr />
<ul>
<li><p>The predictive mean averaged squared error (PMASE) is defined as
<span class="math display">\[\begin{equation}
\textrm{PMASE}(h_{n}) = E\Big[ \frac{1}{n} \sum_{i=1}^{n} \{ Y_{i}&#39; -  \hat{m}_{h_{n}}(x_{i}) \}^{2} \Big]
\end{equation}\]</span>
where <span class="math inline">\(Y_{i}&#39;\)</span> is a “future” independent observation that has the same covariate as <span class="math inline">\(Y_{i}\)</span>.</p></li>
<li><p>We assume that <span class="math inline">\(Y_{i}&#39; = m(x_{i}) + \varepsilon_{i}&#39;\)</span> where <span class="math inline">\(\varepsilon_{i}&#39;\)</span> is independent of <span class="math inline">\(\varepsilon_{i}\)</span>.</p></li>
<li><p>So,
<span class="math display">\[\begin{eqnarray}
\textrm{PMASE}(h_{n}) &amp;=&amp; E\Big[ \frac{1}{n} \sum_{i=1}^{n} \{ m(x_{i}) -  \hat{m}_{h_{n}}(x_{i}) + \varepsilon_{i}&#39; \}^{2} \Big] 
\nonumber \\
&amp;=&amp; E\Big[ \frac{1}{n} \sum_{i=1}^{n} \{ m(x_{i}) -  \hat{m}_{h_{n}}(x_{i}) \}^{2} \Big]  +  E\Big[ \frac{1}{n} \sum_{i=1}^{n} (\varepsilon_{i}&#39;)^{2} \Big]  \nonumber \\
&amp;=&amp; \textrm{PMASE}( h_{n} ) + \sigma^{2} \nonumber
\end{eqnarray}\]</span></p></li>
</ul>
<hr />
<ul>
<li><p>The <span class="math inline">\(C_{p}\)</span> statistic is based on the idea that, if <span class="math inline">\(\sigma^{2}\)</span> was known, then the following quantity
would be an unbiased estimate of <span class="math inline">\(\textrm{PMASE}( h_{n} )\)</span>:
<span class="math display">\[\begin{equation}
\frac{1}{n}\sum_{i=1}^{n} \{ Y_{i} - \hat{m}_{h_{n}}(x_{i}) \}^{2}  + \frac{2\sigma^{2}}{n}\textrm{tr}( \mathbf{A}_{h_{n}}) \nonumber
\end{equation}\]</span></p></li>
<li><p>The <span class="math inline">\(C_{p}\)</span> statistic formula is obtained by plugging in an estimate <span class="math inline">\(\hat{\sigma}^{2}\)</span> of the residual variance into the above
formula:
<span class="math display">\[\begin{equation}
C_{p}(h_{n}) = \frac{1}{n}\sum_{i=1}^{n} \{ Y_{i} - \hat{m}_{h_{n}}(x_{i}) \}^{2}  + \frac{2\hat{\sigma}^{2}}{n}\textrm{tr}( \mathbf{A}_{h_{n}}) \nonumber
\end{equation}\]</span></p></li>
<li><p>The reason this is called the “<span class="math inline">\(C_{p}\)</span> statistic” is that in the case of a linear regression model with <span class="math inline">\(p\)</span>
columns in the design matrix, we have <span class="math inline">\(\hat{\mathbf{m}} = \mathbf{X}(\mathbf{X}^{T}\mathbf{X})^{-1}\mathbf{X}^{T}\mathbf{Y}\)</span>
and <span class="math inline">\(\textrm{tr}\{ \mathbf{X}(\mathbf{X}^{T}\mathbf{X})^{-1}\mathbf{X}^{T} \} = p\)</span> so an
estimator of the PMASE would be
<span class="math display">\[\begin{equation}
C_{p} = \frac{1}{n}\sum_{i=1}^{n}\{ Y_{i} - \hat{m}_{h_{n}}(x_{i}) \}^{2}  + \frac{2\hat{\sigma}^{2}}{n}p  \nonumber
\end{equation}\]</span></p></li>
<li><p>For this reason, <span class="math inline">\(\textrm{tr}( \mathbf{A}_{h_{n}} )\)</span> is often referred to as
the “degrees of freedom” of a nonparametric estimator of the form <span class="math inline">\(\hat{\mathbf{m}} = \mathbf{A}_{h_{n}}\mathbf{Y}\)</span>.</p></li>
</ul>
<hr />
<ul>
<li><p>The main drawback of the <span class="math inline">\(C_{p}\)</span> statistic is that it requires an estimate of the residual variance <span class="math inline">\(\sigma^{2}\)</span>.</p></li>
<li><p>So, if we choose a fairly small bandwidth <span class="math inline">\(\tilde{h}_{n}\)</span> so that the bias is close to zero, we could use the following
estimate of <span class="math inline">\(\sigma^{2}\)</span>
<span class="math display">\[\begin{equation}
\hat{\sigma}^{2}( \tilde{h}_{n} ) = \frac{  \sum_{i=1}^{n}\{ Y_{i} - \hat{m}_{\tilde{h}_{n}}(x_{i}) \}^{2}  }{ n - 2\textrm{tr}(\mathbf{A}_{\tilde{h}_{n}}) + \textrm{tr}(\mathbf{A}_{\tilde{h}_{n}}\mathbf{A}_{\tilde{h_{n}}}^{T}) }  \nonumber
\end{equation}\]</span></p></li>
</ul>
<hr />
<ul>
<li><strong>Exercise 8.4</strong> Suppose the <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(\mathbf{A}_{h_{n}}\)</span> satifies <span class="math inline">\(\mathbf{A}_{h_{n}}\mathbf{m} = \mathbf{m}\)</span>.
Show that
<span class="math display">\[\begin{equation}
\frac{\mathbf{Y}^{T}(\mathbf{I} - \mathbf{A}_{h_{n}})^{T}(\mathbf{I} - \mathbf{A}_{h_{n}})\mathbf{Y} }{ n - 2\textrm{tr}( \mathbf{A}_{h_{n}}) + \textrm{tr}(\mathbf{A}_{h_{n}}\mathbf{A}_{h_{n}}^{T}) } \nonumber
\end{equation}\]</span>
is an unbiased estimator of <span class="math inline">\(\sigma^{2}\)</span>.</li>
</ul>
<hr />
</div>
<div id="leave-one-out-cross-validation" class="section level3">
<h3><span class="header-section-number">11.4.3</span> Leave-one-out Cross Validation</h3>
<ul>
<li><p>Similar to the way we defined a leave-on-out density estimate in Chapter 8, we will define
the leave-one-out estimate of the regression function at <span class="math inline">\(x\)</span> as:
<span class="math display">\[\begin{equation}
\hat{m}_{h_{n},-i}(x) - \textrm{ estimate of $m(x)$ found by using all data except $(Y_{i}, x_{i})$.}
\end{equation}\]</span></p></li>
<li><p>The leave-one-out cross validation (LOO-CV) estimate of the PMASE is defined to be
<span class="math display">\[\begin{equation}
\textrm{LOOCV}(h_{n}) = \frac{1}{n}\sum_{i=1}^{n} \{ Y_{i} - \hat{m}_{h_{n}, -i}(x_{i}) \}^{2}  \nonumber
\end{equation}\]</span></p></li>
<li><p>The intuition here is that; because the estimate <span class="math inline">\(\hat{m}_{h_{n}, -i}(x_{i})\)</span> is computed without
the <span class="math inline">\(i^{th}\)</span> observation, <span class="math inline">\(Y_{i}\)</span> plays the role of a “future observation” (relative to the dataset that
does not contain <span class="math inline">\(Y_{i}\)</span>).</p></li>
<li><p>Hence, <span class="math inline">\(\{ Y_{i} - \hat{m}_{h_{n}, -i}(x_{i}) \}^{2}\)</span> should be a sensible replacement for
the unobservable <span class="math inline">\(\{ Y_{i}&#39; - \hat{m}_{h_{n}}(x_{i}) \}^{2}\)</span>.</p></li>
</ul>
<hr />
<ul>
<li><p>While we could compute <span class="math inline">\(\textrm{LOOCV}(h_{n})\)</span> by computing <span class="math inline">\(\hat{m}_{h_{n}, -i}(x_{i})\)</span>
separately for <span class="math inline">\(i = 1,\ldots,n\)</span>, there is a much more efficient way of computing <span class="math inline">\(\textrm{LOOCV}(h_{n})\)</span>.</p></li>
<li><p>We are assuming that <span class="math inline">\(\hat{m}_{h_{n}}(x)\)</span> can be represented as a linear combination of the responses
<span class="math display">\[\begin{equation}
\hat{m}_{h_{n}}(x) = \sum_{j=1}^{n} a_{j}^{h_{n}}(x)Y_{j}, \nonumber
\end{equation}\]</span>
where we can think of <span class="math inline">\(a_{j}^{h_{n}}(x)\)</span> as weights that sum to <span class="math inline">\(1\)</span>.</p></li>
<li><p>If we did not use <span class="math inline">\(Y_{i}\)</span> to compute <span class="math inline">\(\hat{m}_{h_{n}}(x)\)</span>, this estimate would look like
<span class="math display">\[\begin{equation}
\hat{m}_{h_{n}, -i}(x) = \sum_{j=1}^{n} a_{j,-i}^{h_{n}}(x)Y_{j}
\end{equation}\]</span>
where
<span class="math display">\[\begin{equation}
a_{j,-i}^{h_{n}}(x) = 
\begin{cases}
0 &amp; \textrm{ if } j = i \nonumber \\
\frac{ a_{j}^{h_{n}}(x)}{ \sum_{k \neq i} a_{k}^{h_{n}}(x) }  \nonumber
\end{cases}
\end{equation}\]</span></p></li>
<li><p>If you want to better convince yourself that the above formula for <span class="math inline">\(a_{j,-i}^{h_{n}}(x_{i})\)</span> is true,
try an example using the Nadaraya-Watson estimator with <span class="math inline">\(n=3\)</span>.</p></li>
</ul>
<hr />
<ul>
<li><p>Because <span class="math inline">\(\sum_{k \neq i} a_{k}^{h_{n}}(x_{i}) = 1 - a_{i}^{h_{n}}( x_{i} )\)</span>, we can express <span class="math inline">\(Y_{i} - \hat{m}_{h_{n},-i}(x_{i})\)</span> as
<span class="math display" id="eq:loocv-simplification">\[\begin{eqnarray}
Y_{i} - \hat{m}_{h_{n}, -i}(x_{i}) &amp;=&amp; Y_{i} - \sum_{j \neq i}^{n} a_{j,-i}^{h_{n}}(x_{i})Y_{j} \nonumber \\
&amp;=&amp; Y_{i} - \frac{1}{1 - a_{i}^{h_{n}}( x_{i} ) } \sum_{j \neq i}^{n} a_{j}^{h_{n}}(x_{i})Y_{j} \nonumber \\
&amp;=&amp; Y_{i} - \Big[ \frac{1}{1 - a_{i}^{h_{n}}( x_{i} ) } \sum_{j = 1}^{n} a_{j}^{h_{n}}(x_{i})Y_{j} \Big]  +  \frac{a_{j}^{h_{n}}(x_{i})Y_{i} }{1 - a_{i}^{h_{n}}( x_{i} ) }  \nonumber \\
&amp;=&amp; \frac{Y_{i} - \hat{m}_{h_{n}}(x_{i}) }{1 - a_{i}^{h_{n}}( x_{i} ) } 
\tag{11.5}
\end{eqnarray}\]</span></p></li>
<li><p>Using <a href="kernel-regression-and-local-regression.html#eq:loocv-simplification">(11.5)</a>, we can re-write the LOOCV estimate as
<span class="math display">\[\begin{equation}
\textrm{LOOCV}(h_{n}) = \frac{1}{n}\sum_{i=1}^{n} \Big( \frac{ Y_{i} - \hat{m}_{h_{n}}(x_{i})}{ 1 - a_{i}^{h_{n}}( x_{i} )  } \Big)^{2}  \nonumber
\end{equation}\]</span>
where <span class="math inline">\(a_{i}^{h_{n}}(x_{i})\)</span> are just the diagonal elements of our original matrix <span class="math inline">\(\mathbf{A}_{h_{n}}\)</span>.</p></li>
</ul>
</div>
<div id="example-choosing-the-best-bin-width-for-the-local-average-estimator." class="section level3">
<h3><span class="header-section-number">11.4.4</span> Example: Choosing the Best Bin Width for the Local Average Estimator.</h3>
<p><strong>Cp Statistic</strong></p>
<ul>
<li><p>The diagonal entries of the <span class="math inline">\(\mathbf{A}_{h_{n}}\)</span> matrix for the local average estimator
are <span class="math inline">\(1/n_{h_{n}}(x_{1}), \ldots, 1/n_{h_{n}}(x_{n})\)</span>.</p></li>
<li><p>So, the “degrees of freedom” for the local average estimator is
<span class="math display">\[\begin{equation}
\textrm{tr}( \mathbf{A}_{h_{n}}) = \sum_{i=1}^{n} \frac{1}{n_{h_{n}}(x_{i}) }
\end{equation}\]</span></p></li>
<li><p>Notice that if we choose <span class="math inline">\(h_{n}\)</span> large enough so that <span class="math inline">\(n_{h_{n}}(x_{i}) = n\)</span> for all <span class="math inline">\(x_{i}\)</span>,
then the degrees of freedom is equal to <span class="math inline">\(1\)</span>.</p></li>
<li><p>The <span class="math inline">\(C_{p}\)</span> statistic for the local average estimator is
<span class="math display">\[\begin{equation}
\frac{1}{n}\sum_{i=1}^{n} \{ Y_{i} - \hat{m}_{h_{n}}(x_{i}) \}^{2}  + \frac{ 2\hat{\sigma}^{2} }{n}\sum_{i=1}^{n} \frac{1}{n_{h_{n}}(x_{i}) } \nonumber
\end{equation}\]</span></p></li>
</ul>
<hr />
<ul>
<li><p>Let’s try to compute <span class="math inline">\(C_{p}(h_{n})\)</span> for the <code>bonedat</code> dataset.</p></li>
<li><p>The first step is to write a function that computes the <span class="math inline">\(n_{h_{n}}(x_{i})\)</span> for a given value of <span class="math inline">\(h_{n}\)</span>. This will allow us to find the degrees of freedom
and will also be helpful later when computing LOOCV.</p></li>
</ul>
<div class="sourceCode" id="cb227"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb227-1" data-line-number="1">NumInBins &lt;-<span class="st"> </span><span class="cf">function</span>(hh, xx) {</a>
<a class="sourceLine" id="cb227-2" data-line-number="2">  <span class="co">## This function returns a vector of length n</span></a>
<a class="sourceLine" id="cb227-3" data-line-number="3">  <span class="co">## Elements of this vector will be: n_[h_n](x_1), n_[h_n](x_2), ...</span></a>
<a class="sourceLine" id="cb227-4" data-line-number="4">  n &lt;-<span class="st"> </span><span class="kw">length</span>(xx)</a>
<a class="sourceLine" id="cb227-5" data-line-number="5">  num.bin &lt;-<span class="st"> </span><span class="kw">numeric</span>(n)</a>
<a class="sourceLine" id="cb227-6" data-line-number="6">  <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb227-7" data-line-number="7">    num.bin[k] &lt;-<span class="st"> </span><span class="kw">sum</span>(xx <span class="op">&gt;</span><span class="st"> </span>xx[k] <span class="op">-</span><span class="st"> </span>hh <span class="op">&amp;</span><span class="st"> </span>xx <span class="op">&lt;</span><span class="st"> </span>xx[k] <span class="op">+</span><span class="st"> </span>hh)</a>
<a class="sourceLine" id="cb227-8" data-line-number="8">  } </a>
<a class="sourceLine" id="cb227-9" data-line-number="9">  <span class="kw">return</span>(num.bin)</a>
<a class="sourceLine" id="cb227-10" data-line-number="10">}</a></code></pre></div>
<hr />
<ul>
<li>We also want a function that returns the vector with elements <span class="math inline">\(\hat{m}_{h_{n}}(x_{1}), \hat{m}_{h_{n}}(x_{2}) , \ldots \hat{m}_{h_{n}}(x_{n})\)</span>.</li>
</ul>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb228-1" data-line-number="1">MyLocAvgEst &lt;-<span class="st"> </span><span class="cf">function</span>(xx, yy, hh) {</a>
<a class="sourceLine" id="cb228-2" data-line-number="2">  n &lt;-<span class="st"> </span><span class="kw">length</span>(xx)</a>
<a class="sourceLine" id="cb228-3" data-line-number="3">  m.hat.loc &lt;-<span class="st"> </span><span class="kw">numeric</span>(n)</a>
<a class="sourceLine" id="cb228-4" data-line-number="4">  <span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb228-5" data-line-number="5">    in.bin &lt;-<span class="st"> </span>xx <span class="op">&gt;</span><span class="st"> </span>xx[k] <span class="op">-</span><span class="st"> </span>hh <span class="op">&amp;</span><span class="st"> </span>xx <span class="op">&lt;</span><span class="st"> </span>xx[k] <span class="op">+</span><span class="st"> </span>hh</a>
<a class="sourceLine" id="cb228-6" data-line-number="6">    m.hat.loc[k] &lt;-<span class="st"> </span><span class="kw">mean</span>(yy[in.bin])</a>
<a class="sourceLine" id="cb228-7" data-line-number="7">  }</a>
<a class="sourceLine" id="cb228-8" data-line-number="8">  <span class="kw">return</span>(m.hat.loc)</a>
<a class="sourceLine" id="cb228-9" data-line-number="9">}</a></code></pre></div>
<hr />
<ul>
<li><p>The final step is to compute an estimate of <span class="math inline">\(\sigma^{2}\)</span>.</p></li>
<li><p>Using the estimate
<span class="math display">\[\begin{equation}
\hat{\sigma}^{2}( \tilde{h}_{n} ) = \frac{  \sum_{i=1}^{n}\{ Y_{i} - \hat{m}_{\tilde{h}_{n}}(x_{i}) \}^{2}  }{ n - 2\textrm{tr}(\mathbf{A}_{\tilde{h}_{n}}) + \textrm{tr}(\mathbf{A}_{\tilde{h}_{n}}\mathbf{A}_{\tilde{h_{n}}}^{T}) }
\end{equation}\]</span>
that we mentioned before with <span class="math inline">\(\tilde{h}_{n} = 0.1\)</span>, I got a an estimate of <span class="math inline">\(\sigma^{2}\)</span> which was quite close to <span class="math inline">\(0.0015\)</span></p></li>
</ul>
<div class="sourceCode" id="cb229"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb229-1" data-line-number="1">sigsq.est &lt;-<span class="st"> </span><span class="fl">0.0015</span></a></code></pre></div>
<hr />
<ul>
<li>Now, we are ready to compute the <span class="math inline">\(C_{p}\)</span> statistic. We will compute <span class="math inline">\(C_{p}(h_{n})\)</span> for <span class="math inline">\(h_{n} = 0.01, 0.11, \ldots, 10.01\)</span>. This can be done with the following code:</li>
</ul>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb230-1" data-line-number="1">hseq &lt;-<span class="st"> </span><span class="kw">seq</span>(.<span class="dv">01</span>, <span class="fl">10.01</span>, <span class="dt">by=</span>.<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb230-2" data-line-number="2">ngrid &lt;-<span class="st"> </span><span class="kw">length</span>(hseq)</a>
<a class="sourceLine" id="cb230-3" data-line-number="3">n &lt;-<span class="st"> </span><span class="kw">length</span>(bonedat<span class="op">$</span>age)</a>
<a class="sourceLine" id="cb230-4" data-line-number="4">Cp &lt;-<span class="st"> </span><span class="kw">numeric</span>(ngrid)</a>
<a class="sourceLine" id="cb230-5" data-line-number="5"><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>ngrid) {</a>
<a class="sourceLine" id="cb230-6" data-line-number="6">   m.hat &lt;-<span class="st"> </span><span class="kw">MyLocAvgEst</span>(bonedat<span class="op">$</span>age, bonedat<span class="op">$</span>spnbmd, hseq[k])</a>
<a class="sourceLine" id="cb230-7" data-line-number="7">   dfval &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="dv">1</span><span class="op">/</span><span class="kw">NumInBins</span>(hseq[k], bonedat<span class="op">$</span>age))</a>
<a class="sourceLine" id="cb230-8" data-line-number="8">   Cp[k] &lt;-<span class="st"> </span><span class="kw">mean</span>( (bonedat<span class="op">$</span>spnbmd <span class="op">-</span><span class="st"> </span>m.hat)<span class="op">^</span><span class="dv">2</span> ) <span class="op">+</span><span class="st"> </span>(<span class="dv">2</span><span class="op">*</span>sigsq.est<span class="op">*</span>dfval)<span class="op">/</span>n</a>
<a class="sourceLine" id="cb230-9" data-line-number="9">}</a></code></pre></div>
<ul>
<li>We can plot the values of <span class="math inline">\(C_{p}(h_{n})\)</span> vs. <span class="math inline">\(h_{n}\)</span> to roughly see where the minimum value is. From the graph, it looks to be slighly less than <span class="math inline">\(1\)</span>.</li>
</ul>
<div class="sourceCode" id="cb231"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb231-1" data-line-number="1"><span class="kw">plot</span>(hseq, Cp, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="fl">0.001</span>,.<span class="dv">003</span>), <span class="dt">main=</span><span class="st">&quot;Bone Data: Cp Stat for Loc. Avg. Est.&quot;</span>, </a>
<a class="sourceLine" id="cb231-2" data-line-number="2">     <span class="dt">xlab=</span><span class="st">&quot;hn&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Cp&quot;</span>)</a>
<a class="sourceLine" id="cb231-3" data-line-number="3"><span class="kw">lines</span>(hseq, Cp)</a></code></pre></div>
<p><img src="11-kernel-regression_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<ul>
<li>More precisely, the value of <span class="math inline">\(h_{n}\)</span> from our sequence which has the smallest value of <span class="math inline">\(C_{p}(h_{n})\)</span> is <span class="math inline">\(0.81\)</span>.</li>
</ul>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb232-1" data-line-number="1">hseq[<span class="kw">which.min</span>(Cp)]</a></code></pre></div>
<pre><code>## [1] 0.81</code></pre>
<hr />
<p><strong>LOOCV</strong></p>
<ul>
<li><p>We can use the functions that we have written to compute <span class="math inline">\(\textrm{LOOCV}(h_{n})\)</span>.</p></li>
<li><p>It is useful to notice that <span class="math inline">\(1 - a_{i}^{h_{n}}( x_{i} ) = 1 - 1/n_{h_{n}}(x_{i})\)</span> using the notation
we used in the description of the LOOCV.</p></li>
<li><p><code>R</code> code to compute <span class="math inline">\(\textrm{LOOCV}(h_{n})\)</span> at the same sequence of <span class="math inline">\(h_{n}\)</span> values used for the <span class="math inline">\(C_{p}\)</span>
statistic is given below:</p></li>
</ul>
<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb234-1" data-line-number="1">LOOCV &lt;-<span class="st"> </span><span class="kw">numeric</span>(ngrid)</a>
<a class="sourceLine" id="cb234-2" data-line-number="2"><span class="cf">for</span>(k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>ngrid) {</a>
<a class="sourceLine" id="cb234-3" data-line-number="3">  m.hat &lt;-<span class="st"> </span><span class="kw">MyLocAvgEst</span>(bonedat<span class="op">$</span>age, bonedat<span class="op">$</span>spnbmd, hseq[k])</a>
<a class="sourceLine" id="cb234-4" data-line-number="4">  n.hn &lt;-<span class="st"> </span><span class="kw">NumInBins</span>(hseq[k], bonedat<span class="op">$</span>age)</a>
<a class="sourceLine" id="cb234-5" data-line-number="5">  dd &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span>n.hn</a>
<a class="sourceLine" id="cb234-6" data-line-number="6">  LOOCV[k] &lt;-<span class="st"> </span><span class="kw">mean</span>( ((bonedat<span class="op">$</span>spnbmd <span class="op">-</span><span class="st"> </span>m.hat)<span class="op">/</span>dd)<span class="op">^</span><span class="dv">2</span> ) </a>
<a class="sourceLine" id="cb234-7" data-line-number="7">}</a></code></pre></div>
<ul>
<li>We can plot the values of <span class="math inline">\(\textrm{LOOCV}(h_{n})\)</span> vs. <span class="math inline">\(h_{n}\)</span> to roughly see where the minimum value is.</li>
</ul>
<div class="sourceCode" id="cb235"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb235-1" data-line-number="1"><span class="kw">plot</span>(hseq, LOOCV, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="fl">0.001</span>,.<span class="dv">003</span>), <span class="dt">main=</span><span class="st">&quot;Bone Data: LOOCV Stat for Loc. Avg. Est.&quot;</span>, </a>
<a class="sourceLine" id="cb235-2" data-line-number="2">     <span class="dt">xlab=</span><span class="st">&quot;hn&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;LOOCV&quot;</span>)</a>
<a class="sourceLine" id="cb235-3" data-line-number="3"><span class="kw">lines</span>(hseq, LOOCV)</a></code></pre></div>
<p><img src="11-kernel-regression_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<ul>
<li>The value of <span class="math inline">\(h_{n}\)</span> from our sequence which has the smallest value of <span class="math inline">\(\textrm{LOOCV}(h_{n})\)</span> is <span class="math inline">\(0.81\)</span>.</li>
</ul>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb236-1" data-line-number="1">hseq[<span class="kw">which.min</span>(LOOCV)]</a></code></pre></div>
<pre><code>## [1] 0.81</code></pre>
<p><img src="11-kernel-regression_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
</div>
</div>
<div id="additional-functions-in-r" class="section level2">
<h2><span class="header-section-number">11.5</span> Additional functions in R</h2>
<ul>
<li><p>The R functions <code>lowess</code> and <code>loess</code> are widely used functions for smoothing via local regression.</p></li>
<li><p><code>loess</code> is basically an expanded version of <code>lowess</code>. The function <code>loess</code> has more options than
the <code>lowess</code> function and is written to resemble the <code>lm</code> in function in <code>R</code>.</p></li>
<li><p>Note that <code>loess</code> and <code>lowess</code> have different default settings for some of the model fitting parameters
so they can differ somewhat in the values they return unless you set these parameters to be equal.</p></li>
<li><p><code>loess</code> allows for multivariate covariates <span class="math inline">\(\mathbf{x}_{i}\)</span> while <code>lowess</code> does not.</p></li>
</ul>
<hr />
<ul>
<li><code>lowess</code> does local quadratic and local linear regression. The format of the <code>lowess</code> function is the following:</li>
</ul>
<div class="sourceCode" id="cb238"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb238-1" data-line-number="1"><span class="kw">loess</span>(formula, data, span)</a></code></pre></div>
<ul>
<li><p><strong>formula</strong> - usally of the form <code>y ~ x</code> if using a single response vector <code>y</code> and covariate <code>x</code></p></li>
<li><p><strong>data</strong> - the dataset from which <code>y</code> and <code>x</code> are drawn from</p></li>
<li><p><strong>span</strong> - the “span” of the smoother. This can be thought of as playing the role of the bandwidth. Default of span <span class="math inline">\(= \alpha\)</span> is set to <span class="math inline">\(0.75\)</span>. Usually, <span class="math inline">\(0 &lt; \alpha &lt; 1\)</span>.</p></li>
<li><p><strong>degree</strong> - the degree of the polynomial used for local regression. The default is set to <span class="math inline">\(2\)</span>.</p></li>
<li><p>The <code>loess</code> function will return two vectors <code>$x</code> and <code>$fitted</code>. The <code>$x</code> is just the vector of covariates from the original data, and
the <code>$fitted</code> vector is the value of the estimated regression function <span class="math inline">\(\hat{m}( x_{i} )\)</span> at these points.</p></li>
<li><p>The <code>lowess</code> function performs local linear regression with a few extra steps included to make the fitting procedure more robust.</p></li>
</ul>
<hr />
<ul>
<li><p>For weights <span class="math inline">\(W_{i}^{\alpha}(x)\)</span> in the local linear regression problem, loess and lowess uses the following tri-cube function
<span class="math display">\[\begin{equation}
W_{i}^{\alpha}(x)
= \begin{cases}
\Bigg( 1 - \Big( \frac{| x - x_{i}| }{| x - x_{(q)}| } \Big)^{3} \Bigg)^{3} &amp; \text{ if } |x - x_{i}| \leq |x - x_{(q)}|  \\
0 &amp; \textrm{ otherwise }
\end{cases}
\end{equation}\]</span></p></li>
<li><p>The weights <span class="math inline">\(W_{i}^{\alpha}(x)\)</span> here play the same role as <span class="math inline">\(K( \tfrac{x - x_{i}}{ h_{n} } )\)</span> did in our description of local linear regression in Section 10.3.</p></li>
<li><p>Here, <span class="math inline">\(x_{(q)}\)</span> is the <span class="math inline">\(\lfloor \alpha \times n \rfloor\)</span> furthest observations away from <span class="math inline">\(x\)</span>, and <span class="math inline">\(\lfloor \alpha \times n \rfloor\)</span> denotes
<span class="math inline">\(f n\)</span> rounded down to the nearest integer.</p></li>
<li><p>So, values of the span <span class="math inline">\(\alpha\)</span> which are closer to <span class="math inline">\(0\)</span> mean that
you are using a smaller number of observations when performing each local regression while
values close to <span class="math inline">\(1\)</span> mean that nearly all the observations receive positive weight when performing each local regression.</p></li>
<li><p>After fitting a local regression with weights <span class="math inline">\(W_{i}^{\alpha}(x)\)</span>, <code>lowess</code> actually does an additional local<br />
regression with updated weights that reduce the influence of outliers. <code>loess</code> will do the same thing if <code>family</code> is set to “symmetric” rather than “gaussian”.</p></li>
</ul>
<hr />
<ul>
<li>Let’s try plotting a <code>loess</code> fit using the bone data. We will set <code>span = 2/3</code> instead of 3/4</li>
</ul>
<div class="sourceCode" id="cb239"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb239-1" data-line-number="1">bone.low.fit &lt;-<span class="st"> </span><span class="kw">loess</span>(spnbmd <span class="op">~</span><span class="st"> </span>age, <span class="dt">data=</span>bonedat, <span class="dt">span=</span><span class="dv">2</span><span class="op">/</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb239-2" data-line-number="2"></a>
<a class="sourceLine" id="cb239-3" data-line-number="3"><span class="kw">plot</span>(bone.low.fit, <span class="dt">ylab=</span><span class="st">&quot;Relative Change in Bone MD&quot;</span>, </a>
<a class="sourceLine" id="cb239-4" data-line-number="4">     <span class="dt">xlab=</span><span class="st">&quot;Age&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Bone Data: Lowess Fit&quot;</span>, <span class="dt">las=</span><span class="dv">1</span>, <span class="dt">pch=</span><span class="dv">16</span>)</a>
<a class="sourceLine" id="cb239-5" data-line-number="5"><span class="co">## plot a line of the the fitted values vs. x. Need to sort </span></a>
<a class="sourceLine" id="cb239-6" data-line-number="6"><span class="co">## the x_i&#39;s first though if we want a nice looking line:</span></a>
<a class="sourceLine" id="cb239-7" data-line-number="7"><span class="kw">lines</span>(bone.low.fit<span class="op">$</span>x[<span class="kw">order</span>(bone.low.fit<span class="op">$</span>x)], bone.low.fit<span class="op">$</span>fitted[<span class="kw">order</span>(bone.low.fit<span class="op">$</span>x)], </a>
<a class="sourceLine" id="cb239-8" data-line-number="8">      <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb239-9" data-line-number="9"><span class="kw">abline</span>(<span class="dv">0</span>,<span class="dv">0</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a></code></pre></div>
<p><img src="11-kernel-regression_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<hr />
<ul>
<li>If you want to change the settings for <code>lowess</code> and <code>loess</code> so that they are using the exact same fitting procedure,
you can use the following approach:</li>
</ul>
<div class="sourceCode" id="cb240"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb240-1" data-line-number="1">lowess.fit &lt;-<span class="st"> </span><span class="kw">lowess</span>(<span class="dt">x=</span>bonedat<span class="op">$</span>age, <span class="dt">y=</span>bonedat<span class="op">$</span>spnbmd, <span class="dt">iter=</span><span class="dv">3</span>, <span class="dt">delta=</span><span class="dv">0</span>, <span class="dt">f=</span><span class="dv">2</span><span class="op">/</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb240-2" data-line-number="2">loess.fit &lt;-<span class="st"> </span><span class="kw">loess</span>(spnbmd <span class="op">~</span><span class="st"> </span>age, <span class="dt">data=</span>bonedat, <span class="dt">span=</span><span class="dv">2</span><span class="op">/</span><span class="dv">3</span>, <span class="dt">degree=</span><span class="dv">1</span>, <span class="dt">family=</span><span class="st">&quot;symmetric&quot;</span>, </a>
<a class="sourceLine" id="cb240-3" data-line-number="3">                   <span class="dt">iterations=</span><span class="dv">4</span>, <span class="dt">surface=</span><span class="st">&quot;direct&quot;</span>)</a></code></pre></div>
<hr />
</div>
<div id="multivariate-problems" class="section level2">
<h2><span class="header-section-number">11.6</span> Multivariate Problems</h2>
</div>
<div id="additional-reading-5" class="section level2">
<h2><span class="header-section-number">11.7</span> Additional Reading</h2>
<ul>
<li>Additional reading which covers the material discussed in this chapter includes:
<ul>
<li>Chapter 4 from <span class="citation">Härdle et al. (<a href="#ref-hardle2012">2012</a>)</span></li>
<li>Chapter 5 from <span class="citation">Wasserman (<a href="#ref-wasserman2006">2006</a>)</span></li>
</ul></li>
</ul>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-hardle2012">
<p>Härdle, Wolfgang Karl, Marlene Müller, Stefan Sperlich, and Axel Werwatz. 2012. <em>Nonparametric and Semiparametric Models</em>. Springer Science &amp; Business Media.</p>
</div>
<div id="ref-wasserman2006">
<p>Wasserman, Larry. 2006. <em>All of Nonparametric Statistics</em>. Springer Science &amp; Business Media.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ci.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="inference-for-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ElementsNonparStat.pdf"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
