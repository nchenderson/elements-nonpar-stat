<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Introduction | Elements of Nonparametric Statistics</title>
  <meta name="description" content="Course notes for Biostatistics 685/Statistics 560 (Winter 2020)." />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Introduction | Elements of Nonparametric Statistics" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://nchenderson.github.io/elements-nonpar-stat/" />
  
  <meta property="og:description" content="Course notes for Biostatistics 685/Statistics 560 (Winter 2020)." />
  <meta name="github-repo" content="nchenderson/elements-nonpar-stat" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Introduction | Elements of Nonparametric Statistics" />
  
  <meta name="twitter:description" content="Course notes for Biostatistics 685/Statistics 560 (Winter 2020)." />
  

<meta name="author" content="Nicholas Henderson" />


<meta name="date" content="2020-03-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="getting-started.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Biostat 685/Stat 560</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#sec:whatisnonpar"><i class="fa fa-check"></i><b>1.1</b> What is Nonparametric Statistics?</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#sec:course-outline"><i class="fa fa-check"></i><b>1.2</b> Outline of Course</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#sec:example-nonpar-tests"><i class="fa fa-check"></i><b>1.3</b> Example 1: Nonparametric vs. Parametric Two-Sample Testing</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#sec:example-nonpar-estimation"><i class="fa fa-check"></i><b>1.4</b> Example 2: Nonparametric Estimation</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#sec:example-nonpar-confint"><i class="fa fa-check"></i><b>1.5</b> Example 3: Confidence Intervals</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#sec:example-nonpar-regress1"><i class="fa fa-check"></i><b>1.6</b> Example 4: Nonparametric Regression with a Single Covariate</a></li>
<li class="chapter" data-level="1.7" data-path="intro.html"><a href="intro.html#sec:example-nonpar-regress2"><i class="fa fa-check"></i><b>1.7</b> Example 5: Classification and Regression Trees (CART)</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>2</b> Working with R</a></li>
<li class="part"><span><b>I Nonparametric Testing</b></span></li>
<li class="chapter" data-level="3" data-path="rank-tests.html"><a href="rank-tests.html"><i class="fa fa-check"></i><b>3</b> Rank and Sign Statistics</a><ul>
<li class="chapter" data-level="3.1" data-path="rank-tests.html"><a href="rank-tests.html#ranks"><i class="fa fa-check"></i><b>3.1</b> Ranks</a><ul>
<li class="chapter" data-level="3.1.1" data-path="rank-tests.html"><a href="rank-tests.html#definition"><i class="fa fa-check"></i><b>3.1.1</b> Definition</a></li>
<li class="chapter" data-level="3.1.2" data-path="rank-tests.html"><a href="rank-tests.html#handling-ties"><i class="fa fa-check"></i><b>3.1.2</b> Handling Ties</a></li>
<li class="chapter" data-level="3.1.3" data-path="rank-tests.html"><a href="rank-tests.html#properties-of-ranks"><i class="fa fa-check"></i><b>3.1.3</b> Properties of Ranks</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="rank-tests.html"><a href="rank-tests.html#the-wilcoxon-rank-sum-wrs-test-a-two-sample-test"><i class="fa fa-check"></i><b>3.2</b> The Wilcoxon Rank Sum (WRS) Test: A Two-Sample Test</a><ul>
<li class="chapter" data-level="3.2.1" data-path="rank-tests.html"><a href="rank-tests.html#goal-of-the-test"><i class="fa fa-check"></i><b>3.2.1</b> Goal of the Test</a></li>
<li class="chapter" data-level="3.2.2" data-path="rank-tests.html"><a href="rank-tests.html#definition-of-the-wrs-test-statistic"><i class="fa fa-check"></i><b>3.2.2</b> Definition of the WRS Test Statistic</a></li>
<li class="chapter" data-level="3.2.3" data-path="rank-tests.html"><a href="rank-tests.html#computing-p-values-for-the-wrs-test"><i class="fa fa-check"></i><b>3.2.3</b> Computing p-values for the WRS Test</a></li>
<li class="chapter" data-level="3.2.4" data-path="rank-tests.html"><a href="rank-tests.html#computing-the-wrs-test-in-r"><i class="fa fa-check"></i><b>3.2.4</b> Computing the WRS test in R</a></li>
<li class="chapter" data-level="3.2.5" data-path="rank-tests.html"><a href="rank-tests.html#additional-notes-for-the-wrs-test"><i class="fa fa-check"></i><b>3.2.5</b> Additional Notes for the WRS test</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="rank-tests.html"><a href="rank-tests.html#one-sample-tests"><i class="fa fa-check"></i><b>3.3</b> One Sample Tests</a><ul>
<li class="chapter" data-level="3.3.1" data-path="rank-tests.html"><a href="rank-tests.html#sign-test"><i class="fa fa-check"></i><b>3.3.1</b> The Sign Test</a></li>
<li class="chapter" data-level="3.3.2" data-path="rank-tests.html"><a href="rank-tests.html#the-wilcoxon-signed-rank-test"><i class="fa fa-check"></i><b>3.3.2</b> The Wilcoxon Signed Rank Test</a></li>
<li class="chapter" data-level="3.3.3" data-path="rank-tests.html"><a href="rank-tests.html#using-r-to-perform-the-sign-and-wilcoxon-tests"><i class="fa fa-check"></i><b>3.3.3</b> Using R to Perform the Sign and Wilcoxon Tests</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="rank-tests.html"><a href="rank-tests.html#power-and-comparisons-with-parametric-tests"><i class="fa fa-check"></i><b>3.4</b> Power and Comparisons with Parametric Tests</a><ul>
<li class="chapter" data-level="3.4.1" data-path="rank-tests.html"><a href="rank-tests.html#the-power-function-of-a-test"><i class="fa fa-check"></i><b>3.4.1</b> The Power Function of a Test</a></li>
<li class="chapter" data-level="3.4.2" data-path="rank-tests.html"><a href="rank-tests.html#power-comparisons-and-asymptotic-relative-efficiency"><i class="fa fa-check"></i><b>3.4.2</b> Power Comparisons and Asymptotic Relative Efficiency</a></li>
<li class="chapter" data-level="3.4.3" data-path="rank-tests.html"><a href="rank-tests.html#efficiency-examples"><i class="fa fa-check"></i><b>3.4.3</b> Efficiency Examples</a></li>
<li class="chapter" data-level="3.4.4" data-path="rank-tests.html"><a href="rank-tests.html#efficiency-comparisons-for-several-distributions"><i class="fa fa-check"></i><b>3.4.4</b> Efficiency Comparisons for Several Distributions</a></li>
<li class="chapter" data-level="3.4.5" data-path="rank-tests.html"><a href="rank-tests.html#a-power-contest"><i class="fa fa-check"></i><b>3.4.5</b> A Power “Contest”</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="rank-tests.html"><a href="rank-tests.html#linear-rank-statistics-in-general"><i class="fa fa-check"></i><b>3.5</b> Linear Rank Statistics in General</a><ul>
<li class="chapter" data-level="3.5.1" data-path="rank-tests.html"><a href="rank-tests.html#definition-1"><i class="fa fa-check"></i><b>3.5.1</b> Definition</a></li>
<li class="chapter" data-level="3.5.2" data-path="rank-tests.html"><a href="rank-tests.html#properties-of-linear-rank-statistics"><i class="fa fa-check"></i><b>3.5.2</b> Properties of Linear Rank Statistics</a></li>
<li class="chapter" data-level="3.5.3" data-path="rank-tests.html"><a href="rank-tests.html#other-examples-of-linear-rank-statistics"><i class="fa fa-check"></i><b>3.5.3</b> Other Examples of Linear Rank Statistics</a></li>
<li class="chapter" data-level="3.5.4" data-path="rank-tests.html"><a href="rank-tests.html#choosing-the-scores-a_ni"><i class="fa fa-check"></i><b>3.5.4</b> Choosing the scores <span class="math inline">\(a_{N}(i)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="rank-tests.html"><a href="rank-tests.html#additional-reading"><i class="fa fa-check"></i><b>3.6</b> Additional Reading</a></li>
<li class="chapter" data-level="3.7" data-path="rank-tests.html"><a href="rank-tests.html#exercises"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="krusk-wallis.html"><a href="krusk-wallis.html"><i class="fa fa-check"></i><b>4</b> Rank Tests for Multiple Groups</a><ul>
<li class="chapter" data-level="4.1" data-path="krusk-wallis.html"><a href="krusk-wallis.html#the-kruskal-wallis-test"><i class="fa fa-check"></i><b>4.1</b> The Kruskal-Wallis Test</a><ul>
<li class="chapter" data-level="4.1.1" data-path="krusk-wallis.html"><a href="krusk-wallis.html#definition-2"><i class="fa fa-check"></i><b>4.1.1</b> Definition</a></li>
<li class="chapter" data-level="4.1.2" data-path="krusk-wallis.html"><a href="krusk-wallis.html#asymptotic-distribution-and-connection-to-one-way-anova"><i class="fa fa-check"></i><b>4.1.2</b> Asymptotic Distribution and Connection to One-Way ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="krusk-wallis.html"><a href="krusk-wallis.html#performing-the-kruskal-wallis-test-in-r"><i class="fa fa-check"></i><b>4.2</b> Performing the Kruskal-Wallis Test in R</a></li>
<li class="chapter" data-level="4.3" data-path="krusk-wallis.html"><a href="krusk-wallis.html#comparison-of-specific-groups"><i class="fa fa-check"></i><b>4.3</b> Comparison of Specific Groups</a></li>
<li class="chapter" data-level="4.4" data-path="krusk-wallis.html"><a href="krusk-wallis.html#an-additional-example"><i class="fa fa-check"></i><b>4.4</b> An Additional Example</a></li>
<li class="chapter" data-level="4.5" data-path="krusk-wallis.html"><a href="krusk-wallis.html#additional-reading-1"><i class="fa fa-check"></i><b>4.5</b> Additional Reading</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="permutation.html"><a href="permutation.html"><i class="fa fa-check"></i><b>5</b> Permutation Tests</a><ul>
<li class="chapter" data-level="5.1" data-path="permutation.html"><a href="permutation.html#notation"><i class="fa fa-check"></i><b>5.1</b> Notation</a></li>
<li class="chapter" data-level="5.2" data-path="permutation.html"><a href="permutation.html#permutation-tests-for-the-two-sample-problem"><i class="fa fa-check"></i><b>5.2</b> Permutation Tests for the Two-Sample Problem</a><ul>
<li class="chapter" data-level="5.2.1" data-path="permutation.html"><a href="permutation.html#example-1"><i class="fa fa-check"></i><b>5.2.1</b> Example 1</a></li>
<li class="chapter" data-level="5.2.2" data-path="permutation.html"><a href="permutation.html#permutation-test-p-values"><i class="fa fa-check"></i><b>5.2.2</b> Permutation Test p-values</a></li>
<li class="chapter" data-level="5.2.3" data-path="permutation.html"><a href="permutation.html#example-2-ratios-of-means"><i class="fa fa-check"></i><b>5.2.3</b> Example 2: Ratios of Means</a></li>
<li class="chapter" data-level="5.2.4" data-path="permutation.html"><a href="permutation.html#example-3-differences-in-quantiles"><i class="fa fa-check"></i><b>5.2.4</b> Example 3: Differences in Quantiles</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="permutation.html"><a href="permutation.html#the-permutation-test-as-a-conditional-test"><i class="fa fa-check"></i><b>5.3</b> The Permutation Test as a Conditional Test</a></li>
<li class="chapter" data-level="5.4" data-path="permutation.html"><a href="permutation.html#a-permutation-test-for-correlation"><i class="fa fa-check"></i><b>5.4</b> A Permutation Test for Correlation</a></li>
<li class="chapter" data-level="5.5" data-path="permutation.html"><a href="permutation.html#a-permutation-test-for-variable-importance-in-regression-and-machine-learning"><i class="fa fa-check"></i><b>5.5</b> A Permutation Test for Variable Importance in Regression and Machine Learning</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ustat.html"><a href="ustat.html"><i class="fa fa-check"></i><b>6</b> U-Statistics</a><ul>
<li class="chapter" data-level="6.1" data-path="ustat.html"><a href="ustat.html#definition-3"><i class="fa fa-check"></i><b>6.1</b> Definition</a></li>
<li class="chapter" data-level="6.2" data-path="ustat.html"><a href="ustat.html#examples"><i class="fa fa-check"></i><b>6.2</b> Examples</a><ul>
<li class="chapter" data-level="6.2.1" data-path="ustat.html"><a href="ustat.html#example-1-the-sample-mean"><i class="fa fa-check"></i><b>6.2.1</b> Example 1: The Sample Mean</a></li>
<li class="chapter" data-level="6.2.2" data-path="ustat.html"><a href="ustat.html#example-2-the-sample-variance"><i class="fa fa-check"></i><b>6.2.2</b> Example 2: The Sample Variance</a></li>
<li class="chapter" data-level="6.2.3" data-path="ustat.html"><a href="ustat.html#example-3-ginis-mean-difference"><i class="fa fa-check"></i><b>6.2.3</b> Example 3: Gini’s Mean Difference</a></li>
<li class="chapter" data-level="6.2.4" data-path="ustat.html"><a href="ustat.html#example-4-wilcoxon-signed-rank-statistic"><i class="fa fa-check"></i><b>6.2.4</b> Example 4: Wilcoxon Signed Rank Statistic</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="ustat.html"><a href="ustat.html#inference-using-u-statistics"><i class="fa fa-check"></i><b>6.3</b> Inference using U-statistics</a></li>
<li class="chapter" data-level="6.4" data-path="ustat.html"><a href="ustat.html#u-statistics-for-two-sample-problems"><i class="fa fa-check"></i><b>6.4</b> U-statistics for Two-Sample Problems</a><ul>
<li class="chapter" data-level="6.4.1" data-path="ustat.html"><a href="ustat.html#the-mann-whitney-statistic"><i class="fa fa-check"></i><b>6.4.1</b> The Mann-Whitney Statistic</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="ustat.html"><a href="ustat.html#measures-of-association"><i class="fa fa-check"></i><b>6.5</b> Measures of Association</a><ul>
<li class="chapter" data-level="6.5.1" data-path="ustat.html"><a href="ustat.html#spearmans-rank-correlation"><i class="fa fa-check"></i><b>6.5.1</b> Spearman’s Rank Correlation</a></li>
<li class="chapter" data-level="6.5.2" data-path="ustat.html"><a href="ustat.html#kendalls-tau"><i class="fa fa-check"></i><b>6.5.2</b> Kendall’s tau</a></li>
<li class="chapter" data-level="6.5.3" data-path="ustat.html"><a href="ustat.html#distance-covariance-and-correlation"><i class="fa fa-check"></i><b>6.5.3</b> Distance Covariance and Correlation</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Nonparametric Estimation</b></span></li>
<li class="chapter" data-level="7" data-path="edf.html"><a href="edf.html"><i class="fa fa-check"></i><b>7</b> The Empirical Distribution Function</a><ul>
<li class="chapter" data-level="7.1" data-path="edf.html"><a href="edf.html#definition-and-basic-properties"><i class="fa fa-check"></i><b>7.1</b> Definition and Basic Properties</a></li>
<li class="chapter" data-level="7.2" data-path="edf.html"><a href="edf.html#confidence-intervals-for-ft"><i class="fa fa-check"></i><b>7.2</b> Confidence intervals for F(t)</a></li>
<li class="chapter" data-level="7.3" data-path="edf.html"><a href="edf.html#the-empirical-distribution-function-in-r"><i class="fa fa-check"></i><b>7.3</b> The Empirical Distribution Function in R</a></li>
<li class="chapter" data-level="7.4" data-path="edf.html"><a href="edf.html#the-kolmogorov-smirnov-test"><i class="fa fa-check"></i><b>7.4</b> The Kolmogorov-Smirnov Test</a></li>
<li class="chapter" data-level="7.5" data-path="edf.html"><a href="edf.html#the-empirical-distribution-function-and-statistical-functionals"><i class="fa fa-check"></i><b>7.5</b> The empirical distribution function and statistical functionals</a></li>
<li class="chapter" data-level="7.6" data-path="edf.html"><a href="edf.html#additional-reading-2"><i class="fa fa-check"></i><b>7.6</b> Additional Reading</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="density-estimation.html"><a href="density-estimation.html"><i class="fa fa-check"></i><b>8</b> Density Estimation</a><ul>
<li class="chapter" data-level="8.1" data-path="density-estimation.html"><a href="density-estimation.html#introduction"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="density-estimation.html"><a href="density-estimation.html#histograms"><i class="fa fa-check"></i><b>8.2</b> Histograms</a><ul>
<li class="chapter" data-level="8.2.1" data-path="density-estimation.html"><a href="density-estimation.html#definition-5"><i class="fa fa-check"></i><b>8.2.1</b> Definition</a></li>
<li class="chapter" data-level="8.2.2" data-path="density-estimation.html"><a href="density-estimation.html#histograms-in-r"><i class="fa fa-check"></i><b>8.2.2</b> Histograms in R</a></li>
<li class="chapter" data-level="8.2.3" data-path="density-estimation.html"><a href="density-estimation.html#performance-of-the-histogram-estimate-and-bin-width-selection"><i class="fa fa-check"></i><b>8.2.3</b> Performance of the Histogram Estimate and Bin Width Selection</a></li>
<li class="chapter" data-level="8.2.4" data-path="density-estimation.html"><a href="density-estimation.html#choosing-the-histogram-bin-width"><i class="fa fa-check"></i><b>8.2.4</b> Choosing the Histogram Bin Width</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="density-estimation.html"><a href="density-estimation.html#a-box-type-density-estimate"><i class="fa fa-check"></i><b>8.3</b> A Box-type Density Estimate</a></li>
<li class="chapter" data-level="8.4" data-path="density-estimation.html"><a href="density-estimation.html#kernel-density-estimation"><i class="fa fa-check"></i><b>8.4</b> Kernel Density Estimation</a><ul>
<li class="chapter" data-level="8.4.1" data-path="density-estimation.html"><a href="density-estimation.html#definition-6"><i class="fa fa-check"></i><b>8.4.1</b> Definition</a></li>
<li class="chapter" data-level="8.4.2" data-path="density-estimation.html"><a href="density-estimation.html#bias-variance-and-amise-of-kernel-density-estimates"><i class="fa fa-check"></i><b>8.4.2</b> Bias, Variance, and AMISE of Kernel Density Estimates</a></li>
<li class="chapter" data-level="8.4.3" data-path="density-estimation.html"><a href="density-estimation.html#bandwidth-selection-with-the-normal-reference-rule-and-silvermans-rule-of-thumb"><i class="fa fa-check"></i><b>8.4.3</b> Bandwidth Selection with the Normal Reference Rule and Silverman’s “Rule of Thumb”</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="density-estimation.html"><a href="density-estimation.html#cross-validation-for-bandwidth-selection"><i class="fa fa-check"></i><b>8.5</b> Cross-Validation for Bandwidth Selection</a><ul>
<li class="chapter" data-level="8.5.1" data-path="density-estimation.html"><a href="density-estimation.html#squared-error-cross-validation"><i class="fa fa-check"></i><b>8.5.1</b> Squared-Error Cross-Validation</a></li>
<li class="chapter" data-level="8.5.2" data-path="density-estimation.html"><a href="density-estimation.html#computing-the-cross-validation-bandwidth"><i class="fa fa-check"></i><b>8.5.2</b> Computing the Cross-validation Bandwidth</a></li>
<li class="chapter" data-level="8.5.3" data-path="density-estimation.html"><a href="density-estimation.html#likelihood-cross-validation"><i class="fa fa-check"></i><b>8.5.3</b> Likelihood Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="density-estimation.html"><a href="density-estimation.html#density-estimation-in-r"><i class="fa fa-check"></i><b>8.6</b> Density Estimation in R</a></li>
<li class="chapter" data-level="8.7" data-path="density-estimation.html"><a href="density-estimation.html#additional-reading-3"><i class="fa fa-check"></i><b>8.7</b> Additional Reading</a></li>
</ul></li>
<li class="part"><span><b>III Quantifying Uncertainty</b></span></li>
<li class="chapter" data-level="9" data-path="bootstrap-main.html"><a href="bootstrap-main.html"><i class="fa fa-check"></i><b>9</b> The Bootstrap</a><ul>
<li class="chapter" data-level="9.1" data-path="bootstrap-main.html"><a href="bootstrap-main.html#introduction-1"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="bootstrap-main.html"><a href="bootstrap-main.html#description-of-the-bootstrap"><i class="fa fa-check"></i><b>9.2</b> Description of the Bootstrap</a><ul>
<li class="chapter" data-level="9.2.1" data-path="bootstrap-main.html"><a href="bootstrap-main.html#description"><i class="fa fa-check"></i><b>9.2.1</b> Description</a></li>
<li class="chapter" data-level="9.2.2" data-path="bootstrap-main.html"><a href="bootstrap-main.html#example-confidence-intervals-for-the-rate-parameter-of-an-exponential-distribution"><i class="fa fa-check"></i><b>9.2.2</b> Example: Confidence Intervals for the Rate Parameter of an Exponential Distribution</a></li>
<li class="chapter" data-level="9.2.3" data-path="bootstrap-main.html"><a href="bootstrap-main.html#example-confidence-intervals-for-the-ratio-of-two-quantiles"><i class="fa fa-check"></i><b>9.2.3</b> Example: Confidence Intervals for the Ratio of Two Quantiles</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="bootstrap-main.html"><a href="bootstrap-main.html#why-is-the-bootstrap-procedure-reasonable"><i class="fa fa-check"></i><b>9.3</b> Why is the Bootstrap Procedure Reasonable?</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ci.html"><a href="ci.html"><i class="fa fa-check"></i><b>10</b> The Bootstrap and Confidence Intervals</a><ul>
<li class="chapter" data-level="10.1" data-path="ci.html"><a href="ci.html#bootstrapping"><i class="fa fa-check"></i><b>10.1</b> Bootstrapping</a></li>
</ul></li>
<li class="part"><span><b>IV Nonparametric Regression: Part I</b></span></li>
<li class="chapter" data-level="11" data-path="kernel-regression.html"><a href="kernel-regression.html"><i class="fa fa-check"></i><b>11</b> Kernel Regression</a><ul>
<li class="chapter" data-level="11.1" data-path="kernel-regression.html"><a href="kernel-regression.html#introduction-2"><i class="fa fa-check"></i><b>11.1</b> Introduction</a><ul>
<li class="chapter" data-level="11.1.1" data-path="kernel-regression.html"><a href="kernel-regression.html#an-example"><i class="fa fa-check"></i><b>11.1.1</b> An Example</a></li>
<li class="chapter" data-level="11.1.2" data-path="kernel-regression.html"><a href="kernel-regression.html#linear-smoothers-and-naive-nonparametric-estimates"><i class="fa fa-check"></i><b>11.1.2</b> Linear Smoothers and Naive Nonparametric Estimates</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="kernel-regression.html"><a href="kernel-regression.html#the-nadaraya-watson-estimator"><i class="fa fa-check"></i><b>11.2</b> The Nadaraya-Watson estimator</a></li>
<li class="chapter" data-level="11.3" data-path="kernel-regression.html"><a href="kernel-regression.html#local-linear-and-polynomial-regression"><i class="fa fa-check"></i><b>11.3</b> Local Linear and Polynomial Regression</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="inference-for-regression.html"><a href="inference-for-regression.html"><i class="fa fa-check"></i><b>12</b> Splines and Penalized Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="inference-for-regression.html"><a href="inference-for-regression.html#introduction-3"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="inference-for-regression.html"><a href="inference-for-regression.html#spline-basis-functions"><i class="fa fa-check"></i><b>12.2</b> Spline Basis Functions</a></li>
<li class="chapter" data-level="12.3" data-path="inference-for-regression.html"><a href="inference-for-regression.html#smoothing-splinespenalized-regression"><i class="fa fa-check"></i><b>12.3</b> Smoothing Splines/Penalized Regression</a><ul>
<li class="chapter" data-level="12.3.1" data-path="inference-for-regression.html"><a href="inference-for-regression.html#selection-of-smoothing-parameter"><i class="fa fa-check"></i><b>12.3.1</b> Selection of Smoothing Parameter</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>V Nonparametric Regression: Part II</b></span></li>
<li class="chapter" data-level="13" data-path="decision-tree.html"><a href="decision-tree.html"><i class="fa fa-check"></i><b>13</b> Decision Trees and CART</a></li>
<li class="chapter" data-level="14" data-path="ensemble.html"><a href="ensemble.html"><i class="fa fa-check"></i><b>14</b> Ensemble Methods for Prediction</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Elements of Nonparametric Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="intro" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> Introduction</h1>
<hr />
<hr />
<div id="sec:whatisnonpar" class="section level2">
<h2><span class="header-section-number">1.1</span> What is Nonparametric Statistics?</h2>
<p><strong>What is Parametric Statistics?</strong></p>
<ul>
<li><p>Parametric models refer to probability distributions that can
be fully described by a fixed number of parameters that do not change
with the sample size.</p></li>
<li>Typical examples include
<ul>
<li>Gaussian</li>
<li>Poisson</li>
<li>Exponential</li>
<li>Beta</li>
</ul></li>
<li><p>Could also refer to a regression setting where the mean function
is described by a fixed number of parameters.</p></li>
</ul>
<p><strong>What is Nonparametric Statistics?</strong></p>
<ul>
<li><p>It is difficult to give a concise, all-encompassing definition, but nonparametric
statistics generally refers to statistical methods where there is not a clear parametric component.</p></li>
<li><p>A more practical definition is that nonparametric statistics refers to flexible statistical procedures where
very few assumptions are made regarding the distribution of the data or the form
of a regression model.</p></li>
<li><p>The uses of nonparametric methods in several common statistical contexts are described in Sections <a href="intro.html#sec:example-nonpar-tests">1.3</a> - <a href="intro.html#sec:example-nonpar-regress2">1.7</a>.</p></li>
</ul>
</div>
<div id="sec:course-outline" class="section level2">
<h2><span class="header-section-number">1.2</span> Outline of Course</h2>
<p>This course is roughly divided into the following 5 categories.</p>
<ol style="list-style-type: decimal">
<li><strong>Nonparametric Testing</strong>
<ul>
<li>Rank-based Tests</li>
<li>Permutation Tests</li>
</ul></li>
<li><strong>Estimation of Basic Nonparametric Quantities</strong>
<ul>
<li>The Empirical Distribution Function</li>
<li>Density Estimation</li>
</ul></li>
<li><strong>Nonparametric Confidence Intervals</strong>
<ul>
<li>Bootstrap</li>
<li>Jacknife</li>
</ul></li>
<li><strong>Nonparametric Regression Part I (Smoothing Methods)</strong>
<ul>
<li>Kernel Methods</li>
<li>Splines</li>
<li>Local Regression</li>
</ul></li>
<li><strong>Nonparametric Regression Part II (Machine Learning Methods)</strong>
<ul>
<li>Decision Trees/CART</li>
<li>Ensemble Methods</li>
</ul></li>
</ol>
</div>
<div id="sec:example-nonpar-tests" class="section level2">
<h2><span class="header-section-number">1.3</span> Example 1: Nonparametric vs. Parametric Two-Sample Testing</h2>
<p>Suppose we have data from two groups. For example, outcomes from
two different treatments.</p>
<ul>
<li><p><strong>Group 1 outcomes</strong>: <span class="math inline">\(X_{1}, \ldots, X_{n}\)</span> an i.i.d (independent and identically distributed) sample from distribution function <span class="math inline">\(F_{X}\)</span>.
This means that
<span class="math display">\[\begin{equation}
F_{X}(t) = P( X_{i} \leq t) \quad \textrm{ for any } 1 \leq i \leq n  \nonumber
\end{equation}\]</span></p></li>
<li><p><strong>Group 2 outcomes</strong>: <span class="math inline">\(Y_{1}, \ldots, Y_{m}\)</span> an i.i.d. sample from distribution function <span class="math inline">\(F_{Y}\)</span>.
<span class="math display">\[\begin{equation}
F_{Y}(t) = P( Y_{i} \leq t) \quad \textrm{ for any } 1 \leq i \leq n  \nonumber
\end{equation}\]</span></p></li>
<li><p>To test the impact of a new treatment, we usually want to test whether or not <span class="math inline">\(F_{X}\)</span> differs from <span class="math inline">\(F_{Y}\)</span> in some way.
This can be stated in hypothesis testing language as
<span class="math display" id="eq:nonpar-twosample-hypothesis">\[\begin{eqnarray}
H_{0}&amp;:&amp; F_{X} = F_{Y} \quad \textrm{( populations are the same)} \nonumber \\
H_{A}&amp;:&amp; F_{X} \neq F_{Y} \quad \textrm{( populations are different)} \tag{1.1}
                                                                      \end{eqnarray}\]</span></p></li>
</ul>
<p><strong>Parametric Tests</strong></p>
<ul>
<li><p>Perhaps the most common parametric test for <a href="intro.html#eq:nonpar-twosample-hypothesis">(1.1)</a> is the <strong>t-test</strong>. The t-test assumes that
<span class="math display">\[\begin{equation}
F_{X} = \textrm{Normal}(\mu_{x}, \sigma^{2}) \quad \textrm{ and } \quad F_{Y} = \textrm{Normal}(\mu_{y}, \sigma^{2})
\end{equation}\]</span></p></li>
<li><p>Under this parametric assumption, the hypothesis test <a href="intro.html#eq:nonpar-twosample-hypothesis">(1.1)</a> reduces to
<span class="math display">\[\begin{equation}
H_{0}: \mu_{x} = \mu_{y}  \quad \textrm{ vs. } \quad H_{A}: \mu_{x} \neq \mu_{y}
\end{equation}\]</span></p></li>
<li><p>The standard t-statistic (with a pooled estimate of <span class="math inline">\(\sigma^{2}\)</span>) is the following
<span class="math display">\[\begin{equation}
T = \frac{\bar{X} - \bar{Y}}{ s_{p}\sqrt{\frac{1}{n} + \frac{1}{m}}  },
\end{equation}\]</span>
where <span class="math inline">\(\bar{X} = \frac{1}{n}\sum_{i=1}^{n} X_{i}\)</span> and <span class="math inline">\(\bar{Y} = \frac{1}{m}\sum_{i=1}^{m} Y_{i}\)</span> are
the group-specific sample means and <span class="math inline">\(s_{p}^{2}\)</span> is the pooled estimate of <span class="math inline">\(\sigma^{2}\)</span>
<span class="math display">\[\begin{equation}
s_{p}^{2} = \frac{1}{m + n - 2}\Big\{ \sum_{i=1}^{n} (X_{i} - \bar{X})^{2} + \sum_{i=1}^{m} (Y_{i} - \bar{Y})^{2}   \Big\}
\end{equation}\]</span></p></li>
</ul>
<hr />
<ul>
<li><p>The t-test is based on the <strong>null distribution</strong> of <span class="math inline">\(T\)</span> - the distribution of <span class="math inline">\(T\)</span> under the null hypothesis.</p></li>
<li><p>Under the assumption of normality, the null distribution of <span class="math inline">\(T\)</span> is a t distribution with <span class="math inline">\(n + m - 2\)</span> degrees of freedom.</p></li>
</ul>
<p><img src="01-Introduction_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<ul>
<li><p>Notice that the null distribution of <span class="math inline">\(T\)</span> depends on the parametric assumption that both <span class="math inline">\(F_{X} = \textrm{Normal}(\mu_{x}, \sigma^{2})\)</span>
and <span class="math inline">\(F_{Y} = \textrm{Normal}(\mu_{y}, \sigma^{2})\)</span>. Appealing to the Central Limit Theorem, one could
argue that is a quite reasonable assumption.</p></li>
<li><p>In addition to using the assumption that <span class="math inline">\(F_{X} = \textrm{Normal}(\mu_{x}, \sigma^{2})\)</span> and <span class="math inline">\(F_{Y} = \textrm{Normal}(\mu_{y}, \sigma^{2})\)</span>, we used this parametric assumption (at least implicitly) in the formulation of the hypothesis test itself because we assumed that any difference between <span class="math inline">\(F_{X}\)</span> and <span class="math inline">\(F_{Y}\)</span> would be fully described by difference in <span class="math inline">\(\mu_{x}\)</span> and <span class="math inline">\(\mu_{y}\)</span>.</p></li>
<li><p>So, in a sense, you are using the assumption of normality twice in the construction of the two-sample t-test.</p></li>
</ul>
<hr />
<p><strong>Nonparametric Tests</strong></p>
<ul>
<li><p>Two-sample nonparametric tests are meant to be “distribution-free”. This means the null distribution of the test statistic does not depend on any parametric
assumptions about the two populations <span class="math inline">\(F_{X}\)</span> and <span class="math inline">\(F_{Y}\)</span>.</p></li>
<li><p>Many such tests are based on <strong>ranks</strong>. The distribution of the ranks under the assumption that <span class="math inline">\(F_{X} = F_{Y}\)</span> do
not depend on the form of <span class="math inline">\(F_{X}\)</span> (assuming <span class="math inline">\(F_{X}\)</span> is continuous).</p></li>
<li><p>Also, the statements of hypotheses tests for nonparametric tests should not rely on any parametric assumptions about <span class="math inline">\(F_{X}\)</span> and <span class="math inline">\(F_{Y}\)</span>.</p></li>
<li><p>For example, <span class="math inline">\(H_{A}: F_{X} \neq F_{Y}\)</span> or <span class="math inline">\(H_{A}: F_{X} \geq F_{Y}\)</span>.</p></li>
</ul>
<hr />
<ul>
<li><p>Nonparametric tests usually tradeoff power for greater robustness.</p></li>
<li><p>In general, if the parametric assumptions are correct, a nonparametric test will have less power than its parametric counterpart.</p></li>
<li><p>If the parametric assumptions are not correct, parametric tests might have inappropriate type-I error control
or lose power.</p></li>
</ul>
</div>
<div id="sec:example-nonpar-estimation" class="section level2">
<h2><span class="header-section-number">1.4</span> Example 2: Nonparametric Estimation</h2>
<ul>
<li><p>Suppose we have <span class="math inline">\(n\)</span> observations <span class="math inline">\((X_{1}, \ldots, X_{n})\)</span> which are assumed to be i.i.d. (independent and identically distributed).
The distribution function of <span class="math inline">\(X_{i}\)</span> is <span class="math inline">\(F_{X}\)</span>.</p></li>
<li><p>Suppose we are interested in estimating the entire distribution function <span class="math inline">\(F_{X}\)</span> rather than specific features
of the distribution of <span class="math inline">\(X_{i}\)</span> such as the mean or standard deviation.</p></li>
<li>In a <strong>parametric</strong> approach to estimating <span class="math inline">\(F_{X}\)</span>, we would assume the distribution of <span class="math inline">\(X_{i}\)</span> belongs to some parametric family of distributions.
For example,
<ul>
<li><span class="math inline">\(X_{i} \sim \textrm{Normal}(\mu, \sigma^{2})\)</span></li>
<li><span class="math inline">\(X_{i} \sim \textrm{Exponential}(\lambda)\)</span></li>
<li><span class="math inline">\(X_{i} \sim \textrm{Beta}(\alpha, \beta)\)</span></li>
</ul></li>
</ul>
<hr />
<ul>
<li><p>If we assume that <span class="math inline">\(X_{i} \sim \textrm{Normal}( \mu, \sigma^{2} )\)</span>, we only need to estimate 2 parameters to
fully describe the distribution of <span class="math inline">\(X_{i}\)</span>, and the number of parameters will not depend on the sample size.</p></li>
<li><p>In a nonparametric approach to characterizing the distribution of <span class="math inline">\(X_{i}\)</span>, we need to instead
estimate the entire distribution function <span class="math inline">\(F_{X}\)</span> or density function <span class="math inline">\(f_{X}\)</span>.</p></li>
<li><p>The distribution function <span class="math inline">\(F_{X}\)</span> is usually estimated by the <strong>empirical distribution function</strong>
<span class="math display">\[\begin{equation}
\hat{F}_{n}(t) = \frac{1}{n}\sum_{i=1}^{n} I( X_{i} \leq t),
\end{equation}\]</span>
where <span class="math inline">\(I()\)</span> denotes the indicator function. That is, <span class="math inline">\(I( X_{i} \leq t) = 1\)</span> if <span class="math inline">\(X_{i} \leq t\)</span>,
and <span class="math inline">\(I(X_{i} \leq t) = 0\)</span> if <span class="math inline">\(X_{i} &gt; t\)</span>.</p></li>
<li><p>The empirical distribution function is a discrete distribution function,
and it can be thought of as an estimate having <span class="math inline">\(n\)</span> “parameters”.</p></li>
</ul>
<hr />
<ul>
<li><p>Kernel density estimation is probably the most common nonparametric method for estimating
a probability distribution function <span class="math inline">\(f_{X}(t) = F_{X}&#39;(t)\)</span>.</p></li>
<li><p>The density function of <span class="math inline">\(X_{i}\)</span> is often estimated by a kernel density estimator (KDE). This
is defined as
<span class="math display">\[\begin{equation}
\hat{f}_{n}(t) = \frac{1}{n h_{n}} \sum_{i=1}^{n} K\Big( \frac{t - X_{i}}{ h_{n} } \Big).
\end{equation}\]</span></p></li>
<li><span class="math inline">\(K()\)</span> - the kernel function</li>
<li><p><span class="math inline">\(h_{n}\)</span> - the bandwidth</p></li>
<li><p>The KDE is a type of smoothing procedure.</p></li>
</ul>
<p><img src="01-Introduction_files/figure-html/unnamed-chunk-3-1.png" width="672" /><img src="01-Introduction_files/figure-html/unnamed-chunk-3-2.png" width="672" /></p>
</div>
<div id="sec:example-nonpar-confint" class="section level2">
<h2><span class="header-section-number">1.5</span> Example 3: Confidence Intervals</h2>
<ul>
<li><p>Inference for a wide range of statistical procedures is based on the following argument
<span class="math display" id="eq:normal-approx">\[\begin{equation}
\hat{\theta}_{n} \textrm{ has an approximate Normal}\Big( \theta, \widehat{\textrm{Var}(\hat{\theta}_{n})} \Big) \textrm{ distribution }
\tag{1.2}
\end{equation}\]</span></p></li>
<li><p>Above, <span class="math inline">\(\hat{\theta}_{n}\)</span> is an estimate of a parameter <span class="math inline">\(\theta\)</span>, and <span class="math inline">\(\widehat{\textrm{Var}(\hat{\theta}_{n})}\)</span> is an estimate of the variance of <span class="math inline">\(\hat{\theta}_{n}\)</span>.</p></li>
<li><p><span class="math inline">\(se_{n} = \sqrt{\widehat{\textrm{Var}(\hat{\theta}_{n})}}\)</span> is usually referred to as the <strong>standard error</strong>.</p></li>
<li><p><span class="math inline">\(95\%\)</span> confidence intervals are reported using the following formula
<span class="math display">\[\begin{equation}
[\hat{\theta}_{n} - 1.96 se_{n}, \hat{\theta}_{n} + 1.96 se_{n}  ]
\end{equation}\]</span></p></li>
<li>Common examples of this include:
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\hat{\theta}_{n} = \bar{X}_{n}\)</span>.</li>
</ol>
<p>In this case, appeals to the Central Limit Theorem would justify approximation <a href="intro.html#eq:normal-approx">(1.2)</a>. The variance of <span class="math inline">\(\hat{\theta}_{n}\)</span> would be <span class="math inline">\(\sigma^{2}/n\)</span>, and the standard error would typically be <span class="math inline">\(se_{n} = \hat{\sigma}/\sqrt{n}\)</span>.</p>
<ol start="2" style="list-style-type: decimal">
<li><span class="math inline">\(\hat{\theta}_{n} = \textrm{Maximum Likelihood Estimate of } \theta\)</span>.</li>
</ol>
<p>In this case, asymptotics would justify the approximate distribution <span class="math inline">\(\hat{\theta}_{n} \sim \textrm{Normal}(\theta, \frac{1}{nI(\theta)} )\)</span>, where <span class="math inline">\(I(\theta)\)</span> denotes the Fisher information. The standard error in this context is often <span class="math inline">\(se_{n} = \{ n I(\hat{\theta}_{n}) \}^{-1/2}\)</span>.</p></li>
</ul>
<hr />
<ul>
<li><p>Confidence intervals using <a href="intro.html#eq:normal-approx">(1.2)</a> rely on a parametric approximation to the
sampling distribution of the statistic <span class="math inline">\(\hat{\theta}_{n}\)</span>.</p></li>
<li><p>Moreover, even if one wanted to use something like <a href="intro.html#eq:normal-approx">(1.2)</a>, working out
standard error formulas can be a great challenge in more complicated situations.</p></li>
</ul>
<hr />
<ul>
<li><p>The <strong>bootstrap</strong> is a simulation-based approach for computing standard errors and
confidence intervals.</p></li>
<li><p>The bootstrap does not rely on any particular parametric assumptions and
can be applied in almost any context
(though bootstrap confidence intervals can fail to work as desired in some situations).</p></li>
<li><p>Through resampling from the original dataset, the bootstrap uses many possible alternative datasets to
assess the variability in <span class="math inline">\(\hat{\theta}_{n}\)</span>.</p></li>
</ul>
<table border="1">
<tr>
<th>
</th>
<th>
OriginalDat
</th>
<th>
Dat1
</th>
<th>
Dat2
</th>
<th>
Dat3
</th>
<th>
Dat4
</th>
</tr>
<tr>
<td align="center">
Obs. 1
</td>
<td align="center">
0.20
</td>
<td align="center">
0.20
</td>
<td align="center">
0.80
</td>
<td align="center">
0.20
</td>
<td align="center">
0.30
</td>
</tr>
<tr>
<td align="center">
Obs. 2
</td>
<td align="center">
0.50
</td>
<td align="center">
0.20
</td>
<td align="center">
0.80
</td>
<td align="center">
0.20
</td>
<td align="center">
0.70
</td>
</tr>
<tr>
<td align="center">
Obs. 3
</td>
<td align="center">
0.30
</td>
<td align="center">
0.30
</td>
<td align="center">
0.50
</td>
<td align="center">
0.80
</td>
<td align="center">
0.20
</td>
</tr>
<tr>
<td align="center">
Obs. 4
</td>
<td align="center">
0.80
</td>
<td align="center">
0.30
</td>
<td align="center">
0.70
</td>
<td align="center">
0.50
</td>
<td align="center">
0.50
</td>
</tr>
<tr>
<td align="center">
Obs. 5
</td>
<td align="center">
0.70
</td>
<td align="center">
0.70
</td>
<td align="center">
0.20
</td>
<td align="center">
0.30
</td>
<td align="center">
0.20
</td>
</tr>
<tr>
<td align="center">
theta.hat
</td>
<td align="center">
0.50
</td>
<td align="center">
0.34
</td>
<td align="center">
0.60
</td>
<td align="center">
0.40
</td>
<td align="center">
0.38
</td>
</tr>
</table>
<hr />
<ul>
<li><p>In the above example, we have 4 <strong>boostrap replications</strong> for the statistic <span class="math inline">\(\hat{\theta}\)</span>:
<span class="math display">\[\begin{eqnarray}
\hat{\theta}^{(1)} &amp;=&amp; 0.34 \\ 
\hat{\theta}^{(2)} &amp;=&amp; 0.60 \\
\hat{\theta}^{(3)} &amp;=&amp; 0.40 \\ 
\hat{\theta}^{(4)} &amp;=&amp; 0.38
\end{eqnarray}\]</span></p></li>
<li><p>In the above example, the bootstrap standard error for <span class="math inline">\(\hat{\theta}_{n}\)</span> would be
the standard deviation of the bootstrap replications
<span class="math display">\[\begin{eqnarray}
se_{boot} &amp;=&amp; \Big( \frac{1}{3} \sum_{b=1}^{4} \{ \hat{\theta}^{(b)} - \hat{\theta}^{(-)}  \}^{2} \Big)^{1/2} \nonumber \\
&amp;=&amp; \Big( (0.34 - 0.43)^{2}/3 + (0.60 - 0.43)^{2}/3 + (0.40 - 0.43)^{2}/3 + (0.38 - 0.43)^{2}/3 \Big)^{1/2} \nonumber \\
&amp;=&amp; 0.116
\end{eqnarray}\]</span>
where <span class="math inline">\(\hat{\theta}^{(-)} = 0.43\)</span> is the average of the bootstrap replications.</p></li>
<li><p>One would then report the confidence interval <span class="math inline">\([\hat{\theta} - 1.96 \times 0.116, \hat{\theta} + 1.96 \times 0.116]\)</span>.
In practice, the number of bootstrap replications is typically much larger than <span class="math inline">\(4\)</span>.</p></li>
<li><p>It is often better to construct confidence intervals using the percentiles from the bootstrap distribution
of <span class="math inline">\(\hat{\theta}\)</span> rather than use a confidence interval of the form: <span class="math inline">\(\hat{\theta} \pm 1.96 \times se_{boot}\)</span>.</p></li>
</ul>
<div class="figure"><span id="fig:unnamed-chunk-4"></span>
<img src="01-Introduction_files/figure-html/unnamed-chunk-4-1.png" alt="Bootstrap distribution of the sample standard deviation for the age variable from the kidney fitness data. Dasjed vertical lines are placed at the 2.5 and 97.5 percentiles of the bootstrap distribution." width="672" />
<p class="caption">
Figure 1.1: Bootstrap distribution of the sample standard deviation for the age variable from the kidney fitness data. Dasjed vertical lines are placed at the 2.5 and 97.5 percentiles of the bootstrap distribution.
</p>
</div>
</div>
<div id="sec:example-nonpar-regress1" class="section level2">
<h2><span class="header-section-number">1.6</span> Example 4: Nonparametric Regression with a Single Covariate</h2>
<ul>
<li><p>Regression is a common way of modeling the relationship between two different variables.</p></li>
<li><p>Suppose we have <span class="math inline">\(n\)</span> pairs of observations <span class="math inline">\((y_{1}, x_{1}), \ldots, (y_{n}, x_{n})\)</span> where
<span class="math inline">\(y_{i}\)</span> and <span class="math inline">\(x_{i}\)</span> are suspected to have some association.</p></li>
<li><p>Linear regression would assume that these <span class="math inline">\(y_{i}\)</span> and <span class="math inline">\(x_{i}\)</span> are related by the following
<span class="math display">\[\begin{equation}
y_{i} = \beta_{0} + \beta_{1}x_{i} + \varepsilon_{i} 
\end{equation}\]</span>
with the assumption <span class="math inline">\(\varepsilon_{i} \sim \textrm{Normal}(0, \sigma^{2})\)</span> often made.</p></li>
<li><p>In this model, there are only 3 parameters: <span class="math inline">\((\beta_{0}, \beta_{1}, \sigma^{2})\)</span>,
and the number of parameters stays fixed for all <span class="math inline">\(n\)</span>.</p></li>
</ul>
<p><img src="01-Introduction_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<hr />
<ul>
<li><p>The nonparametric counterpart to linear regression is usually formulated in the following way
<span class="math display">\[\begin{equation}
y_{i} = m( x_{i} ) + \varepsilon_{i}
\end{equation}\]</span></p></li>
<li><p>Typically, one makes very few assumptions about the form of the mean function <span class="math inline">\(m\)</span>, and it is not assumed <span class="math inline">\(m\)</span>
can be described by a finite number of parameters.</p></li>
<li><p>There are a large number of nonparametric methods for estimating <span class="math inline">\(m\)</span>.</p></li>
<li><p>One popular method is the use of <strong>smoothing splines</strong>.</p></li>
<li><p>With smoothing splines, one considers mean functions of the form
<span class="math display" id="eq:smoothspline-model">\[\begin{equation}
m(x) = \sum_{j=1}^{n} \beta_{j}g_{j}(x) 
\tag{1.3}
\end{equation}\]</span>
where <span class="math inline">\(g_{1}, \ldots, g_{n}(x)\)</span> are a collection of spline basis functions.</p></li>
</ul>
<hr />
<ul>
<li><p>Because of the large number of parameters in <a href="intro.html#eq:smoothspline-model">(1.3)</a>, one should
estimate the basis function weights <span class="math inline">\(\beta_{j}\)</span> through penalized regression
<span class="math display" id="eq:smoothspline-estimation">\[\begin{equation}
\textrm{minimize} \quad \sum_{i=1}^{n} \Big( y_{i} - \sum_{j=1}^{n} \beta_{j}g_{j}( x_{i} ) \Big)^{2} + \lambda \sum_{i=1}^{n}\sum_{j=1}^{n} \Omega_{ij}\beta_{i}\beta_{j}
\tag{1.4}
\end{equation}\]</span>
where <span class="math inline">\(\Omega_{ij} = \int g_{i}&#39;&#39;(t)g_{j}&#39;&#39;(t) dt\)</span>.</p></li>
<li><p>Using coefficient estimates <span class="math inline">\(\hat{\beta}_{1}, \ldots, \hat{\beta}_{n}\)</span> found from solving <a href="intro.html#eq:smoothspline-model">(1.3)</a>, the nonparametric estimate of the mean function is defined as
<span class="math display">\[\begin{equation}
\hat{m}(x) = \sum_{j=1}^{n} \hat{\beta}_{j}g_{j}(x) 
\end{equation}\]</span></p></li>
<li><p>While the estimation in <a href="intro.html#eq:smoothspline-estimation">(1.4)</a> resembles parametric estimation for linear regression, notice
that the number of parameters to be estimated will change with the sample size.</p></li>
<li><p>Allowing the number of basis functions to grow with <span class="math inline">\(n\)</span> is important. For a sufficiently large number of basis functions, one should be able to approximate the
true mean function <span class="math inline">\(m(x)\)</span> arbitrarily closely.</p></li>
</ul>
<p><img src="01-Introduction_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div id="sec:example-nonpar-regress2" class="section level2">
<h2><span class="header-section-number">1.7</span> Example 5: Classification and Regression Trees (CART)</h2>
<ul>
<li><p>Suppose we now have observations <span class="math inline">\((y_{1}, \mathbf{x}_{1}), \ldots, (y_{n}, \mathbf{x}_{n})\)</span> where
<span class="math inline">\(y_{i}\)</span> is a continuous response and <span class="math inline">\(\mathbf{x}_{i}\)</span> is a p-dimensional vector of covariates.</p></li>
<li><p>Regression trees are a nonparametric approach for predicting <span class="math inline">\(y_{i}\)</span> from <span class="math inline">\(\mathbf{x}_{i}\)</span>.</p></li>
<li><p>Here, the regression function is a <strong>decision tree</strong> rather than some fitted curve.</p></li>
<li><p>With a decision tree, a final prediction from a covariate vector <span class="math inline">\(\mathbf{x}_{i}\)</span> is obtained by answering
a sequence of “yes or no” questions.</p></li>
<li><p>When the responses <span class="math inline">\(y_{i}\)</span> are binary, such trees are referred to as classification trees.
Hence, the name: classification and regression trees (CART).</p></li>
</ul>
<p><img src="01-Introduction_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<!-- If true go left down tree. E.g., if someone is in treatment arm A, they get a prediction of 13.95  -->
<p><img src="01-Introduction_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<hr />
<ul>
<li><p>Classification and regression trees are constructed through <strong>recursive partitioning</strong>.</p></li>
<li><p>Recursive partitioning is the process of deciding if and how to split a given
node into two child nodes.</p></li>
<li><p>Tree splits are usually chosen to minimize the “within-node” sum of squares.</p></li>
<li><p>The size of the final tree is determined by a process of “pruning” the tree
with cross-validation determining the best place to stop pruning.</p></li>
<li><p>Regression trees are an example of a more algorithmic approach to
constructing predictions (as opposed to probability modeling in more
traditional statistical methods) with a strong emphasis on predictive
performance as measured through cross-validation.</p></li>
</ul>
<hr />
<ul>
<li><p>While single regression trees have the advantage of being directly interpretable,
their prediction performance is often not that great.</p></li>
<li><p>However, using collections of trees can be very effective for prediction and
has been used in many popular learning methods. Examples include: random forests,
boosting, and Bayesian additive regression trees (BART).</p></li>
<li><p>Methods such as these can perform well on much larger datasets. We will discuss
additional methods if time allows.</p></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="getting-started.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ElementsNonparStat.pdf"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
