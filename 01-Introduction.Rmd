# Introduction {#intro}

---




---

## What is Nonparametric Statistics? {#sec:whatisnonpar}

**What is Parametric Statistics?**

* Parametric models refer to probability distributions that can 
be fully described by a fixed number of parameters that do not change
with the sample size.

* Typical examples include
   + Gaussian 
   + Poisson
   + Exponential
   + Beta

* Could also refer to a regression setting where the mean function
is described by a fixed number of parameters.

**What is Nonparametric Statistics?**

* It is difficult to give a concise, all-encompassing definition, but nonparametric
statistics generally refers to statistical methods where there is not a clear parametric component.


* The uses of nonparametric methods in several common statistical contexts are described in Sections \@ref(sec:example-nonpar-tests) - \@ref(sec:example-nonpar-regress2).

## Outline of Course {#sec:course-outline}

This course is roughly divided into the following 5 categories.

1. **Nonparametric Testing**
    + Rank-based Tests
    + Permutation Tests
1. **Estimation of Basic Nonparametric Quantities** 
    + The Empirical Distribution Function
    + Density Estimation
1. **Nonparametric Confidence Intervals**
    + Bootstrap 
    + Jacknife
1. **Nonparametric Regression Part I (Smoothing Methods)**
    + Kernel Methods
    + Splines
    + Local Regression
1. **Nonparametric Regression Part II (Machine Learning Methods)**
    + Decision Trees/CART
    + Ensemble Methods
   

## Example 1: Nonparametric vs. Parametric Two-Sample Testing {#sec:example-nonpar-tests}

Suppose we have data from two groups. For example, outcomes from 
two different treatments.

* **Group 1 outcomes**: $X_{1}, \ldots, X_{n}$ an i.i.d (independent and identically distributed) sample from distribution function $F_{X}$. 
This means that 
\begin{equation}
F_{X}(t) = P( X_{i} \leq t) \quad \textrm{ for any } 1 \leq i \leq n  \nonumber
\end{equation}

* **Group 2 outcomes**: $Y_{1}, \ldots, Y_{m}$ an i.i.d. sample from distribution function $F_{Y}$.
\begin{equation}
F_{Y}(t) = P( Y_{i} \leq t) \quad \textrm{ for any } 1 \leq i \leq n  \nonumber
\end{equation}


* To test the impact of a new treatment, we usually want to test whether or not $F_{X}$ differs from $F_{Y}$ in some way.
This can be stated in hypothesis testing language as
\begin{eqnarray}
H_{0}&:& F_{X} = F_{Y} \quad \textrm{( populations are the same)} \nonumber \\
H_{A}&:& F_{X} \neq F_{Y} \quad \textrm{( populations are different)} (\#eq:nonpar-twosample-hypothesis)
\end{eqnarray}

**Parametric Tests**

* Perhaps the most common parametric test for \@ref(eq:nonpar-twosample-hypothesis) is the **t-test**. The t-test assumes that
\begin{equation}
F_{X} = \textrm{Normal}(\mu_{x}, \sigma^{2}) \quad \textrm{ and } \quad F_{Y} = \textrm{Normal}(\mu_{y}, \sigma^{2})
\end{equation}
* Under this parametric assumption, the hypothesis test \@ref(eq:nonpar-twosample-hypothesis) reduces to 
\begin{equation}
H_{0}: \mu_{x} = \mu_{y}  \quad \textrm{ vs. } \quad H_{A}: \mu_{x} \neq \mu_{y}
\end{equation}

* The standard t-statistic (with a pooled estimate of $\sigma^{2}$) is the following
\begin{equation}
T = \frac{\bar{X} - \bar{Y}}{ s_{p}\sqrt{\frac{1}{n} + \frac{1}{m}}  },
\end{equation}
where $\bar{X} = \frac{1}{n}\sum_{i=1}^{n} X_{i}$ and $\bar{Y} = \frac{1}{m}\sum_{i=1}^{m} Y_{i}$ are
the group-specific sample means and $s_{p}^{2}$ is the pooled estimate of $\sigma^{2}$
\begin{equation}
s_{p}^{2} = \frac{1}{m + n - 2}\Big\{ \sum_{i=1}^{n} (X_{i} - \bar{X})^{2} + \sum_{i=1}^{m} (Y_{i} - \bar{Y})^{2}   \Big\}
\end{equation}

---

* The t-test is based on the **null distribution** of $T$ - the distribution of $T$ under the null hypothesis.

* Under the assumption of normality, the null distribution of $T$ is a t distribution
with $n + m - 2$ degrees of freedom.

```{r, echo=FALSE}
xgrid <- seq(-4,4, length.out=500)
yy <- dt(xgrid, df=18)
plot(xgrid, yy, type="n", ylab="Density", xlab="t", main="Null Distribution of T when n = m = 10", las=1)
lines(xgrid, yy, lwd=2)
```

* Notice that the null distribution of $T$ depends on the parametric assumption that both $F_{X} = \textrm{Normal}(\mu_{x}, \sigma^{2})$
and $F_{Y} = \textrm{Normal}(\mu_{y}, \sigma^{2})$. (Mention CLT argument here)

* In addition to using the assumption that $F_{X} = \textrm{Normal}(\mu_{x}, \sigma^{2})$
and $F_{Y} = \textrm{Normal}(\mu_{y}, \sigma^{2})$, we used this parametric assumption in the formulation of the hypothesis test itself because we 
assumed that any difference between $F_{X}$ and $F_{Y}$ would be fully described by difference in $\mu_{x}$ and $\mu_{y}$.

---

**Nonparametric Tests**

* Two-sample nonparametric tests are meant to be "distribution-free". This means
the null distribution of the test statistic does not depend on any parametric
assumptions about the two populations $F_{X}$ and $F_{Y}$. 

* Also, the hypotheses tests themselves do not rely on any parametric assumptions.

* For example, 

## Example 2: Nonparametric Estimation {#sec:example-nonpar-estimation}

* Suppose we have $n$ observations $(X_{1}, \ldots, X_{n})$ which are assumed to be i.i.d. (independent and identically distributed).
The distribution function of $X_{i}$ is $F_{X}$.

* Suppose we are interested in estimating the entire distribution function $F_{X}$ rather than specific features
of the distribution of $X_{i}$ such as the mean or standard deviation. 

* In a **parametric** approach to estimating $F_{X}$, we would assume the distribution of $X_{i}$ belongs to some parametric family of distributions.
For example,
   + $X_{i} \sim \textrm{Normal}(\mu, \sigma^{2})$ 
   + $X_{i} \sim \textrm{Exponential}(\lambda)$
   + $X_{i} \sim \textrm{Beta}(\alpha, \beta)$

* If we assume that $X_{i} \sim \textrm{Normal}( \mu, \sigma^{2} )$, we only need to estimate 2 parameters to
fully describe the distribution of $X_{i}$, and the number of parameters will not depend on the sample size. 


## Example 3: Confidence Intervals {#sec:example-nonpar-confint}


* Inference for a wide range of statistical procedures is 
based on the following argument
\begin{equation}
\hat{\theta}_{n} \textrm{ has an approximate Normal}\Big( \theta, \widehat{\textrm{Var}(\hat{\theta}_{n})} \Big)
\textrm{ distribution }
(\#eq:normal-approx)
\end{equation}

* Above, $\hat{\theta}_{n}$ is an estimate of a parameter $\theta$, and
$\widehat{\textrm{Var}(\hat{\theta}_{n})}$ is an estimate of the variance of $\hat{\theta}_{n}$.

* $se_{n} = \sqrt{\widehat{\textrm{Var}(\hat{\theta}_{n})}}$ is usually referred to as the **standard error**.

* $95\%$ Confidence intervals are reported using the following formula
\begin{equation}
[\hat{\theta}_{n} - 1.96 se_{n}, \hat{\theta}_{n} + 1.96 se_{n}  ]
\end{equation}

* Some typical examples of this include

   + $\hat{\theta}_{n} = \bar{X}_{n}$. In this case, appeals to the Central Limit Theorem would
    justify approximation \@ref(eq:normal-approx). The variance of $\hat{\theta}_{n}$ would be $\sigma^{2}$, and the standard error would typically be $se_{n} = \hat{\sigma}/\sqrt{n}$.
    
    + $\hat{\theta}_{n} = \textrm{Maximum Likelihood Estimate of } \theta$. In this case, asymptotics 
    would justify the approximate distribution $\hat{\theta}_{n} \sim \textrm{Normal}(\theta, \frac{1}{nI(\theta)} )$, where $I(\theta)$ denotes the Fisher information. The standard error
    in this context is often $se_{n} = 1/\sqrt{n I(\hat{\theta}_{n})}$.

---


## Example 4: Nonparametric Regression with a Single Covariate {#sec:example-nonpar-regress1}

* Regression is a common way of modeling the relationship between two different variables.

* Suppose we have $n$ pairs of observations $(y_{1}, x_{1}), \ldots, (y_{n}, x_{n})$ where
$y_{i}$ and $x_{i}$ are suspected to have some association.

* Linear regression would assume that these $y_{i}$ and $x_{i}$ are related by the following
\begin{equation}
y_{i} = \beta_{0} + \beta_{1}x_{i} + \varepsilon_{i} 
\end{equation}
with the assumption $\varepsilon_{i} \sim \textrm{Normal}(0, \sigma^{2})$ often made.

* In this model, there are only 3 parameters: $(\beta_{0}, \beta_{1}, \sigma^{2})$,
and the number of parameters stays fixed for all $n$.

```{r, echo=FALSE}
xx <- runif(200, min=-1/2, max=1)
yy <- xx^3 + .1*rnorm(200)
lm.unif <- lm(yy ~ xx)
plot(xx, yy, xlab="x", ylab="y", type="n", las=1, main="Linear regression model for (xi, yi) with fitted curve")
points(xx, yy, pch=16, cex=.8)
lines(xx, lm.unif$coef[1] + lm.unif$coef[2]*xx, lty=2, lwd=2)
```

---

* The nonparametric counterpart to linear regression is usually formulated in the following way
\begin{equation}
y_{i} = m( x_{i} ) + \varepsilon_{i}
\end{equation}

* Typically, one makes very few assumptions about the form of the mean function $m$, and it is not assumed $m$
can be described by a finite number of parameters.

* There are a large number of nonparametric methods for estimating $m$.

* One popular method is the use of **smoothing splines**.

* With smoothing splines one considers mean functions of the form
\begin{equation}
m(x) = \sum_{j=1}^{n} \beta_{j}g_{j}(x) 
\end{equation}




## Example 5: Nonparametric Regression {#sec:example-nonpar-regress2}

