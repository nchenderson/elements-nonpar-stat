# Splines and Penalized Regression {#inference-for-regression}

## Introduction

* In this chapter, we will focus on using basis functions for estimating 
the regression function.

* That is, we will look at regression function estimates of the form
\begin{equation}
\hat{m}(x) = \hat{\beta}_{0}\varphi_{0}(x) + \sum_{j=1}^{p} \hat{\beta}_{j}\varphi_{j}(x)  \nonumber
\end{equation}
where $\varphi_{0}(x), \varphi_{1}(x), \ldots, \varphi_{p}(x)$ will be referred to as basis functions.

* We will usually either ignore $\varphi_{0}(x)$ or assume that $\varphi_{0}(x) = 1$.

* If you use a relatively large number of appropriately chosen basis functions,
you can represent even quite complicated functions with some linear
combination of the basis functions.

---

**Examples**


---

### Regressogram (Piecewise Constant Estimate)

* Let's consider the regressogram again. The regressogram estimate could be written as
\begin{equation}
\hat{m}_{h_{n}}^{R}(x) = \sum_{k=1}^{D_{n}} a_{k, h_{n}}\varphi_{k}(x) = \sum_{k=1}^{D_{n}} a_{k,h_{n}} I\big( x \in B_{k} \big)
\end{equation}
where the coefficents $a_{k, h_{n}}$ are given by
\begin{equation}
a_{k, h_{n}} = \frac{1}{ n_{k,h_{n}} } \sum_{i=1}^{n} Y_{i}I\big( x_{i} \in B_{k} \big)  \nonumber
\end{equation}

* We can think of the regressogram as a basis function estimate with the basis functions 
\begin{equation}
\varphi_{1}(x) = I\big( x \in B_{1} \big), \ldots, \varphi_{D_{n}}(x) = I\big( x \in B_{D_{n}} \big) \nonumber
\end{equation}

* The regressogram estimate will be a piecewise constant function that is constant within each of the bins.

```{r, echo=FALSE, fig.height=3.7, fig.cap="Basis functions for a regressogram with the following 3 bins: [0,1/3), [1/3, 2/3), [2/3, 1)"}
par(mfrow=c(1,3), mar=c(4.0, 4.0, 3.0, 0.25))
plot(0,0, type="n", xlab="x", ylab="y", main=expression(varphi[1](x)), xlim=c(0,1), ylim=c(0,1.1),
     las=1, cex.main=1.2)
lines(c(0,1/3), c(1,1), lwd=3)
abline(v=1/3, lty=2)
abline(v=2/3, lty=2)
plot(0,0, type="n", xlab="x", ylab="y", main=expression(varphi[2](x)), xlim=c(0,1), ylim=c(0,1.1),
     las=1, cex.main=1.2)
lines(c(1/3,2/3), c(1,1), lwd=3)
abline(v=1/3, lty=2)
abline(v=2/3, lty=2)
plot(0,0, type="n", xlab="x", ylab="y", main=expression(varphi[3](x)), xlim=c(0,1), ylim=c(0,1.1),
     las=1, cex.main=1.2)
lines(c(2/3,1), c(1,1), lwd=3)
abline(v=1/3, lty=2)
abline(v=2/3, lty=2)
```




### Piecewise Linear Estimates

* Instead of a piecewise constant estimate of $m(x)$, we could use an estimate which is piecewise linear 
by using the following $2p$ basis functions
\begin{eqnarray}
\varphi_{1}(x) &=& I(x \in B_{1}) \nonumber \\
&\vdots&  \nonumber \\
\varphi_{p}(x) &=& I(x \in B_{p}) \nonumber \\
\varphi_{p+1}(x) &=& x I(x \in B_{1})  \nonumber \\
&\vdots& \nonumber \\
\varphi_{2p}(x) &=& x I(x \in B_{p}) \nonumber
\end{eqnarray}
if we now let $p = D_{n}$ denote the number of "bins".

* The figure below shows an example of a regression function that is piecewise linear with 3 different bins. 

* While a piecewise linear model is perhaps a more flexible method than the regressogram,
the piecwise linear model will still have big jumps at the bin boundaries and have an overall
unpleasant appearance.

```{r, echo=FALSE, fig.cap="Example of a regression function estimate that is piecewise linear within 3 bins."}
set.seed(53186)
n <- 100
xx <- runif(n)
yy <- (2*(xx-1/2))^3 + rnorm(n,sd=.25)

ind1 <- xx < 1/3
ind2 <- xx < 2/3 & xx > 1/3
ind3 <- xx > 2/3
lm.fit1 <- lm(yy[ind1] ~ xx[ind1])
lm.fit2 <- lm(yy[ind2] ~ xx[ind2])
lm.fit3 <- lm(yy[ind3] ~ xx[ind3])

plot(xx, yy, las=1, xlab="x", ylab="Regression function estimate")
lines(c(0, 1/3), lm.fit1$coef[1] + lm.fit1$coef[2]*c(0, 1/3), lwd=3)
lines(c(1/3, 2/3), lm.fit2$coef[1] + lm.fit2$coef[2]*c(1/3, 2/3), lwd=3)
lines(c(2/3, 1), lm.fit3$coef[1] + lm.fit3$coef[2]*c(2/3, 1), lwd=3)
abline(v=1/3, lty=2)
abline(v=2/3, lty=2)
```


### Piecewise Cubic Estimates 

* If we wanted to allow for more flexible forms of the regression function estimate within
each bin we could fit a higher order polynomial model within each bin. 

* That is, the regression function estimate within the $k^{th}$ bin will have the form
\begin{equation}
\hat{m}(x)I(x \in B_{k}) = \hat{\beta}_{0k} + \hat{\beta}_{1k}x + \hat{\beta}_{2k}x^{2} + \hat{\beta}_{3k}x^{3} \nonumber 
\end{equation}


* To fit a piecewise cubic model with $p$ bins, we would need the following $4p$ basis functions
\begin{eqnarray}
\varphi_{1}(x) &=& I(x \in B_{1}),  \ldots,  \varphi_{p}(x) = I(x \in B_{p}) \nonumber \\
\varphi_{p+1}(x) &=& xI(x \in B_{1}),  \ldots, \varphi_{2p}(x) = xI(x \in B_{p}) \nonumber \\
\varphi_{2p+1}(x) &=& x^{2}I(x \in B_{1}),  \ldots,  \varphi_{3p}(x) = x^{2}I(x \in B_{p}) \nonumber \\
\varphi_{3p+1}(x) &=& x^{3}I(x \in B_{1}), \ldots, \varphi_{4p}(x) = x^{3}I(x \in B_{p}) \nonumber \\
\end{eqnarray}

```{r, echo=FALSE, fig.cap="Example of a regression function estimate that is piecewise cubic within 3 bins."}
lm.cubfit1 <- lm(yy[ind1] ~ xx[ind1] + I(xx[ind1]^2) + I(xx[ind1]^3))
lm.cubfit2 <- lm(yy[ind2] ~ xx[ind2] + I(xx[ind2]^2) + I(xx[ind2]^3))
lm.cubfit3 <- lm(yy[ind3] ~ xx[ind3] + I(xx[ind3]^2) + I(xx[ind3]^3))

tt1 <- seq(0, 1/3, length.out=50)
tt2 <- seq(1/3, 2/3, length.out=50)
tt3 <- seq(2/3, 1, length.out=50)
plot(xx, yy, las=1, xlab="x", ylab="Regression function estimate")
lines(tt1, lm.cubfit1$coef[1] + lm.cubfit1$coef[2]*tt1 + lm.cubfit1$coef[3]*tt1^2 + lm.cubfit1$coef[4]*tt1^3, lwd=3)
lines(tt2, lm.cubfit2$coef[1] + lm.cubfit2$coef[2]*tt2 + lm.cubfit2$coef[3]*tt2^2 + lm.cubfit2$coef[4]*tt2^3, lwd=3)
lines(tt3, lm.cubfit3$coef[1] + lm.cubfit3$coef[2]*tt3 + lm.cubfit3$coef[3]*tt3^2 + lm.cubfit3$coef[4]*tt3^3, lwd=3)
abline(v=1/3, lty=2)
abline(v=2/3, lty=2)
```


## Piecewise Linear Estimates with Continuity (Linear Splines)

* $u_{j}$ will be referred to as knots

\begin{eqnarray}
\varphi_{0}(x) &=& 1  \nonumber \\
\varphi_{1}(x) &=& x  \nonumber \\
\varphi_{2}(x) &=& (x - u_{1})_{+} \nonumber \\
\varphi_{3}(x) &=& (x - u_{2})_{+} \nonumber \\
&\vdots& \nonumber \\
\varphi_{p}(x) &=& (x - u_{p-1})_{+} \nonumber
\end{eqnarray}


### Piecewise Cubic Estimates with constraints 1


## Cubic Splines and Spline Basis Functions

* The B-spline functions are a basis for cubic splines. Are they also a basis 
for the set of natural cubic splines?


## Smoothing Splines/Penalized Regression



