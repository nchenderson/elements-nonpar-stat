# (PART) Nonparametric Regression: Part II {-} 

# Regression Trees and CART {#decision-tree}

## Introduction

* Let's think about the regressogram estimate again. 

* The regressogram estimate of the regression function is a piecewise constant function that is constant within each of $p$ "bins"
\begin{equation}
\hat{m}_{h_{n}}^{R}(x) = \frac{1}{a}\sum_{i=1}^{n} Y_{i} I(x_{i} \in B_{k}), \qquad \textrm{ if } x \in B_{k} \nonumber
\end{equation}

* Figure \@ref(fig:cart-motivate) shows an example of a regressogram estimate with 3 bins. 

```{r, cart-motivate, echo=FALSE, fig.cap="Regressogram estimate with the 3 bins: [0,1/3), [1/3, 2/3), [2/3, 1)."}
set.seed(53186)
n <- 100
xx <- runif(n)
yy <- (2*xx - 1/2)^3 + rnorm(n,sd=.25)

ind1 <- xx < 1/3
ind2 <- xx < 2/3 & xx > 1/3
ind3 <- xx > 2/3

plot(xx, yy, las=1, xlab="x", ylab="Regression function estimate")
lines(c(0, 1/3), rep(mean(yy[ind1]), 2), lwd=3)
lines(c(1/3, 2/3), rep(mean(yy[ind2]), 2), lwd=3)
lines(c(2/3, 1), rep(mean(yy[ind3]), 2), lwd=3)
abline(v=1/3, lty=2)
abline(v=2/3, lty=2)
```

---

* Suppose we were forced to combined two adjacent bins to estimate the regression function. For the data shown in \@ref(fig:cart-motivate), 
which two bins should we combine if we were forced to do so? The only options here are to combine bins 1 and 2 or to combine bins 2 and 3.

* I would say we should combine bins 1 and 2. Look at Figures \@ref(fig:cart-motivate2) and \@ref(fig:cart-motivate3) for a comparison of these two choices.

* The responses $Y_{i}$ change much more over the range of the third bin than they do over the first and second bins. Hence, an 
intercept model for the first two bins is not all that bad. 

* In contrast, an intercept model for the last two bins is a terrible model. 

---

```{r, cart-motivate2, echo=FALSE, fig.cap="Regressogram estimate with the 2 bins: [0,1/3), [1/3, 1)."}

plot(xx, yy, las=1, xlab="x", ylab="Regression function estimate")
lines(c(0, 1/3), rep(mean(yy[ind1]), 2), lwd=3)
lines(c(1/3, 1), rep(mean(yy[ind2 | ind3]), 2), lwd=3)
abline(v=1/3, lty=2)
```

```{r, cart-motivate3, echo=FALSE, fig.cap="Regressogram estimate with the 2 bins: [0,2/3), [2/3, 1)."}

plot(xx, yy, las=1, xlab="x", ylab="Regression function estimate")
lines(c(0, 2/3), rep(mean(yy[ind1 | ind2]), 2), lwd=3)
lines(c(2/3, 1), rep(mean(yy[ind3]), 2), lwd=3)
abline(v=2/3, lty=2)
```

* The intuition for why the choice of bins $[0,2/3), [2/3, 1)$ is better than the choice of bins $[0,1/3), [1/3, 1)$ can 
be formalized by considering the within-bin variation of $Y_{i}$.

* For two bins $B_{1}$ and $B_{2}$, the within-bin sum of squares (WBSS) of $Y_{i}$ is
\begin{equation}
\textrm{WBSS} = \sum_{k=1}^{2} \sum_{i=1} (Y_{i} - \bar{Y}_{k})^{2}I(x_{i} \in B_{k}) \nonumber
\end{equation}

* You want to choose the bins in order to minimize the within-bin sum of squares. The reason for this is that: if the within-bin
sum of squares is low, an intercept model for each bin will fit the data very well.

---

* For the data shown in Figures \@ref(fig:cart-motivate) - \@ref(fig:cart-motivate3), the WBSS when using the bins $[0, 1/3), [1/3, 1)$
is
```{r}
sum((yy[xx < 1/3] - mean(yy[xx < 1/3]))^2) + sum((yy[xx >= 1/3] - mean(yy[xx >= 1/3]))^2) 
```

* The WBSS when using the bins $[0,2/3), [2/3, 1)$ is
```{r}
sum((yy[xx < 2/3] - mean(yy[xx < 2/3]))^2) + sum((yy[xx >= 2/3] - mean(yy[xx >= 2/3]))^2) 
```


## Regression Trees with a Single Covariate

* For a single covariate, regression trees estimate the regression function by a piecewise constant
function that is constant within each of several bins. We will focus on the well-known 
CART (Classification and Regression Trees) method for using regression trees.

* More generally, with multivariate covariates CART will fit a regression function that is constant
within each of many multi-dimensional "rectangles".

* The main difference between CART and the regressogram is that the placements
and widths of the bins in CART are chosen in a more selective manner than the regressogram.

* Specifically, rather than just using a collection of bins of fixed width, CART chooses where to place the bin boundaries by considering
the resulting within-bin sum of squares.

---

* CART constructs the bins through sequential binary splits of the x axis. 

* In the first step, CART will divide the covariates into two bins $B_{1}$ and $B_{2}$. These bins will
have the form $B_{1} = (-\infty, t_{1})$ and $B_{2} = [t_{1}, \infty)$.

* At the next step, CART will create $4$ bins by further dividing each of these two bins into two more bins. So, 
we will have four bins $B_{11}, B_{12}, B_{21}, B_{22}$. 

* Bins $B_{11}$ and $B_{12}$ will have the form $B_{11} = (-\infty, t_{11})$ and $B_{12} = [t_{11}, t_{1})$,
and bins $B_{21}$ and $B_{22}$ will have the form $B_{21} = [t_{1}, t_{21})$ and $B_{22} = [t_{21}, \infty)$.


---

* This sequential process for constructing bins is typically depicted with a binary decision tree.

* Figure \@ref(fig:dectree-basic) shows the decision tree representation of a CART regression function estimate with $4$ bins.

```{r, dectree-basic, echo=FALSE, fig.cap="Binary decision tree representing a regression function estimate with 4 bins where it is assumed that all the covariates are between $0$ and $1$. The 4 bins here are [0, 0.6), [0.6, 0.74), [0.74, 0.89), [0.89, 1)."}
library(rpart)
library(rpart.plot)
model <- rpart(yy ~ xx, cp = .02)
rpart.plot(model, extra=0, cex=1.5)
```

* Figure \@ref(fig:fourbin-example) shows the regression function estimate which corresponds to the decision tree shown in Figure \@ref(fig:dectree-basic).

```{r, fourbin-example, echo=FALSE, fig.cap="Regression function estimate that corresponds to the decision tree shown in the previous figure."}
ind1 <- xx < 0.6
ind2 <- xx < 0.74 & xx >= 0.6
ind3 <- xx < 0.89 & xx >= 0.74
ind4 <- xx >= 0.89

plot(xx, yy, las=1, xlab="x", ylab="Regression function estimate", las=1)
lines(c(0, 0.6), rep(mean(yy[ind1]), 2), lwd=3)
lines(c(0.6, 0.74), rep(mean(yy[ind2]), 2), lwd=3)
lines(c(0.74, 0.89), rep(mean(yy[ind3]), 2), lwd=3)
lines(c(0.89, 1), rep(mean(yy[ind4]), 2), lwd=3)
abline(v=0.6, lty=2)
abline(v=0.74, lty=2)
abline(v=0.89, lty=2)
```


### Determining the Split Points

* The first two bins are determined by the "split point" $t_{1}$. To find this split point, CART looks 
at the within-bin sum of squares induced by a splitting point $t$.
\begin{eqnarray}
\textrm{WBSS}(t) &=& \sum_{k=1}^{2} \sum_{i=1}^{n} (Y_{i} - \bar{Y}_{k})^{2}I(x_{i} \in B_{k}) \nonumber \\
&=&  \sum_{i=1}^{n} (Y_{i} - \bar{Y}_{k})^{2}I(x_{i} < t) + \sum_{i=1}^{n} (Y_{i} - \bar{Y}_{k})^{2}I(x_{i} \geq t) \nonumber
\end{eqnarray}

* The first split point $t_{1}$ is the value of $t$ which minimizes this within-bin sum of squares criterion. That is,
\begin{equation}
t_{1} = \arg\min_{t} \textrm{ WBSS}(t) = \textrm{argmin}_{t \in \{x_{1}, \ldots, x_{n}  \}} \textrm{ WBSS}(t) \nonumber
\end{equation}

* To find $t_{1}$, we only have to take the minimum over the set of covariates since the value of $\textrm{WBSS}(t)$ only changes 
at each $x_{i}$.

* Figure \@ref(fig:wbss-example) shows a plot of $\textrm{WBSS}(t)$ vs. $t$ for the data shown in Figures \@ref(fig:cart-motivate) - \@ref(fig:cart-motivate3).

* Figure \@ref(fig:wbss-example) suggests that the value of $t_{1}$ will be around $0.75$.

```{r, wbss-example, echo=FALSE, fig.cap="Plot of WBSS(t) vs. t for the data shown in the above figures."}
WBSS <- function(y, x) {
   qq <- length(x)
   uu <- sort(x)
   wbss <- numeric(qq) 
   for(k in 1:qq) {
     wbss[k] <- sum((yy[xx < uu[k]] - mean(yy[xx < uu[k]]))^2) + sum((yy[xx >= uu[k]] - mean(yy[xx >= uu[k]]))^2) 
   }
   return(list(x=uu, y=wbss))
}

tmp <- WBSS(y=yy, x=xx)
plot(tmp$x, tmp$y, main="Within-Bin Sum of Squares by First Splitting Point", ylab="WBSS", xlab="split point", las=1)
```

---

* Finding subsequent splitting points. 

## Regression Trees With Multiple Covariates


```{r, echo=FALSE, fig.height=6}
x <- 1:50/51
y <- 1:50/51
z <- matrix(0, nrow=length(x), ncol=length(y))
nx <- length(x)
for(i in 1:nx) {
   for(j in 1:nx) {
       if(x[i] < 0.5) {
           z[i,j] <- ifelse(x[i] < 0.2, 2, 1.2)
       } else if(x[i] >= 0.5) {
           z[i,j] <- ifelse(y[j] < 0.7, 0, 0.66)
       }
   }
}
persp(x, y, z, zlim=c(-.2, 2.5), cex=1.5)
```



