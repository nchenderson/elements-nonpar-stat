# (PART) Nonparametric Regression: Part II {-} 

# Decision Trees and CART {#decision-tree}

## Introduction

* Let's think about the regressogram estimate again. 

* The regressogram estimate of the regression function is a piecewise constant function that is constant within each of $p$ "bins"
\begin{equation}
\hat{m}_{h_{n}}^{R}(x) = \frac{1}{a}\sum_{i=1}^{n} Y_{i} I(x_{i} \in B_{k}), \qquad \textrm{ if } x \in B_{k} \nonumber
\end{equation}

* Figure \@ref(fig:cart-motivate) shows an example of a regressogram estimate with 3 bins. 

```{r, cart-motivate, echo=FALSE, fig.cap="Regressogram estimate with the 3 bins: [0,1/3), [1/3, 2/3), [2/3, 1)."}
set.seed(53186)
n <- 100
xx <- runif(n)
yy <- (2*xx - 1/2)^3 + rnorm(n,sd=.25)

ind1 <- xx < 1/3
ind2 <- xx < 2/3 & xx > 1/3
ind3 <- xx > 2/3

plot(xx, yy, las=1, xlab="x", ylab="Regression function estimate")
lines(c(0, 1/3), rep(mean(yy[ind1]), 2), lwd=3)
lines(c(1/3, 2/3), rep(mean(yy[ind2]), 2), lwd=3)
lines(c(2/3, 1), rep(mean(yy[ind3]), 2), lwd=3)
abline(v=1/3, lty=2)
abline(v=2/3, lty=2)
```

---

* Suppose we were forced to combined two adjacent bins to estimate the regression function. For the data shown in \@ref(fig:cart-motivate), 
which two bins should we combine if we were forced to do so? The only options here are to combine bins 1 and 2 or to combine bins 2 and 3.

* I would say we should combine bins 1 and 2. Look at Figures \@ref(fig:cart-motivate2) and \@ref(fig:cart-motivate3) for a comparison of these two choices.

* The responses $Y_{i}$ change much more over the range of the third bin than they do over the first and second bins. Hence, an 
intercept model for the first two bins is not all that bad. 

* In contrast, an intercept model for the last two bins is a terrible model. 

---

* This intuition for why the choice of bins $[0,2/3), [2/3, 1)$ is better than the choice of bins $[0,1/3), [1/3, 1)$ can 
be formalized by considering the within-bin variance of $Y_{i}$.

* If we have $p$ bins, the within-bin variance of $Y_{i}$ would be
\begin{equation}
WBVar = \sum_{k=1}^{p} \frac{1}{n_{k}-1}\sum_{i=1} (Y_{i} - \bar{Y}_{k})^{2}I(x_{i} \in B_{k}) \nonumber
\end{equation}


```{r, cart-motivate2, echo=FALSE, fig.cap="Regressogram estimate with the 2 bins: [0,1/3), [1/3, 1)."}

plot(xx, yy, las=1, xlab="x", ylab="Regression function estimate")
lines(c(0, 1/3), rep(mean(yy[ind1]), 2), lwd=3)
lines(c(1/3, 1), rep(mean(yy[ind2 | ind3]), 2), lwd=3)
abline(v=1/3, lty=2)
```

```{r, cart-motivate3, echo=FALSE, fig.cap="Regressogram estimate with the 2 bins: [0,2/3), [2/3, 1)."}

plot(xx, yy, las=1, xlab="x", ylab="Regression function estimate")
lines(c(0, 2/3), rep(mean(yy[ind1 | ind2]), 2), lwd=3)
lines(c(2/3, 1), rep(mean(yy[ind3]), 2), lwd=3)
abline(v=2/3, lty=2)
```


```{r}
var(yy[ind1 | ind2])
var(yy[ind3])
var(yy[ind2 | ind3])
var(yy[ind1])
```










