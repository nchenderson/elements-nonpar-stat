# (PART) Nonparametric Regression: Part II {-} 

# Regression Trees and CART {#decision-tree}

## Introduction

* Let's think about the regressogram estimate again. 

* The regressogram estimate of the regression function is a piecewise constant function that is constant within each of $p$ "bins"
\begin{equation}
\hat{m}_{h_{n}}^{R}(x) = \frac{1}{a}\sum_{i=1}^{n} Y_{i} I(x_{i} \in B_{k}), \qquad \textrm{ if } x \in B_{k} \nonumber
\end{equation}

* Figure \@ref(fig:cart-motivate) shows an example of a regressogram estimate with 3 bins. 

```{r, cart-motivate, echo=FALSE, fig.cap="Regressogram estimate with the 3 bins: [0,1/3), [1/3, 2/3), [2/3, 1)."}
set.seed(53186)
n <- 100
xx <- runif(n)
yy <- (2*xx - 1/2)^3 + rnorm(n,sd=.25)

ind1 <- xx < 1/3
ind2 <- xx < 2/3 & xx > 1/3
ind3 <- xx > 2/3

plot(xx, yy, las=1, xlab="x", ylab="Regression function estimate")
lines(c(0, 1/3), rep(mean(yy[ind1]), 2), lwd=3)
lines(c(1/3, 2/3), rep(mean(yy[ind2]), 2), lwd=3)
lines(c(2/3, 1), rep(mean(yy[ind3]), 2), lwd=3)
abline(v=1/3, lty=2)
abline(v=2/3, lty=2)
```

---

* Suppose we were forced to combined two adjacent bins to estimate the regression function. For the data shown in \@ref(fig:cart-motivate), 
which two bins should we combine if we were forced to do so? The only options here are to combine bins 1 and 2 or to combine bins 2 and 3.

* I would say we should combine bins 1 and 2. Look at Figures \@ref(fig:cart-motivate2) and \@ref(fig:cart-motivate3) for a comparison of these two choices.

* The responses $Y_{i}$ change much more over the range of the third bin than they do over the first and second bins. Hence, an 
intercept model for the first two bins is not all that bad. 

* In contrast, an intercept model for the last two bins is a terrible model. 

---

```{r, cart-motivate2, echo=FALSE, fig.cap="Regressogram estimate with the 2 bins: [0,1/3), [1/3, 1)."}

plot(xx, yy, las=1, xlab="x", ylab="Regression function estimate")
lines(c(0, 1/3), rep(mean(yy[ind1]), 2), lwd=3)
lines(c(1/3, 1), rep(mean(yy[ind2 | ind3]), 2), lwd=3)
abline(v=1/3, lty=2)
```

```{r, cart-motivate3, echo=FALSE, fig.cap="Regressogram estimate with the 2 bins: [0,2/3), [2/3, 1)."}

plot(xx, yy, las=1, xlab="x", ylab="Regression function estimate")
lines(c(0, 2/3), rep(mean(yy[ind1 | ind2]), 2), lwd=3)
lines(c(2/3, 1), rep(mean(yy[ind3]), 2), lwd=3)
abline(v=2/3, lty=2)
```

* The intuition for why the choice of bins $[0,2/3), [2/3, 1)$ is better than the choice of bins $[0,1/3), [1/3, 1)$ can 
be formalized by considering the within-bin variation of $Y_{i}$.

* For two bins $B_{1}$ and $B_{2}$, the within-bin sum of squares (WBSS) of $Y_{i}$ is
\begin{equation}
\textrm{WBSS} = \sum_{k=1}^{2} \sum_{i=1} (Y_{i} - \bar{Y}_{k})^{2}I(x_{i} \in B_{k}) \nonumber
\end{equation}

* You want to choose the bins in order to minimize the within-bin sum of squares. The reason for this is that: if the within-bin
sum of squares is low, an intercept model for each bin will fit the data very well.

---

* For the data shown in Figures \@ref(fig:cart-motivate) - \@ref(fig:cart-motivate3), the WBSS when using the bins $[0, 1/3), [1/3, 1)$
is
```{r}
sum((yy[xx < 1/3] - mean(yy[xx < 1/3]))^2) + sum((yy[xx >= 1/3] - mean(yy[xx >= 1/3]))^2) 
```

* The WBSS when using the bins $[0,2/3), [2/3, 1)$ is
```{r}
sum((yy[xx < 2/3] - mean(yy[xx < 2/3]))^2) + sum((yy[xx >= 2/3] - mean(yy[xx >= 2/3]))^2) 
```


## Regression Trees with a Single Covariate

* For a single covariate, regression trees estimate the regression function by a piecewise constant
function that is constant within each of several bins. We will focus on the well-known 
CART (Classification and Regression Trees) method for using regression trees.

* More generally, with multivariate covariates CART will fit a regression function that is constant
within each of many multi-dimensional "rectangles".

* The main difference between CART and the regressogram is that the placements
and widths of the bins in CART are chosen in a more selective manner than the regressogram.

* Specifically, rather than just using a collection of bins of fixed width, CART chooses where to place the bin boundaries by considering
the resulting within-bin sum of squares.

---

* CART constructs the bins through sequential binary splits. 

* That is, in the first step, CART will divide the covariates into two bins. Then, in the next step,

```{r, echo=FALSE}

WBSS <- function(y, x) {
   qq <- length(x)
   uu <- sort(x)
   wbss <- numeric(qq) 
   for(k in 1:qq) {
     wbss[k] <- sum((yy[xx < uu[k]] - mean(yy[xx < uu[k]]))^2) + sum((yy[xx >= uu[k]] - mean(yy[xx >= uu[k]]))^2) 
   }
   return(list(x=uu, y=wbss))
}

tmp <- WBSS(y=yy, x=xx)
plot(tmp$x, tmp$y, main="Within-Bin Sum of Squares by Cut Point", ylab="WBSS", xlab="cut point", las=1)
```

---

* This sequential process for constructing bins is typically depicted throug a binary decision tree.





## Regression Trees With Multiple Covariates

