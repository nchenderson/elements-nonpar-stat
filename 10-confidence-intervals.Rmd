# Bootstrap Examples and the Jackknife {#ci}
 

## The Parametric Bootstrap for an AR(1) model

* Consider the time series $X_{1}, X_{2}, \ldots, X_{m}$. Here,
$X_{t}$ denotes an observation made at time $t$.

* An autoregressive model of order 1 (usually called an AR(1) model) for this time series is
\begin{eqnarray}
X_{1} &=& \frac{c_{0}}{1 - \alpha} + \varepsilon_{1} \nonumber \\
X_{t} &=& c_{0} + \alpha X_{t-1} + \varepsilon_{t}, \qquad t=2,\ldots,m. \nonumber
\end{eqnarray}

* It is usually assumed that $|\alpha| < 1$.

* In the AR(1) model, it is assumed that 
    + $E(\varepsilon_{t}) = 0$
    + $\textrm{Var}(\varepsilon_{t}) = \sigma^{2}$,
    + $\varepsilon_{2}, \ldots, \varepsilon_{m}$ are i.i.d.
    + $\varepsilon_{t}$ and $X_{t-1}$ are independent.

* In addition to these assumptions, we will assume that
\begin{equation}
\varepsilon_{t} \sim \textrm{Normal}(0, \sigma^{2})  \nonumber 
\end{equation}

* The AR(1) model implies that 
\begin{equation}
\textrm{Corr}(X_{t}, X_{t-1}) = \alpha  \nonumber 
\end{equation}
and, more generally, that
\begin{equation}
\textrm{Corr}(X_{t}, X_{t-p}) = \alpha^{p}  \nonumber
\end{equation}


---

* For known values of $c_{0}, \alpha$, and $\sigma^{2}$, we can simulate
an AR(1) time series with the following `R` code:
```{r}
SimulateParAR1 <- function(m, c0, alpha, sig.sq) {
     xx <- numeric(m)
     xx[1] <- c0/(1 - alpha) + rnorm(1, sd=sqrt(sig.sq))
     for(t in 2:m) { 
         xx[t] <- c0 + alpha*xx[t-1] + rnorm(1, sd=sqrt(sig.sq))
     }
     return(xx)
}
```


```{r, echo=FALSE, fig.height=6}
x <- SimulateParAR1(100, 1, 0.8, sig.sq=.25)
par(mfrow=c(1,1))
plot(1:100, x, las=1, ylab=expression(X[t]), xlab="Time",
     main="Simulated AR(1) process: c0 = 1, alpha=0.8, sigma=0.5")
lines(1:100, x, lwd=2, las=1)
```
 
* In `R`, estimates of $c_{0}, \alpha,$ and $\sigma^{2}$ can be found by using the `ar` function. For example,
```{r}
x <- SimulateParAR1(1000, 1, 0.8, sig.sq=.25)
ar1.fit <- ar(x, aic=FALSE, order.max = 1, method="mle")

c0.est <- ar1.fit$x.mean*(1 - ar1.fit$ar)
alpha.est <- ar1.fit$ar
sigsq.est <- ar1.fit$var.pred
```

---

* Suppose we want to construct confidence intervals for $\alpha$ and $\sigma$ using a bootstrap method.

* Using the direct, nonparametric bootstrap described in the previous chapter will not work
because our observations are not independent. There are "block bootstraps" that
are designed to work for time series, but we will not discuss those here (see e.g., @buhlmann2002 or Chapter 8 of @davison1997 for 
more details).

* With the parametric bootstrap, we only have to use the following steps to generate bootstrap replications
$\hat{\alpha}_{r}^{*}$ and $\hat{\sigma}_{r}^{2,*}$ for estimates of $\alpha$ and $\hat{\sigma}^{2}$.
 
* For $r = 1, \ldots, R$:
    + Simulate a time series $X_{1}^{*}, \ldots, X_{m}^{*}$ from an AR(1) model with parameters $(\hat{c}_{0}, \hat{\alpha}, \hat{\sigma}^{2})$.
    + Compute $\hat{\alpha}_{r}^{*} = \hat{\alpha}(X_{1}^{*}, \ldots, X_{m}^{*})$.
    + Compute $\hat{\sigma}_{r}^{2,*} = \hat{\sigma}^{2}(X_{1}^{*}, \ldots, X_{m}^{*})$

---

* To see how this parametric bootstrap works, we will use the `nhtemp` dataset that is available in `R`.

```{r, echo=FALSE, fig.height=6}
plot(nhtemp, main = "nhtemp data",
     ylab = "Mean annual temperature in New Haven, CT (deg. F)", lwd=2, las=1)
```

* The `nhtemp` dataset contains the mean annual temperature in New Haven, Connecticut from the years 1912-1971
```{r}
head(nhtemp)
```

* The estimated autocorrelation parameter $\alpha$ is about $0.31$ for this data
```{r}
ar1.temp <- ar(nhtemp, aic=FALSE, order.max = 1)
c0.hat <- ar1.temp$x.mean*(1 - ar1.temp$ar)
alpha.hat <- ar1.temp$ar
sigsq.hat <- ar1.temp$var.pred
alpha.hat
```

* Now, that we have estimated all the parameter of the AR(1) model, we can run our parametric bootstrap for $\hat{\alpha}$ and $\hat{\sigma}$:
```{r}
R <- 500
alpha.boot <- numeric(R)
sigsq.boot <- numeric(R)
for(r in 1:R) {
  x <- SimulateParAR1(60, c0=c0.hat, alpha=alpha.hat, sig.sq=sigsq.hat)
  ar1.fit <- ar(x, aic=FALSE, order.max = 1)
  
  alpha.boot[r] <- ar1.fit$ar
  sigsq.boot[r] <- ar1.fit$var.pred
}
```

* Normal bootstrap standard error confidence intervals for $\alpha$ and $\sigma^{2}$ are
```{r}
round(c(alpha.hat - 1.96*sd(alpha.boot), alpha.hat + 1.96*sd(alpha.boot)), 3)
round(c(sigsq.hat - 1.96*sd(sigsq.boot), sigsq.hat + 1.96*sd(sigsq.boot)), 3)
```

* We can compare our confidence interval for $\alpha$ with the confidence interval
obtained from using a large-sample approximation:
```{r}
asymp.se <- sqrt(ar1.temp$asy.var.coef)
round(c(alpha.hat - 1.96*asymp.se, alpha.hat + 1.96*asymp.se), 3)
```


## Using the Bootstrap in Regression

* In linear regression with a single, univariate covariate, we work with the following model
\begin{equation}
Y_{i} = \beta_{0} + \beta_{1}x_{i} + \varepsilon_{i}, \qquad i = 1, \ldots, n.  \nonumber 
\end{equation}
    + $Y_{i}$ - the responses
    + $x_{i}$ - the covariates
    + $\beta_{0}, \beta_{1}$ - the regression coefficients
    + $\varepsilon_{i}$ - the residuals
    
* Typically, confidence intervals for the regression coefficients $\beta_{0}$ and $\beta_{1}$
are constructed under the assumption that $\varepsilon_{i} \sim \textrm{Normal}(0, \sigma^{2})$.

* The bootstrap allows us to compute confidence intervals for $(\beta_{0}, \beta_{1})$ without
relying on this normality assumption.

* How to compute bootstrap confidence intervals for $\beta_{0}$ and $\beta_{1}$?

---

* The least-squares estimates of $\beta_{0}$ and $\beta_{1}$ are
\begin{equation}
\hat{\beta}_{0} = \bar{y} - \hat{\beta}_{1}\bar{x} \qquad \qquad \hat{\beta}_{1} = \frac{\sum_{i=1}^{n}(x_{i} - \bar{x})(y_{i} - \bar{y})}{S_{xx}}  \nonumber
\end{equation}
where $S_{xx} = \sum_{i=1}^{n}( x_{i} - \bar{x})^{2}$.

* Assuming the covariates are fixed design points, the variance of $\hat{\beta}_{0}$ and $\hat{\beta}_{1}$ are
\begin{equation}
\textrm{Var}(\hat{\beta}_{0}) = \sigma^{2}\Big(\frac{\tfrac{1}{n}x_{i}^{2}}{S_{xx}} \Big) \qquad \textrm{Var}(\hat{\beta}_{1}) = \frac{\sigma^{2}}{S_{xx}} \nonumber
\end{equation}


---

**Parametric Bootstrap for Regression**

* With a parametric bootstrap, we will simulate outcomes $Y_{i}$ from the model
\begin{equation}
Y_{i} = \hat{\beta}_{0} + \hat{\beta}_{1}x_{i} + \varepsilon_{i},  \nonumber
\end{equation}
     + $\hat{\beta}_{0}$ and $\hat{\beta}_{1}$ are the least-squares estimates of $\beta_{0}$ and $\beta_{1}$,
     + $\varepsilon_{1}, \ldots, \varepsilon_{n}$ are i.i.d. random variables with mean zero and variance $\hat{\sigma}^{2}$.

* It is most common to assume that $\varepsilon_{i} \sim \textrm{Normal}(0, \hat{\sigma}^{2})$, 
where $\hat{\sigma}^{2}$ is an estimate of the residual variance.

* However, we could easily use an alternative parametric model for $\varepsilon_{i}$ if we thought
it was appropriate.

---

* A t-distribution with a small number of degrees of freedom can be useful
when the residuals are thought to follow a distribution with "heavier tails".

* If we assume $\varepsilon_{i} \sim \sigma \times t_{3}$, then $\textrm{Var}(\varepsilon_{i}) = 3\sigma^{2}$.

* So, with a $t_{3}$ residual distribution we want to simulate from the model
\begin{equation}
Y_{i} = \hat{\beta}_{0} + \hat{\beta}_{1}x_{i} + \frac{\hat{\sigma}}{\sqrt{3}}u_{i},  \qquad u_{i} \sim t_{3}, \nonumber
\end{equation}
where $\hat{\sigma}^{2}$ is the following estimate of the residual variance:
\begin{equation}
\hat{\sigma}^{2} = \frac{1}{n-2}\sum_{i=1}^{n} (Y_{i} - \hat{\beta}_{0} - \hat{\beta}_{1})^{2} \nonumber
\end{equation}


---

* To show how this parametric-t bootstrap works in practice we will look
at the kidney function data. 

* We will look at a linear regression where the measure of kidney function is 
the outcome and age is the covariate.

```{r}
kidney <- read.table("https://web.stanford.edu/~hastie/CASI_files/DATA/kidney.txt", 
                     header=TRUE)
```

```{r, echo=FALSE, fig.height=6}
lm.kidney <- lm(tot ~ age, data=kidney)
plot(kidney$age, kidney$tot, xlab="age", ylab="kidney function", main="Kidney Data", las=1)
abline(lm.kidney$coef[1], lm.kidney$coef[2], lwd=2)
legend("topright", legend=c(expression(paste(hat(beta)[0], " = 2.86")), expression(paste(hat(beta)[1], " = -0.08"))), bty="n")
```

* Bootstrap replications of $\hat{\beta}_{0}$ and $\hat{\beta}_{1}$ can
be computed using the following `R` code:
```{r}
## First find the parameter estimates
lm.kidney <- lm(tot ~ age, data=kidney)
beta0.hat <- lm.kidney$coef[1]
beta1.hat <- lm.kidney$coef[2]
sigsq.hat <- sum(lm.kidney$residuals^2)/(157 - 2)

## Using these estimates, run a parametric bootstrap to generate
## bootstrap replications of beta0.hat and beta1.hat
R <- 500
beta0.boot <- numeric(R)
beta1.boot <- numeric(R)
se.beta0.boot <- numeric(R)
se.beta1.boot <- numeric(R)
for(r in 1:R) {
  ysim <- beta0.hat + beta1.hat*kidney$age + sqrt(sigsq.hat/3)*rt(157, df=3)
  lm.boot <- lm(ysim ~ kidney$age)
  
  beta0.boot[r] <- lm.boot$coef[1]
  beta1.boot[r] <- lm.boot$coef[2]
  
  ## This code can be used to find the standard errors from this bootstrap sample
  sig.hatr <- summary(lm.boot)$sigma
  se.beta0.boot[r] <- sig.hatr*sqrt(summary(lm.boot)$cov.unscaled[1,1])
  se.beta1.boot[r] <- sig.hatr*sqrt(summary(lm.boot)$cov.unscaled[2,2])
}
```

---

* Because we have the formulas for the standard errors of $\beta_{0}$ and $\beta_{1}$,
we can use studentized bootstrap confidence intervals without using the double bootstrap approach.

* Estimates of the standard error for the $r^{th}$ bootstrap replication are
\begin{eqnarray}
\hat{se}_{r}(\beta_{0}) &=& \hat{\sigma}_{r}\sqrt{\frac{1}{n} + \frac{\bar{x}^{r}}{S_{xx}^{r}}} \nonumber \\
\hat{se}_{r}(\beta_{1}) &=& \hat{\sigma}_{r}/\sqrt{S_{xx}^{r}}
\end{eqnarray}

* These standard error estimates can be found by applying the above formulas () and () to the $r^{th}$ bootstrap sample.

* Recall from Chapter 9 that the studentized confidence intervals are found by using the following formula.
\begin{equation}
\Big[ T_{n} - se_{boot} \times \hat{K}_{R}^{-1}(1 - \alpha/2), T_{n} - se_{boot} \times \hat{K}_{R}^{-1}(\alpha/2) \Big] \nonumber
\end{equation}

---

```{r, echo=FALSE}
options(digits=3)
```

* `R` code to compute the studentized confidence is given below:
```{r}
## First get estimates of the standard error of our estimates
## I use the formulas for the regression standard errors, but
## we could have used a bootstrap estimate.
se.est0 <- summary(lm.kidney)$sigma*sqrt(summary(lm.boot)$cov.unscaled[1,1])
se.est1 <- summary(lm.kidney)$sigma*sqrt(summary(lm.boot)$cov.unscaled[2,2])

stu.quants0 <- quantile( (beta0.boot - beta0.hat)/se.beta0.boot, probs=c(0.025, 0.975))
stu.quants1 <- quantile( (beta1.boot - beta1.hat)/se.beta1.boot, probs=c(0.025, 0.975))

## Confidence interval for beta0
c(beta0.hat - stu.quants0[2]*se.est0, beta0.hat - stu.quants0[1]*se.est0)

## Confidence interval for beta1
c(beta1.hat - stu.quants1[2]*se.est1, beta1.hat - stu.quants1[1]*se.est1)
```

* Compare these studentized bootstrap confidence intervals with the confidence 
intervals computed under the normality assumption for the residuals:
```{r}
confint(lm.kidney)
```

```{r, echo=FALSE}
options(digits=7)
```
  